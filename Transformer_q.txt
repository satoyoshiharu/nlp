1. Attentionは、注目個所を学習するという適用範囲の広い考え方で、Query/Key/Valueとして定式化されることがある。
正しい
誤り
2. ある入力の中での、ある部分とある部分の関連度をSelf-Attentionといい、ある入力のある部分と別データのある部分との関連度をSource-Target Attentionという。
正しい
誤り
3. ある文の一部の単語をマスクして、その穴を推定させるようにトレーニングしたものをCausal Language Modelという。
正しい
誤り
4. Transformerは、RNNのように系列を順次処理することを避け、入力を一挙に並列処理するため、元の系列情報を単語の埋め込み表現ベクトルに埋め込む。それをPositional Encodingという。
正しい
誤り
5. Transformerは、層を深くして複雑な構造を補まえ、かつ層が深くても学習効果が得られるように、Residual Connectionを利用している。
正しい
誤り
