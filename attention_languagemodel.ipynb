{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"attention_languagemodel.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"l-GqFCft4dRk"},"source":["#データ準備"]},{"cell_type":"code","metadata":{"id":"lyWSbiXS56OF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622870751671,"user_tz":-540,"elapsed":19889,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"fe276a64-e4c8-4c01-8977-ddd92b4897ff"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cuksv4N3ErPv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622870771740,"user_tz":-540,"elapsed":231,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"a20a96dc-1ab2-48e4-a16b-842ff2fac9be"},"source":["!pwd\n","%cd /content/drive/My Drive/Colab Notebooks/NLP20211H"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/My Drive/Colab Notebooks/NLP20211H\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5EZ4B5rp4aW1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622870800961,"user_tz":-540,"elapsed":26114,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"b9f853e4-dcfb-4a5a-b148-b40ae18e18f4"},"source":["!pip install mecab-python3\n","!pip install unidic-lite"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting mecab-python3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/72/20f8f60b858556fdff6c0376b480c230e594621fff8be780603ac9c47f6a/mecab_python3-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (487kB)\n","\r\u001b[K     |▊                               | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 30.4MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 24.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 17.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 245kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 256kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 266kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 276kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 286kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 296kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 307kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 317kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 327kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 337kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 348kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 358kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 368kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 378kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 389kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 399kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 409kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 419kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 430kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 440kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 450kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 460kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 471kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 481kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 491kB 8.4MB/s \n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-1.0.3\n","Collecting unidic-lite\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/2b/8cf7514cb57d028abcef625afa847d60ff1ffbf0049c36b78faa7c35046f/unidic-lite-1.0.8.tar.gz (47.4MB)\n","\u001b[K     |████████████████████████████████| 47.4MB 63kB/s \n","\u001b[?25hBuilding wheels for collected packages: unidic-lite\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-cp37-none-any.whl size=47658838 sha256=a037a1d5a20740a5cb6248d019ebe2be59ac6d3835695f6c9e4911a55cb5b8dd\n","  Stored in directory: /root/.cache/pip/wheels/20/48/8d/b66d8361a27f58f41ec86640e4fd2640de0403a6367511eab7\n","Successfully built unidic-lite\n","Installing collected packages: unidic-lite\n","Successfully installed unidic-lite-1.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"crHqX2waUIkd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622870853849,"user_tz":-540,"elapsed":50658,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"cd820ec0-c918-4e21-8fb4-d09df6e59aa1"},"source":["#　Reference https://www.pytry3g.com/entry/gensim-word2veReference \n","import MeCab\n","import codecs\n","import urllib.parse as parser\n","import urllib.request as request\n","from bs4 import BeautifulSoup\n","\n","tagger = MeCab.Tagger(\"-Owakati\")\n","link = \"https://ja.wikipedia.org/wiki/\"\n","\n","#+\n","#キーワードのリストを受け取り、Wikipediaの該当記事を取得して、fileに出力する。\n","#-\n","def kwd2file(kwd,file):\n","  corpus = []\n","  for word in keyword:\n","      with request.urlopen(link + parser.quote_plus(word)) as response:\n","          html = response.read().decode('utf-8')\n","          soup = BeautifulSoup(html, \"lxml\")\n","          p_tags = soup.find_all('p')\n","          for p in p_tags:\n","              corpus.append(tagger.parse(p.text).strip())\n","  print(\"corpus size {}\".format(len(corpus)))\n","  print(\"corpus samples {}\".format(corpus[0:5]))\n","  with codecs.open(file, \"w\", \"utf-8\") as f:\n","      f.write(\"\\n\".join(corpus))\n","\n","keyword = [\n","           \"ロボット\",\"ロケット\",\"コンピュータ\",\"人工知能\",\"自動車\",\"宇宙\",\"機械\",\"道具\",\"計算\",\"装置\",\n","           \"プログラム\",\"メモリ\",\"CPU\",\"GPU\",\"クラウド\",\"サーバー\",\"クライアント\",\n","           \"情報\",\"構造\",\"ネットワーク\",\"性能\",\"演算\",\"記憶\",\n","           \"太陽\",\"月\",\"彗星\",\"惑星\",\"銀河系\",\"水星\",\"金星\",\"火星\",\"木星\",\"土星\",\"冥王星\",\"天王星\",\n","           \"人間\",\"目\",\"口\",\"耳\",\"舌\",\"肌\",\"指\",\"手\",\"足\",\"頭\",\"首\",\"声\",\"腹\",\"背\",\"胃\",\"腸\",\"髪\",\"血液\",\"免疫\",\n","           \"動物\",\"植物\",\"猫\",\"犬\",\"鳩\",\"シジュウカラ\",\"スズメ\",\"ネズミ\",\"魚\",\"鳥\",\"昆虫\",\"猿\",\"烏\",\"ミミズ\",\"ナメクジ\",\"蟻\",\"熊\",\"クジラ\",\"イルカ\",\n","           \"蛇\",\"ウナギ\",\"トンボ\",\"カマキリ\",\"ミツバチ\",\"ハエ\",\"カブトムシ\",\"豚\",\"牛\",\"鶏\",\n","           \"日本\",\"東京\",\"横浜\",\"北海道\",\"東北\",\"関東\",\"関西\",\"北陸\",\"中部\",\"北陸\",\"近畿\",\"四国\",\"九州\",\"沖縄\",\n","           \"アジア\",\"アメリカ\",\"中国\",\"韓国\",\"台湾\",\"香港\",\"北朝鮮\",\"アフリカ\",\"中近東\",\"オセアニア\",\n","           \"電気\",\"信号\",\"電波\",\"重力\",\"原子\",\"分子\",\"素粒子\",\"量子\",\n","           \"紙\",\"プラスチック\",\"鉄\",\"金\",\"銀\",\"銅\",\"アルミニウム\",\"ビニール\",\"土\",\"火\",\"水\",\"ガラス\",\n","           \"聖徳太子\",\"坂本龍馬\",\"徳川家康\",\"源義経\",\"夏目漱石\",\"宮沢賢治\",\"斎藤茂吉\",\n","           \"サッカー\",\"野球\",\"ラグビー\",\"プロレス\",\"テニス\",\"バレーボール\",\"ホッケー\",\"スキー\",\"スケート\",\n","           ]\n","kwd2file(keyword,'train.txt')\n","keyword = [\"ロボット\",] \n","kwd2file(keyword,'test.txt') #客観的に性能を示すため、本来、トレーニングデータと全く異なるデータを用いる。\n","keyword = [\"ロケット\",]\n","kwd2file(keyword,'valid.txt') #トレーニングの効果を見るため、本来、トレーニングと別データ。トレーニング用のデータを10等分して、順番に一つを使ったりする。\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["corpus size 8756\n","corpus samples ['ロボット （ robot ） は 、 人 の 代わり に 何 等 か の 作業 を 自律 的 に 行う 装置 、 もしくは 機械 の こと 。', '主に 以下 に 大別 する こと が 可能 で ある 。', '近年 で は 無人 機 「 ドローン 」 を 半ば 自律 化 さ せ た もの [ 1 ] も 存在 し 、 自動 運転 車 の 実現 が 視野 に 入っ て き て おり 、 SF の 世界 が 現実 の もの と なり つつ ある 。', '生命 体 に 通常 以上 の 力 を 発揮 さ せる 方策 と し て 何 ら か の 人工 物 を 埋め込ん だり 置き換える など の 方策 を 採っ た 者 は 一般 に 「 サイボーグ 」 など と 呼ば れ 区別 さ れる こと が 多い 。', 'この 言葉 が 初めて 用い られ た の は 、 1920 年 に チェコスロバキア （ 当時 ） の 小説 家 カレル ・ チャペック が 発表 し た 戯曲 『 R . U . R . （ ロッサム 万能 ロボット 商会 ） 』 に おい て で ある が 、 この 作品 で は 現在 認知 さ れ て いる 金属 製 の 機械 で は なく 、 人間 と は 異なる 組成 の 肉体 と 人間 そっくり の 外見 を 持つ もの を 、 化学 的 合成 で 原形 質 を 使っ て 製作 し た もの で 、 現在 の SF で 言う バイオ ノ イド で ある 。']\n","corpus size 49\n","corpus samples ['ロボット （ robot ） は 、 人 の 代わり に 何 等 か の 作業 を 自律 的 に 行う 装置 、 もしくは 機械 の こと 。', '主に 以下 に 大別 する こと が 可能 で ある 。', '近年 で は 無人 機 「 ドローン 」 を 半ば 自律 化 さ せ た もの [ 1 ] も 存在 し 、 自動 運転 車 の 実現 が 視野 に 入っ て き て おり 、 SF の 世界 が 現実 の もの と なり つつ ある 。', '生命 体 に 通常 以上 の 力 を 発揮 さ せる 方策 と し て 何 ら か の 人工 物 を 埋め込ん だり 置き換える など の 方策 を 採っ た 者 は 一般 に 「 サイボーグ 」 など と 呼ば れ 区別 さ れる こと が 多い 。', 'この 言葉 が 初めて 用い られ た の は 、 1920 年 に チェコスロバキア （ 当時 ） の 小説 家 カレル ・ チャペック が 発表 し た 戯曲 『 R . U . R . （ ロッサム 万能 ロボット 商会 ） 』 に おい て で ある が 、 この 作品 で は 現在 認知 さ れ て いる 金属 製 の 機械 で は なく 、 人間 と は 異なる 組成 の 肉体 と 人間 そっくり の 外見 を 持つ もの を 、 化学 的 合成 で 原形 質 を 使っ て 製作 し た もの で 、 現在 の SF で 言う バイオ ノ イド で ある 。']\n","corpus size 87\n","corpus samples ['ロケット （ 英 : rocket ） は 、 自ら の 質量 の 一部 を 後方 に 射出 し 、 その 反 作用 で 進む 力 （ 推力 ） を 得る 装置 （ ロケット エンジン ） 、 もしくは その 推力 を 利用 し て 移動 する 装置 で ある 。 外気 から 酸化 剤 を 取り込む 物 （ ジェット エンジン ） は 除く 。', '狭義 に は ロケット エンジン 自体 を いう が 、 先端 部 に 人工 衛星 や 宇宙 探査 機 など の ペイロード を 搭載 し て 宇宙 空間 の 特定 の 軌道 に 投入 さ せる 手段 と し て 使わ れる 、 ロケット エンジン を 推進 力 と する ローンチ ・ ヴィークル 全体 を ロケット と いう こと も 多い 。 日本 で は 、 地上 から 照射 さ れ た マイクロ 波 や レーザー ビーム を リフレクター で 反射 し 、 空気 の 電離 に よる プラズマ 発生 時 の 爆発 など を 推進 力 と し 、 燃料 を 使わ ない ローンチ ・ ヴィークル も 「 ロケット 」 と 呼ば れる [ 1 ]。', 'なお 、 推力 を 得る ため に 射出さ れる 質量 （ 推進 剤 、 プロペラント ） が 何 か 、 それ ら を 動かす エネルギー は 何 から 得る か に より 、 ロケット は 様々 な 方式 に 分類 さ れる が 、 ここ で は 最も 一般 的 に 使わ れ て いる 化学 ロケット （ 化学 燃料 ロケット ） を 中心 に 述べる 。', 'また 、 ロケット の 先端 部 に 核 弾頭 や 爆薬 など 軍用 の ペイロード を 搭載 し て 標的 や 目的 地 に 着弾 さ せる 兵器 は 、 日本 で は 無 誘導 の 場合 は 「 ロケット 弾 」 、 誘導 装置 を 持つ もの は ミサイル と し て 区別 さ れる （ 「 ロケット 弾 」 を 参照 ） 。 特に 弾道 飛行 を し て 目的 地 に 着弾 さ せる ミサイル は 、 弾道 ミサイル と し て 区別 し て いる 。 なお 、 北朝鮮 に よる 人工 衛星 の 打ち上げ は 、 国際 社会 から 事実 上 の 弾道 ミサイル 発射 実験 と 見なさ れ て おり 、 国際 連合 安全 保障 理事 会 決議 1718 と 1874 と 2087 で も 禁止 さ れ て いる ため 、 特に 日本 国 内 に おい て は 、 人工 衛星 打ち上げ で あっ て も ロケット で は なく ミサイル と 報道 さ れ て いる 。 また 他国 で は ミサイル と さ れる ところ を 、 ロケット や その 類語 で 呼称 する 国 も ある （ 「 ロシア 戦略 ロケット 軍 」 「 中国 人民 解放 軍 ロケット 軍 」 を 参照 ） 。', 'ロケット の 語源 は 、 1379 年 に イタリア の 芸術 家 兼 技術 者 で ある ムラ トーリ [ 注 1 ] が 西欧 で 初めて 火薬 推進 式 の ロケット を 作り 、 それ を 形状 に ちなん で 「 ロッケッタ 」 （ 伊 : Rocchetta 、 「 小さな 糸巻 棒 」 の 意 ） と 名づけ た こと に よる 。']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9s6rqmBrQl5H","executionInfo":{"status":"ok","timestamp":1622870863241,"user_tz":-540,"elapsed":3746,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}}},"source":["import os\n","from io import open\n","import torch\n","\n","class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = []\n","\n","    #+\n","    #単語を受け取り、idx2word, word2idxを作成する\n","    #-\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)\n","\n","class Corpus(object):\n","    #+\n","    #3つのファイルを開き、単語を辞書に登録し、テキストをIDのリストに変換して、出力する\n","    #-\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","\n","    def tokenize(self, path):\n","        #辞書に単語を登録する\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                #print(words)\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","        #コーパスを単語Indexのリストに変換する\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            idss = []\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                ids = []\n","                for word in words:\n","                    ids.append(self.dictionary.word2idx[word])\n","                idss.append(torch.tensor(ids).type(torch.int64))\n","            ids = torch.cat(idss)\n","        return ids"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z_ua6pXN_wOw"},"source":["#Transformer"]},{"cell_type":"code","metadata":{"id":"qfXVP4QzACtb","executionInfo":{"status":"ok","timestamp":1622870866284,"user_tz":-540,"elapsed":259,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}}},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","#+\n","#位置情報を埋め込み表現に組み込む。埋め込みベクトルサイズで、各単語位置にユニークなパターンを作り、\n","#埋め込みベクトルに加算する。\n","#-\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        # d_modelは、埋め込みサイズ\n","        # max_lenは、入力の最大単語数\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model) #PositionalEncoding表の初期化\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) #最大単語数までの数列生成\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) #Originalと違い、分母は対数をとっているようだ\n","        pe[:, 0::2] = torch.sin(position * div_term) #埋め込みベクトルの偶数番位置設定\n","        pe[:, 1::2] = torch.cos(position * div_term) #埋め込みベクトルの奇数番位置設定\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe) #作ったパターンをオブジェクトの静的領域に置いて、後で使えるようにする\n","\n","    def forward(self, x):\n","        #xのshapeは、(SEQ_LENGTH,BATCH_SIZE_埋め込みサイズ)\n","        x = x + self.pe[:x.size(0), :] #SEQ_LENGTH分だけ位置パターン加算\n","        return self.dropout(x)\n","\n","#+\n","#Transformerを使ったモデル定義\n","#-\n","class TransformerModel(nn.Module):\n","\n","    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n","        super(TransformerModel, self).__init__()\n","        self.src_mask = None\n","        self.pos_encoder = PositionalEncoding(ninp, dropout) #位置エンコーダーオブジェクト生成\n","        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout) #TransformerEncoderLayerオブジェクトを生成\n","        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) #レイヤーオブジェクトを指定数だけ重ねてTransformerEncoderオブジェクトとする\n","        self.encoder = nn.Embedding(ntoken, ninp) #埋め込み表生成\n","        self.ninp = ninp #埋め込みサイズ\n","        self.decoder = nn.Linear(ninp, ntoken) #デコード部分は隠れ状態から語彙サイズへの全結合のみ\n","        self.init_weights()\n","\n","    #+\n","    # 入力のSEQ_LENサイズ四方の以下のようなマスクを生成する\n","    # 1個目（列方向）の予測時は、2個目以降の単語（行方向）が見られない(-inf)、...　ように情報を制限するため\n","    # [[0., -inf, -inf, ... , -inf],\n","    #  [0.,     0., -inf, ..., -inf],\n","    #  ...\n","    #  [0.,     0.,    0., ...,    0.]]\n","    #-\n","    def _generate_square_subsequent_mask(self, sz): \n","        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1) #右上三角が1である行列を転置、左下三角１の行列になる\n","        #０と-infで埋める、右上は-inf、左下は0。Attentionを計算するときに、ーinfの部分を無効にする\n","        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)) \n","        return mask\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n","        nn.init.zeros_(self.decoder.weight)\n","        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n","\n","    def forward(self, src, has_mask=True):\n","        # src shape is (SEQ_LENGTH 35, BATCH_SIZE 20)、以下バッチサイズだけ並行実行\n","        if has_mask:\n","            device = src.device\n","            if self.src_mask is None or self.src_mask.size(0) != len(src):\n","                mask = self._generate_square_subsequent_mask(len(src)).to(device) #マスク生成\n","                self.src_mask = mask\n","                #print('len(src) {}, mask\\n{}'.format(len(src),mask))\n","        else:\n","            self.src_mask = None\n","        src = self.encoder(src) #埋め込みベクトルを取り出す、shapeが(SEQ_LENGTH 35, BATCH_SIZE, 埋め込みサイズ20)\n","        src = src * math.sqrt(self.ninp) #位置情報を安全に埋め込むため、入力値を調整\n","        src = self.pos_encoder(src) #\n","        output = self.transformer_encoder(src, self.src_mask) #SEQ_LENGTH分の単語列の入力を、TransformerEncoderにかける\n","        output = self.decoder(output) #\n","        return F.log_softmax(output, dim=-1)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"6k39DpLqAu5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622871954674,"user_tz":-540,"elapsed":1079798,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"d99f4c95-ab4b-4863-bd79-66fac4769d82"},"source":["# coding: utf-8\n","import time\n","import math\n","import os\n","import torch\n","import torch.nn as nn\n","import easydict\n","\n","torch.manual_seed(1111)\n","device = torch.device(\"cuda\" if True else \"cpu\")\n","corpus = Corpus('./')\n","BATCH_SIZE = 20\n","EVAL_BATCH_SIZE = 10\n","SEQ_LENGTH = 35 #このサイズで逆伝播を停止する。入力が長い場合の対策\n","\n","def batchify(data, bsz):\n","    nbatch = data.size(0) // bsz #コーパスの長さをミニバッチサイズで割り、何個のミニバッチかを得る\n","    data = data.narrow(0, 0, nbatch * bsz) #ミニバッチにハマらなかった部分を捨てる\n","    data = data.view(bsz, -1).t().contiguous() #データをミニバッチサイズで分割して、行列を入れ替える\n","    #->shapeが(バッチの個数,バッチサイズ)になる、train_dataなら23154行、20列の行列になる、\n","    #連続した単語は列方向に並んでいる\n","    return data.to(device)\n","\n","#データをミニバッチに分ける\n","train_data = batchify(corpus.train, BATCH_SIZE)\n","val_data = batchify(corpus.valid, EVAL_BATCH_SIZE)\n","test_data = batchify(corpus.test, EVAL_BATCH_SIZE)\n","\n","ntokens = len(corpus.dictionary)\n","print('ntokens {}'.format(ntokens))\n","\n","model = TransformerModel(\n","    ntokens, \n","    20, #埋め込みサイズ\n","    5, #ヘッド数 \n","    200,#隠れベクトルサイズ \n","    6,#スタック数 \n","    0.5 #dropout\n","    ).to(device)\n","print(model)\n","criterion = nn.NLLLoss()\n","\n","def get_batch(source, i):\n","    #iには、0から順番にSEQ_LENGTHだけ飛んだ値が入ってくる\n","    seq_len = min(SEQ_LENGTH, len(source) - 1 - i)\n","    data = source[i:i+seq_len] #列方向からSEQ_LENGTHだけ切り出す、連続単語の認識はこの長さの範囲だけで実施\n","    target = source[i+1:i+1+seq_len].view(-1) #正解単語になるように1個後ろにずらしてSEQ_LENGTHだけ切り出す\n","    return data, target\n","\n","def train():\n","    model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","    ntokens = len(corpus.dictionary)\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, SEQ_LENGTH)): #SEQ_LENGTH分取り出して繰り返す\n","        data, targets = get_batch(train_data, i) #shapeが(SEQ_LENGTH, BATCH_SIZE)のデータを得る\n","        model.zero_grad() #勾配初期化\n","        output = model(data) #モデルに通す,　outputは、shape(SEQ_LENGTH, BATCH_SIZE, 単語数)\n","        output = output.view(-1, ntokens) #shapeを(SQE_LENGTH*BATCH_SIZE, 単語数)にする\n","        loss = criterion(output, targets) #ロスを求める\n","        loss.backward() #逆伝播\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25) #勾配の最大が0.25になるように、全勾配に調整かける\n","        for p in model.parameters(): #学習レート低減\n","            p.data.add_(-lr, p.grad)\n","        total_loss += loss.item()\n","\n","        if batch % 200 == 0 and batch > 0:\n","            cur_loss = total_loss / 200\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch, len(train_data) // SEQ_LENGTH, lr, elapsed * 1000 / 200, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","def evaluate(data_source):\n","    model.eval()\n","    total_loss = 0.\n","    ntokens = len(corpus.dictionary)\n","    with torch.no_grad(): #評価なので、勾配を計算しない設定にする\n","        for i in range(0, data_source.size(0) - 1, SEQ_LENGTH): #BPTT_SEQ_LENGTH分取り出して繰り返す\n","            data, targets = get_batch(data_source, i) #shapeが(BPTT_SEQ_LENGTH,BATCH_SIZE)のデータを得る\n","            output = model(data) #モデルに通す\n","            output = output.view(-1, ntokens)\n","            total_loss += len(data) * criterion(output, targets).item()\n","    return total_loss / (len(data_source) - 1)\n","\n","# Loop over epochs.\n","lr = 1.0\n","best_val_loss = None\n","try:\n","    for epoch in range(1, 50+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(val_data)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n","        if not best_val_loss or val_loss < best_val_loss: #一番良いパラメータを保存する\n","            with open('jpn_model_t.pt', 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        else:\n","            lr *= 0.5 #改善しなくなったら、学習レートを下げる\n","except KeyboardInterrupt:\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open('jpn_model_t.pt', 'rb') as f:\n","    model = torch.load(f)\n","\n","# Run on test data.\n","test_loss = evaluate(test_data)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["ntokens 37829\n","TransformerModel(\n","  (pos_encoder): PositionalEncoding(\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=20, out_features=20, bias=True)\n","        )\n","        (linear1): Linear(in_features=20, out_features=200, bias=True)\n","        (dropout): Dropout(p=0.5, inplace=False)\n","        (linear2): Linear(in_features=200, out_features=20, bias=True)\n","        (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.5, inplace=False)\n","        (dropout2): Dropout(p=0.5, inplace=False)\n","      )\n","      (1): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=20, out_features=20, bias=True)\n","        )\n","        (linear1): Linear(in_features=20, out_features=200, bias=True)\n","        (dropout): Dropout(p=0.5, inplace=False)\n","        (linear2): Linear(in_features=200, out_features=20, bias=True)\n","        (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.5, inplace=False)\n","        (dropout2): Dropout(p=0.5, inplace=False)\n","      )\n","      (2): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=20, out_features=20, bias=True)\n","        )\n","        (linear1): Linear(in_features=20, out_features=200, bias=True)\n","        (dropout): Dropout(p=0.5, inplace=False)\n","        (linear2): Linear(in_features=200, out_features=20, bias=True)\n","        (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.5, inplace=False)\n","        (dropout2): Dropout(p=0.5, inplace=False)\n","      )\n","      (3): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=20, out_features=20, bias=True)\n","        )\n","        (linear1): Linear(in_features=20, out_features=200, bias=True)\n","        (dropout): Dropout(p=0.5, inplace=False)\n","        (linear2): Linear(in_features=200, out_features=20, bias=True)\n","        (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.5, inplace=False)\n","        (dropout2): Dropout(p=0.5, inplace=False)\n","      )\n","      (4): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=20, out_features=20, bias=True)\n","        )\n","        (linear1): Linear(in_features=20, out_features=200, bias=True)\n","        (dropout): Dropout(p=0.5, inplace=False)\n","        (linear2): Linear(in_features=200, out_features=20, bias=True)\n","        (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.5, inplace=False)\n","        (dropout2): Dropout(p=0.5, inplace=False)\n","      )\n","      (5): TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): _LinearWithBias(in_features=20, out_features=20, bias=True)\n","        )\n","        (linear1): Linear(in_features=20, out_features=200, bias=True)\n","        (dropout): Dropout(p=0.5, inplace=False)\n","        (linear2): Linear(in_features=200, out_features=20, bias=True)\n","        (norm1): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.5, inplace=False)\n","        (dropout2): Dropout(p=0.5, inplace=False)\n","      )\n","    )\n","  )\n","  (encoder): Embedding(37829, 20)\n","  (decoder): Linear(in_features=20, out_features=37829, bias=True)\n",")\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:64: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"],"name":"stderr"},{"output_type":"stream","text":["| epoch   1 |   200/ 1277 batches | lr 1.00 | ms/batch 17.82 | loss  7.60 | ppl  1995.56\n","| epoch   1 |   400/ 1277 batches | lr 1.00 | ms/batch 16.43 | loss  7.03 | ppl  1130.86\n","| epoch   1 |   600/ 1277 batches | lr 1.00 | ms/batch 16.30 | loss  6.95 | ppl  1044.03\n","| epoch   1 |   800/ 1277 batches | lr 1.00 | ms/batch 16.38 | loss  6.88 | ppl   976.65\n","| epoch   1 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.44 | loss  6.88 | ppl   971.95\n","| epoch   1 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.32 | loss  6.84 | ppl   933.40\n","| end of epoch   1 | time: 21.32s | valid loss  6.96 | valid ppl  1054.72\n","| epoch   2 |   200/ 1277 batches | lr 1.00 | ms/batch 16.47 | loss  6.85 | ppl   942.22\n","| epoch   2 |   400/ 1277 batches | lr 1.00 | ms/batch 16.44 | loss  6.81 | ppl   902.79\n","| epoch   2 |   600/ 1277 batches | lr 1.00 | ms/batch 16.54 | loss  6.80 | ppl   900.95\n","| epoch   2 |   800/ 1277 batches | lr 1.00 | ms/batch 16.57 | loss  6.78 | ppl   878.62\n","| epoch   2 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  6.80 | ppl   897.48\n","| epoch   2 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.48 | loss  6.76 | ppl   865.68\n","| end of epoch   2 | time: 21.20s | valid loss  6.74 | valid ppl   848.49\n","| epoch   3 |   200/ 1277 batches | lr 1.00 | ms/batch 16.60 | loss  6.65 | ppl   769.19\n","| epoch   3 |   400/ 1277 batches | lr 1.00 | ms/batch 16.56 | loss  6.54 | ppl   692.91\n","| epoch   3 |   600/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  6.44 | ppl   628.33\n","| epoch   3 |   800/ 1277 batches | lr 1.00 | ms/batch 16.56 | loss  6.34 | ppl   564.02\n","| epoch   3 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  6.30 | ppl   547.17\n","| epoch   3 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.46 | loss  6.23 | ppl   509.78\n","| end of epoch   3 | time: 21.25s | valid loss  6.25 | valid ppl   517.78\n","| epoch   4 |   200/ 1277 batches | lr 1.00 | ms/batch 16.65 | loss  6.23 | ppl   508.26\n","| epoch   4 |   400/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  6.16 | ppl   474.40\n","| epoch   4 |   600/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  6.14 | ppl   462.53\n","| epoch   4 |   800/ 1277 batches | lr 1.00 | ms/batch 16.75 | loss  6.09 | ppl   439.25\n","| epoch   4 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  6.09 | ppl   439.50\n","| epoch   4 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  6.03 | ppl   415.08\n","| end of epoch   4 | time: 21.29s | valid loss  6.04 | valid ppl   417.84\n","| epoch   5 |   200/ 1277 batches | lr 1.00 | ms/batch 16.72 | loss  6.05 | ppl   424.13\n","| epoch   5 |   400/ 1277 batches | lr 1.00 | ms/batch 16.66 | loss  5.99 | ppl   398.34\n","| epoch   5 |   600/ 1277 batches | lr 1.00 | ms/batch 16.60 | loss  5.97 | ppl   391.54\n","| epoch   5 |   800/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.93 | ppl   377.05\n","| epoch   5 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.57 | loss  5.94 | ppl   381.53\n","| epoch   5 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  5.90 | ppl   365.64\n","| end of epoch   5 | time: 21.34s | valid loss  5.93 | valid ppl   374.92\n","| epoch   6 |   200/ 1277 batches | lr 1.00 | ms/batch 16.82 | loss  5.94 | ppl   379.33\n","| epoch   6 |   400/ 1277 batches | lr 1.00 | ms/batch 16.75 | loss  5.88 | ppl   357.86\n","| epoch   6 |   600/ 1277 batches | lr 1.00 | ms/batch 16.64 | loss  5.86 | ppl   352.47\n","| epoch   6 |   800/ 1277 batches | lr 1.00 | ms/batch 16.70 | loss  5.84 | ppl   343.58\n","| epoch   6 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.86 | ppl   349.19\n","| epoch   6 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  5.81 | ppl   334.62\n","| end of epoch   6 | time: 21.40s | valid loss  5.84 | valid ppl   343.92\n","| epoch   7 |   200/ 1277 batches | lr 1.00 | ms/batch 16.67 | loss  5.85 | ppl   348.93\n","| epoch   7 |   400/ 1277 batches | lr 1.00 | ms/batch 16.60 | loss  5.81 | ppl   332.07\n","| epoch   7 |   600/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.79 | ppl   328.28\n","| epoch   7 |   800/ 1277 batches | lr 1.00 | ms/batch 16.59 | loss  5.77 | ppl   320.07\n","| epoch   7 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.78 | ppl   325.26\n","| epoch   7 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.48 | loss  5.75 | ppl   314.89\n","| end of epoch   7 | time: 21.25s | valid loss  5.78 | valid ppl   325.03\n","| epoch   8 |   200/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.80 | ppl   329.45\n","| epoch   8 |   400/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.75 | ppl   312.87\n","| epoch   8 |   600/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  5.74 | ppl   310.32\n","| epoch   8 |   800/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.71 | ppl   302.87\n","| epoch   8 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.56 | loss  5.73 | ppl   308.19\n","| epoch   8 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.70 | ppl   298.42\n","| end of epoch   8 | time: 21.26s | valid loss  5.73 | valid ppl   307.12\n","| epoch   9 |   200/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.75 | ppl   313.92\n","| epoch   9 |   400/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.70 | ppl   298.32\n","| epoch   9 |   600/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.69 | ppl   295.35\n","| epoch   9 |   800/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.67 | ppl   289.54\n","| epoch   9 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.56 | loss  5.68 | ppl   294.15\n","| epoch   9 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  5.65 | ppl   285.34\n","| end of epoch   9 | time: 21.28s | valid loss  5.69 | valid ppl   294.93\n","| epoch  10 |   200/ 1277 batches | lr 1.00 | ms/batch 16.70 | loss  5.71 | ppl   300.67\n","| epoch  10 |   400/ 1277 batches | lr 1.00 | ms/batch 16.71 | loss  5.66 | ppl   286.19\n","| epoch  10 |   600/ 1277 batches | lr 1.00 | ms/batch 16.66 | loss  5.65 | ppl   283.03\n","| epoch  10 |   800/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.63 | ppl   277.68\n","| epoch  10 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.64 | ppl   282.13\n","| epoch  10 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.61 | ppl   273.86\n","| end of epoch  10 | time: 21.31s | valid loss  5.65 | valid ppl   285.06\n","| epoch  11 |   200/ 1277 batches | lr 1.00 | ms/batch 16.68 | loss  5.67 | ppl   289.86\n","| epoch  11 |   400/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.61 | ppl   273.99\n","| epoch  11 |   600/ 1277 batches | lr 1.00 | ms/batch 16.61 | loss  5.61 | ppl   272.36\n","| epoch  11 |   800/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  5.59 | ppl   267.18\n","| epoch  11 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.61 | ppl   271.87\n","| epoch  11 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.56 | loss  5.58 | ppl   265.06\n","| end of epoch  11 | time: 21.29s | valid loss  5.62 | valid ppl   276.11\n","| epoch  12 |   200/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.63 | ppl   280.03\n","| epoch  12 |   400/ 1277 batches | lr 1.00 | ms/batch 16.66 | loss  5.58 | ppl   266.29\n","| epoch  12 |   600/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  5.57 | ppl   263.08\n","| epoch  12 |   800/ 1277 batches | lr 1.00 | ms/batch 16.56 | loss  5.55 | ppl   257.79\n","| epoch  12 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.48 | loss  5.57 | ppl   263.25\n","| epoch  12 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.60 | loss  5.54 | ppl   255.81\n","| end of epoch  12 | time: 21.29s | valid loss  5.58 | valid ppl   265.83\n","| epoch  13 |   200/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  5.60 | ppl   271.63\n","| epoch  13 |   400/ 1277 batches | lr 1.00 | ms/batch 16.67 | loss  5.56 | ppl   258.73\n","| epoch  13 |   600/ 1277 batches | lr 1.00 | ms/batch 16.54 | loss  5.54 | ppl   254.59\n","| epoch  13 |   800/ 1277 batches | lr 1.00 | ms/batch 16.49 | loss  5.52 | ppl   249.44\n","| epoch  13 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.55 | ppl   256.74\n","| epoch  13 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.66 | loss  5.52 | ppl   248.70\n","| end of epoch  13 | time: 21.29s | valid loss  5.56 | valid ppl   259.60\n","| epoch  14 |   200/ 1277 batches | lr 1.00 | ms/batch 16.66 | loss  5.58 | ppl   265.61\n","| epoch  14 |   400/ 1277 batches | lr 1.00 | ms/batch 16.71 | loss  5.53 | ppl   251.12\n","| epoch  14 |   600/ 1277 batches | lr 1.00 | ms/batch 16.46 | loss  5.51 | ppl   248.38\n","| epoch  14 |   800/ 1277 batches | lr 1.00 | ms/batch 16.40 | loss  5.49 | ppl   242.27\n","| epoch  14 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.54 | loss  5.52 | ppl   249.54\n","| epoch  14 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.49 | ppl   243.22\n","| end of epoch  14 | time: 21.24s | valid loss  5.54 | valid ppl   255.49\n","| epoch  15 |   200/ 1277 batches | lr 1.00 | ms/batch 16.65 | loss  5.56 | ppl   258.74\n","| epoch  15 |   400/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  5.51 | ppl   246.03\n","| epoch  15 |   600/ 1277 batches | lr 1.00 | ms/batch 16.54 | loss  5.49 | ppl   241.95\n","| epoch  15 |   800/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.46 | ppl   236.18\n","| epoch  15 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.49 | ppl   242.45\n","| epoch  15 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.46 | ppl   236.23\n","| end of epoch  15 | time: 21.30s | valid loss  5.51 | valid ppl   247.34\n","| epoch  16 |   200/ 1277 batches | lr 1.00 | ms/batch 16.68 | loss  5.53 | ppl   252.53\n","| epoch  16 |   400/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  5.48 | ppl   239.61\n","| epoch  16 |   600/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.47 | ppl   236.71\n","| epoch  16 |   800/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.45 | ppl   231.93\n","| epoch  16 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.69 | loss  5.47 | ppl   237.19\n","| epoch  16 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.49 | loss  5.45 | ppl   232.48\n","| end of epoch  16 | time: 21.29s | valid loss  5.48 | valid ppl   239.34\n","| epoch  17 |   200/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  5.51 | ppl   247.48\n","| epoch  17 |   400/ 1277 batches | lr 1.00 | ms/batch 16.64 | loss  5.46 | ppl   234.86\n","| epoch  17 |   600/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  5.45 | ppl   231.73\n","| epoch  17 |   800/ 1277 batches | lr 1.00 | ms/batch 16.59 | loss  5.42 | ppl   226.06\n","| epoch  17 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.64 | loss  5.45 | ppl   233.17\n","| epoch  17 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.43 | ppl   228.08\n","| end of epoch  17 | time: 21.32s | valid loss  5.45 | valid ppl   231.91\n","| epoch  18 |   200/ 1277 batches | lr 1.00 | ms/batch 16.64 | loss  5.49 | ppl   242.89\n","| epoch  18 |   400/ 1277 batches | lr 1.00 | ms/batch 16.74 | loss  5.44 | ppl   230.77\n","| epoch  18 |   600/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  5.43 | ppl   227.65\n","| epoch  18 |   800/ 1277 batches | lr 1.00 | ms/batch 16.57 | loss  5.40 | ppl   222.31\n","| epoch  18 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.60 | loss  5.43 | ppl   228.82\n","| epoch  18 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.41 | ppl   224.65\n","| end of epoch  18 | time: 21.31s | valid loss  5.44 | valid ppl   230.09\n","| epoch  19 |   200/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  5.48 | ppl   238.93\n","| epoch  19 |   400/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.42 | ppl   226.55\n","| epoch  19 |   600/ 1277 batches | lr 1.00 | ms/batch 16.54 | loss  5.41 | ppl   224.04\n","| epoch  19 |   800/ 1277 batches | lr 1.00 | ms/batch 16.67 | loss  5.39 | ppl   218.61\n","| epoch  19 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.41 | ppl   224.09\n","| epoch  19 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.40 | ppl   221.18\n","| end of epoch  19 | time: 21.30s | valid loss  5.42 | valid ppl   226.72\n","| epoch  20 |   200/ 1277 batches | lr 1.00 | ms/batch 16.71 | loss  5.46 | ppl   235.40\n","| epoch  20 |   400/ 1277 batches | lr 1.00 | ms/batch 16.66 | loss  5.41 | ppl   223.77\n","| epoch  20 |   600/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.39 | ppl   219.76\n","| epoch  20 |   800/ 1277 batches | lr 1.00 | ms/batch 16.64 | loss  5.37 | ppl   215.00\n","| epoch  20 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  5.40 | ppl   221.07\n","| epoch  20 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  5.38 | ppl   217.69\n","| end of epoch  20 | time: 21.30s | valid loss  5.40 | valid ppl   220.35\n","| epoch  21 |   200/ 1277 batches | lr 1.00 | ms/batch 16.65 | loss  5.44 | ppl   231.50\n","| epoch  21 |   400/ 1277 batches | lr 1.00 | ms/batch 16.60 | loss  5.39 | ppl   219.73\n","| epoch  21 |   600/ 1277 batches | lr 1.00 | ms/batch 16.65 | loss  5.38 | ppl   216.79\n","| epoch  21 |   800/ 1277 batches | lr 1.00 | ms/batch 16.65 | loss  5.35 | ppl   211.29\n","| epoch  21 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  5.38 | ppl   217.06\n","| epoch  21 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  5.37 | ppl   214.87\n","| end of epoch  21 | time: 21.33s | valid loss  5.38 | valid ppl   216.72\n","| epoch  22 |   200/ 1277 batches | lr 1.00 | ms/batch 16.66 | loss  5.43 | ppl   228.27\n","| epoch  22 |   400/ 1277 batches | lr 1.00 | ms/batch 16.59 | loss  5.38 | ppl   216.48\n","| epoch  22 |   600/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.36 | ppl   213.14\n","| epoch  22 |   800/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.34 | ppl   207.99\n","| epoch  22 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.36 | ppl   213.13\n","| epoch  22 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.56 | loss  5.36 | ppl   212.37\n","| end of epoch  22 | time: 21.27s | valid loss  5.37 | valid ppl   215.24\n","| epoch  23 |   200/ 1277 batches | lr 1.00 | ms/batch 16.67 | loss  5.42 | ppl   226.00\n","| epoch  23 |   400/ 1277 batches | lr 1.00 | ms/batch 16.67 | loss  5.36 | ppl   213.54\n","| epoch  23 |   600/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.35 | ppl   210.24\n","| epoch  23 |   800/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.33 | ppl   205.55\n","| epoch  23 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.57 | loss  5.35 | ppl   210.72\n","| epoch  23 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.34 | ppl   208.91\n","| end of epoch  23 | time: 21.28s | valid loss  5.37 | valid ppl   214.00\n","| epoch  24 |   200/ 1277 batches | lr 1.00 | ms/batch 16.61 | loss  5.41 | ppl   223.16\n","| epoch  24 |   400/ 1277 batches | lr 1.00 | ms/batch 16.74 | loss  5.35 | ppl   210.37\n","| epoch  24 |   600/ 1277 batches | lr 1.00 | ms/batch 16.59 | loss  5.34 | ppl   208.08\n","| epoch  24 |   800/ 1277 batches | lr 1.00 | ms/batch 16.64 | loss  5.31 | ppl   202.55\n","| epoch  24 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.61 | loss  5.34 | ppl   207.57\n","| epoch  24 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.64 | loss  5.33 | ppl   206.35\n","| end of epoch  24 | time: 21.35s | valid loss  5.33 | valid ppl   206.35\n","| epoch  25 |   200/ 1277 batches | lr 1.00 | ms/batch 16.77 | loss  5.39 | ppl   219.68\n","| epoch  25 |   400/ 1277 batches | lr 1.00 | ms/batch 16.70 | loss  5.33 | ppl   207.24\n","| epoch  25 |   600/ 1277 batches | lr 1.00 | ms/batch 16.69 | loss  5.33 | ppl   206.19\n","| epoch  25 |   800/ 1277 batches | lr 1.00 | ms/batch 16.57 | loss  5.30 | ppl   199.94\n","| epoch  25 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  5.32 | ppl   204.92\n","| epoch  25 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.32 | ppl   204.38\n","| end of epoch  25 | time: 21.36s | valid loss  5.32 | valid ppl   204.53\n","| epoch  26 |   200/ 1277 batches | lr 1.00 | ms/batch 16.62 | loss  5.38 | ppl   217.19\n","| epoch  26 |   400/ 1277 batches | lr 1.00 | ms/batch 16.64 | loss  5.32 | ppl   205.31\n","| epoch  26 |   600/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.31 | ppl   202.63\n","| epoch  26 |   800/ 1277 batches | lr 1.00 | ms/batch 16.56 | loss  5.28 | ppl   197.21\n","| epoch  26 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.31 | ppl   201.87\n","| epoch  26 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  5.31 | ppl   202.32\n","| end of epoch  26 | time: 21.27s | valid loss  5.30 | valid ppl   201.33\n","| epoch  27 |   200/ 1277 batches | lr 1.00 | ms/batch 16.72 | loss  5.37 | ppl   214.52\n","| epoch  27 |   400/ 1277 batches | lr 1.00 | ms/batch 16.65 | loss  5.31 | ppl   202.84\n","| epoch  27 |   600/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.30 | ppl   200.55\n","| epoch  27 |   800/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.27 | ppl   194.89\n","| epoch  27 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  5.30 | ppl   199.72\n","| epoch  27 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.63 | loss  5.30 | ppl   199.91\n","| end of epoch  27 | time: 21.31s | valid loss  5.29 | valid ppl   198.46\n","| epoch  28 |   200/ 1277 batches | lr 1.00 | ms/batch 16.57 | loss  5.36 | ppl   212.30\n","| epoch  28 |   400/ 1277 batches | lr 1.00 | ms/batch 16.64 | loss  5.30 | ppl   200.43\n","| epoch  28 |   600/ 1277 batches | lr 1.00 | ms/batch 16.54 | loss  5.29 | ppl   197.99\n","| epoch  28 |   800/ 1277 batches | lr 1.00 | ms/batch 16.52 | loss  5.26 | ppl   192.59\n","| epoch  28 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.57 | loss  5.28 | ppl   197.25\n","| epoch  28 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.29 | ppl   198.41\n","| end of epoch  28 | time: 21.26s | valid loss  5.28 | valid ppl   197.20\n","| epoch  29 |   200/ 1277 batches | lr 1.00 | ms/batch 16.74 | loss  5.35 | ppl   210.10\n","| epoch  29 |   400/ 1277 batches | lr 1.00 | ms/batch 16.77 | loss  5.29 | ppl   198.24\n","| epoch  29 |   600/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.28 | ppl   196.35\n","| epoch  29 |   800/ 1277 batches | lr 1.00 | ms/batch 16.49 | loss  5.25 | ppl   191.15\n","| epoch  29 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  5.27 | ppl   195.29\n","| epoch  29 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.54 | loss  5.28 | ppl   195.84\n","| end of epoch  29 | time: 21.31s | valid loss  5.28 | valid ppl   196.85\n","| epoch  30 |   200/ 1277 batches | lr 1.00 | ms/batch 16.68 | loss  5.34 | ppl   208.09\n","| epoch  30 |   400/ 1277 batches | lr 1.00 | ms/batch 16.65 | loss  5.28 | ppl   195.88\n","| epoch  30 |   600/ 1277 batches | lr 1.00 | ms/batch 16.51 | loss  5.27 | ppl   193.79\n","| epoch  30 |   800/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.24 | ppl   189.28\n","| epoch  30 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.27 | ppl   193.75\n","| epoch  30 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.60 | loss  5.27 | ppl   194.14\n","| end of epoch  30 | time: 21.29s | valid loss  5.26 | valid ppl   193.21\n","| epoch  31 |   200/ 1277 batches | lr 1.00 | ms/batch 16.57 | loss  5.33 | ppl   206.02\n","| epoch  31 |   400/ 1277 batches | lr 1.00 | ms/batch 16.65 | loss  5.27 | ppl   194.47\n","| epoch  31 |   600/ 1277 batches | lr 1.00 | ms/batch 16.55 | loss  5.26 | ppl   191.52\n","| epoch  31 |   800/ 1277 batches | lr 1.00 | ms/batch 16.56 | loss  5.23 | ppl   186.74\n","| epoch  31 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.54 | loss  5.25 | ppl   191.04\n","| epoch  31 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.54 | loss  5.26 | ppl   192.25\n","| end of epoch  31 | time: 21.26s | valid loss  5.25 | valid ppl   190.32\n","| epoch  32 |   200/ 1277 batches | lr 1.00 | ms/batch 16.67 | loss  5.32 | ppl   204.08\n","| epoch  32 |   400/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  5.26 | ppl   191.70\n","| epoch  32 |   600/ 1277 batches | lr 1.00 | ms/batch 16.53 | loss  5.24 | ppl   189.48\n","| epoch  32 |   800/ 1277 batches | lr 1.00 | ms/batch 16.70 | loss  5.22 | ppl   185.00\n","| epoch  32 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.65 | loss  5.24 | ppl   189.50\n","| epoch  32 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.60 | loss  5.25 | ppl   190.45\n","| end of epoch  32 | time: 21.33s | valid loss  5.25 | valid ppl   189.87\n","| epoch  33 |   200/ 1277 batches | lr 1.00 | ms/batch 16.81 | loss  5.31 | ppl   202.59\n","| epoch  33 |   400/ 1277 batches | lr 1.00 | ms/batch 16.68 | loss  5.25 | ppl   190.07\n","| epoch  33 |   600/ 1277 batches | lr 1.00 | ms/batch 16.50 | loss  5.24 | ppl   188.01\n","| epoch  33 |   800/ 1277 batches | lr 1.00 | ms/batch 16.58 | loss  5.21 | ppl   183.16\n","| epoch  33 |  1000/ 1277 batches | lr 1.00 | ms/batch 16.47 | loss  5.24 | ppl   188.49\n","| epoch  33 |  1200/ 1277 batches | lr 1.00 | ms/batch 16.45 | loss  5.24 | ppl   188.19\n","| end of epoch  33 | time: 21.30s | valid loss  5.25 | valid ppl   190.46\n","| epoch  34 |   200/ 1277 batches | lr 0.50 | ms/batch 16.62 | loss  5.30 | ppl   199.86\n","| epoch  34 |   400/ 1277 batches | lr 0.50 | ms/batch 16.52 | loss  5.23 | ppl   186.41\n","| epoch  34 |   600/ 1277 batches | lr 0.50 | ms/batch 16.47 | loss  5.22 | ppl   184.05\n","| epoch  34 |   800/ 1277 batches | lr 0.50 | ms/batch 16.55 | loss  5.19 | ppl   179.27\n","| epoch  34 |  1000/ 1277 batches | lr 0.50 | ms/batch 16.59 | loss  5.21 | ppl   183.72\n","| epoch  34 |  1200/ 1277 batches | lr 0.50 | ms/batch 16.56 | loss  5.20 | ppl   181.79\n","| end of epoch  34 | time: 21.25s | valid loss  5.19 | valid ppl   179.75\n","| epoch  35 |   200/ 1277 batches | lr 0.50 | ms/batch 16.70 | loss  5.28 | ppl   196.53\n","| epoch  35 |   400/ 1277 batches | lr 0.50 | ms/batch 16.69 | loss  5.22 | ppl   184.62\n","| epoch  35 |   600/ 1277 batches | lr 0.50 | ms/batch 16.46 | loss  5.21 | ppl   182.70\n","| epoch  35 |   800/ 1277 batches | lr 0.50 | ms/batch 16.64 | loss  5.18 | ppl   176.92\n","| epoch  35 |  1000/ 1277 batches | lr 0.50 | ms/batch 16.59 | loss  5.20 | ppl   181.95\n","| epoch  35 |  1200/ 1277 batches | lr 0.50 | ms/batch 16.56 | loss  5.20 | ppl   182.11\n","| end of epoch  35 | time: 21.32s | valid loss  5.18 | valid ppl   178.41\n","| epoch  36 |   200/ 1277 batches | lr 0.50 | ms/batch 16.65 | loss  5.28 | ppl   195.72\n","| epoch  36 |   400/ 1277 batches | lr 0.50 | ms/batch 16.56 | loss  5.21 | ppl   183.26\n","| epoch  36 |   600/ 1277 batches | lr 0.50 | ms/batch 16.59 | loss  5.20 | ppl   180.52\n","| epoch  36 |   800/ 1277 batches | lr 0.50 | ms/batch 16.64 | loss  5.17 | ppl   175.79\n","| epoch  36 |  1000/ 1277 batches | lr 0.50 | ms/batch 16.60 | loss  5.20 | ppl   180.62\n","| epoch  36 |  1200/ 1277 batches | lr 0.50 | ms/batch 16.61 | loss  5.20 | ppl   180.82\n","| end of epoch  36 | time: 21.33s | valid loss  5.17 | valid ppl   176.71\n","| epoch  37 |   200/ 1277 batches | lr 0.50 | ms/batch 16.65 | loss  5.27 | ppl   193.84\n","| epoch  37 |   400/ 1277 batches | lr 0.50 | ms/batch 16.61 | loss  5.21 | ppl   182.83\n","| epoch  37 |   600/ 1277 batches | lr 0.50 | ms/batch 16.54 | loss  5.20 | ppl   180.65\n","| epoch  37 |   800/ 1277 batches | lr 0.50 | ms/batch 16.47 | loss  5.17 | ppl   175.40\n","| epoch  37 |  1000/ 1277 batches | lr 0.50 | ms/batch 16.54 | loss  5.20 | ppl   180.53\n","| epoch  37 |  1200/ 1277 batches | lr 0.50 | ms/batch 16.55 | loss  5.19 | ppl   179.60\n","| end of epoch  37 | time: 21.26s | valid loss  5.17 | valid ppl   176.23\n","| epoch  38 |   200/ 1277 batches | lr 0.50 | ms/batch 16.61 | loss  5.26 | ppl   192.45\n","| epoch  38 |   400/ 1277 batches | lr 0.50 | ms/batch 16.63 | loss  5.20 | ppl   181.31\n","| epoch  38 |   600/ 1277 batches | lr 0.50 | ms/batch 16.65 | loss  5.19 | ppl   179.11\n","| epoch  38 |   800/ 1277 batches | lr 0.50 | ms/batch 16.57 | loss  5.16 | ppl   174.45\n","| epoch  38 |  1000/ 1277 batches | lr 0.50 | ms/batch 16.49 | loss  5.19 | ppl   179.13\n","| epoch  38 |  1200/ 1277 batches | lr 0.50 | ms/batch 16.52 | loss  5.19 | ppl   179.38\n","| end of epoch  38 | time: 21.28s | valid loss  5.17 | valid ppl   176.63\n","| epoch  39 |   200/ 1277 batches | lr 0.25 | ms/batch 16.53 | loss  5.26 | ppl   193.16\n","| epoch  39 |   400/ 1277 batches | lr 0.25 | ms/batch 16.55 | loss  5.20 | ppl   180.60\n","| epoch  39 |   600/ 1277 batches | lr 0.25 | ms/batch 16.54 | loss  5.19 | ppl   178.80\n","| epoch  39 |   800/ 1277 batches | lr 0.25 | ms/batch 16.49 | loss  5.16 | ppl   173.34\n","| epoch  39 |  1000/ 1277 batches | lr 0.25 | ms/batch 16.51 | loss  5.18 | ppl   177.84\n","| epoch  39 |  1200/ 1277 batches | lr 0.25 | ms/batch 16.53 | loss  5.17 | ppl   176.02\n","| end of epoch  39 | time: 21.23s | valid loss  5.15 | valid ppl   172.90\n","| epoch  40 |   200/ 1277 batches | lr 0.25 | ms/batch 16.65 | loss  5.25 | ppl   191.43\n","| epoch  40 |   400/ 1277 batches | lr 0.25 | ms/batch 16.80 | loss  5.19 | ppl   179.19\n","| epoch  40 |   600/ 1277 batches | lr 0.25 | ms/batch 16.71 | loss  5.18 | ppl   177.41\n","| epoch  40 |   800/ 1277 batches | lr 0.25 | ms/batch 16.60 | loss  5.15 | ppl   172.32\n","| epoch  40 |  1000/ 1277 batches | lr 0.25 | ms/batch 16.64 | loss  5.17 | ppl   176.78\n","| epoch  40 |  1200/ 1277 batches | lr 0.25 | ms/batch 16.71 | loss  5.17 | ppl   175.93\n","| end of epoch  40 | time: 21.40s | valid loss  5.15 | valid ppl   171.90\n","| epoch  41 |   200/ 1277 batches | lr 0.25 | ms/batch 16.62 | loss  5.25 | ppl   190.64\n","| epoch  41 |   400/ 1277 batches | lr 0.25 | ms/batch 16.71 | loss  5.19 | ppl   179.06\n","| epoch  41 |   600/ 1277 batches | lr 0.25 | ms/batch 16.64 | loss  5.17 | ppl   176.73\n","| epoch  41 |   800/ 1277 batches | lr 0.25 | ms/batch 16.54 | loss  5.15 | ppl   172.52\n","| epoch  41 |  1000/ 1277 batches | lr 0.25 | ms/batch 16.64 | loss  5.18 | ppl   177.00\n","| epoch  41 |  1200/ 1277 batches | lr 0.25 | ms/batch 16.54 | loss  5.17 | ppl   176.52\n","| end of epoch  41 | time: 21.32s | valid loss  5.15 | valid ppl   172.47\n","| epoch  42 |   200/ 1277 batches | lr 0.12 | ms/batch 16.61 | loss  5.25 | ppl   190.79\n","| epoch  42 |   400/ 1277 batches | lr 0.12 | ms/batch 16.90 | loss  5.19 | ppl   179.06\n","| epoch  42 |   600/ 1277 batches | lr 0.12 | ms/batch 16.56 | loss  5.18 | ppl   178.07\n","| epoch  42 |   800/ 1277 batches | lr 0.12 | ms/batch 16.61 | loss  5.16 | ppl   173.31\n","| epoch  42 |  1000/ 1277 batches | lr 0.12 | ms/batch 16.59 | loss  5.18 | ppl   176.99\n","| epoch  42 |  1200/ 1277 batches | lr 0.12 | ms/batch 16.55 | loss  5.16 | ppl   174.77\n","| end of epoch  42 | time: 21.36s | valid loss  5.15 | valid ppl   172.05\n","| epoch  43 |   200/ 1277 batches | lr 0.06 | ms/batch 16.70 | loss  5.26 | ppl   192.57\n","| epoch  43 |   400/ 1277 batches | lr 0.06 | ms/batch 16.80 | loss  5.19 | ppl   180.29\n","| epoch  43 |   600/ 1277 batches | lr 0.06 | ms/batch 16.92 | loss  5.19 | ppl   178.80\n","| epoch  43 |   800/ 1277 batches | lr 0.06 | ms/batch 16.77 | loss  5.16 | ppl   174.86\n","| epoch  43 |  1000/ 1277 batches | lr 0.06 | ms/batch 16.96 | loss  5.18 | ppl   177.48\n","| epoch  43 |  1200/ 1277 batches | lr 0.06 | ms/batch 16.91 | loss  5.17 | ppl   175.30\n","| end of epoch  43 | time: 21.61s | valid loss  5.16 | valid ppl   173.98\n","| epoch  44 |   200/ 1277 batches | lr 0.03 | ms/batch 16.86 | loss  5.27 | ppl   194.25\n","| epoch  44 |   400/ 1277 batches | lr 0.03 | ms/batch 16.75 | loss  5.21 | ppl   182.96\n","| epoch  44 |   600/ 1277 batches | lr 0.03 | ms/batch 16.99 | loss  5.20 | ppl   181.12\n","| epoch  44 |   800/ 1277 batches | lr 0.03 | ms/batch 16.87 | loss  5.18 | ppl   177.68\n","| epoch  44 |  1000/ 1277 batches | lr 0.03 | ms/batch 16.77 | loss  5.19 | ppl   179.27\n","| epoch  44 |  1200/ 1277 batches | lr 0.03 | ms/batch 16.78 | loss  5.17 | ppl   175.72\n","| end of epoch  44 | time: 21.61s | valid loss  5.17 | valid ppl   175.41\n","| epoch  45 |   200/ 1277 batches | lr 0.02 | ms/batch 16.87 | loss  5.28 | ppl   196.22\n","| epoch  45 |   400/ 1277 batches | lr 0.02 | ms/batch 16.63 | loss  5.22 | ppl   184.73\n","| epoch  45 |   600/ 1277 batches | lr 0.02 | ms/batch 16.65 | loss  5.21 | ppl   182.54\n","| epoch  45 |   800/ 1277 batches | lr 0.02 | ms/batch 16.56 | loss  5.19 | ppl   179.36\n","| epoch  45 |  1000/ 1277 batches | lr 0.02 | ms/batch 16.57 | loss  5.20 | ppl   180.69\n","| epoch  45 |  1200/ 1277 batches | lr 0.02 | ms/batch 16.56 | loss  5.17 | ppl   176.48\n","| end of epoch  45 | time: 21.37s | valid loss  5.17 | valid ppl   175.26\n","| epoch  46 |   200/ 1277 batches | lr 0.01 | ms/batch 16.75 | loss  5.29 | ppl   197.43\n","| epoch  46 |   400/ 1277 batches | lr 0.01 | ms/batch 16.59 | loss  5.22 | ppl   185.82\n","| epoch  46 |   600/ 1277 batches | lr 0.01 | ms/batch 16.85 | loss  5.21 | ppl   183.65\n","| epoch  46 |   800/ 1277 batches | lr 0.01 | ms/batch 16.66 | loss  5.19 | ppl   179.75\n","| epoch  46 |  1000/ 1277 batches | lr 0.01 | ms/batch 16.80 | loss  5.20 | ppl   181.60\n","| epoch  46 |  1200/ 1277 batches | lr 0.01 | ms/batch 16.66 | loss  5.17 | ppl   175.82\n","| end of epoch  46 | time: 21.47s | valid loss  5.16 | valid ppl   173.90\n","| epoch  47 |   200/ 1277 batches | lr 0.00 | ms/batch 16.71 | loss  5.29 | ppl   198.62\n","| epoch  47 |   400/ 1277 batches | lr 0.00 | ms/batch 16.73 | loss  5.23 | ppl   186.68\n","| epoch  47 |   600/ 1277 batches | lr 0.00 | ms/batch 16.67 | loss  5.22 | ppl   184.06\n","| epoch  47 |   800/ 1277 batches | lr 0.00 | ms/batch 16.63 | loss  5.18 | ppl   178.49\n","| epoch  47 |  1000/ 1277 batches | lr 0.00 | ms/batch 16.71 | loss  5.20 | ppl   180.75\n","| epoch  47 |  1200/ 1277 batches | lr 0.00 | ms/batch 16.88 | loss  5.17 | ppl   175.83\n","| end of epoch  47 | time: 21.47s | valid loss  5.15 | valid ppl   172.87\n","| epoch  48 |   200/ 1277 batches | lr 0.00 | ms/batch 16.87 | loss  5.29 | ppl   198.71\n","| epoch  48 |   400/ 1277 batches | lr 0.00 | ms/batch 16.88 | loss  5.23 | ppl   186.94\n","| epoch  48 |   600/ 1277 batches | lr 0.00 | ms/batch 16.79 | loss  5.21 | ppl   183.25\n","| epoch  48 |   800/ 1277 batches | lr 0.00 | ms/batch 16.70 | loss  5.19 | ppl   178.75\n","| epoch  48 |  1000/ 1277 batches | lr 0.00 | ms/batch 16.73 | loss  5.20 | ppl   180.81\n","| epoch  48 |  1200/ 1277 batches | lr 0.00 | ms/batch 16.74 | loss  5.17 | ppl   176.26\n","| end of epoch  48 | time: 21.57s | valid loss  5.15 | valid ppl   172.24\n","| epoch  49 |   200/ 1277 batches | lr 0.00 | ms/batch 16.86 | loss  5.29 | ppl   197.73\n","| epoch  49 |   400/ 1277 batches | lr 0.00 | ms/batch 16.68 | loss  5.23 | ppl   186.44\n","| epoch  49 |   600/ 1277 batches | lr 0.00 | ms/batch 16.72 | loss  5.21 | ppl   183.84\n","| epoch  49 |   800/ 1277 batches | lr 0.00 | ms/batch 16.79 | loss  5.18 | ppl   178.22\n","| epoch  49 |  1000/ 1277 batches | lr 0.00 | ms/batch 16.77 | loss  5.20 | ppl   181.43\n","| epoch  49 |  1200/ 1277 batches | lr 0.00 | ms/batch 16.72 | loss  5.17 | ppl   176.25\n","| end of epoch  49 | time: 21.50s | valid loss  5.15 | valid ppl   171.93\n","| epoch  50 |   200/ 1277 batches | lr 0.00 | ms/batch 16.66 | loss  5.29 | ppl   197.61\n","| epoch  50 |   400/ 1277 batches | lr 0.00 | ms/batch 16.70 | loss  5.23 | ppl   186.72\n","| epoch  50 |   600/ 1277 batches | lr 0.00 | ms/batch 16.66 | loss  5.22 | ppl   184.23\n","| epoch  50 |   800/ 1277 batches | lr 0.00 | ms/batch 16.84 | loss  5.18 | ppl   178.35\n","| epoch  50 |  1000/ 1277 batches | lr 0.00 | ms/batch 16.78 | loss  5.20 | ppl   181.02\n","| epoch  50 |  1200/ 1277 batches | lr 0.00 | ms/batch 16.90 | loss  5.18 | ppl   177.35\n","| end of epoch  50 | time: 21.52s | valid loss  5.15 | valid ppl   171.75\n","| End of training | test loss  5.10 | test ppl   163.48\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SuJg2v_nAO9z"},"source":["import torch\n","\n","torch.manual_seed(1111)\n","device = torch.device(\"cuda\" if True else \"cpu\")\n","with open('jpn_model_t.pt', 'rb') as f:\n","    model = torch.load(f).to(device)\n","model.eval()\n","corpus = Corpus('./')\n","ntokens = len(corpus.dictionary)\n","input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device) #先頭単語をランダムに選ぶ\n","with torch.no_grad():  # no tracking history\n","    sentence = '' #生成文\n","    for i in range(500):\n","        output = model(input)  #outputは語彙サイズの確率ベクトル[[...]]\n","        word_weights = output.squeeze().div(1.0).exp().cpu() #語彙サイズのEXP(x)\n","        word_idx = torch.multinomial(word_weights, 1)[0] #確率が一番大きな単語のIndexをとる\n","        input.fill_(word_idx) #この予測単語を、次のループ時のinputに設定\n","        word = corpus.dictionary.idx2word[word_idx]\n","        sentence += (word  + ('\\n' if i % 20 == 19 else ' '))\n","    print(sentence)"],"execution_count":null,"outputs":[]}]}