{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rnn_languagemodel.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lyWSbiXS56OF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622775261847,"user_tz":-540,"elapsed":19354,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"a1f234d2-1aed-4496-8873-06d092896323"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cuksv4N3ErPv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622775281170,"user_tz":-540,"elapsed":240,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"908daea1-99ce-40de-cfcc-777819ffb3d5"},"source":["!pwd\n","%cd /content/drive/My Drive/Colab Notebooks/NLP20211H/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/My Drive/Colab Notebooks/NLP20211H\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ac66V1yHO7V_"},"source":["#データ準備"]},{"cell_type":"code","metadata":{"id":"oR6c2n5eT8Jj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622775308561,"user_tz":-540,"elapsed":24214,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"e6851764-36b0-4b93-d12a-f42eb813885a"},"source":["!pip install mecab-python3\n","!pip install unidic-lite"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting mecab-python3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/72/20f8f60b858556fdff6c0376b480c230e594621fff8be780603ac9c47f6a/mecab_python3-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (487kB)\n","\r\u001b[K     |▊                               | 10kB 23.3MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 30.1MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 24.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 18.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 9.9MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 8.9MB/s eta 0:00:01\r\u001b[K     |████▊                           | 71kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 81kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 102kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 112kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 122kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 133kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 143kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 153kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 163kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 174kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 184kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 194kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 204kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 215kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 225kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 235kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 245kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 256kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 266kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 276kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 286kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 296kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 307kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 317kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 327kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 337kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 348kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 358kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 368kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 378kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 389kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 399kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 409kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 419kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 430kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 440kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 450kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 460kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 471kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 481kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 491kB 9.6MB/s \n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-1.0.3\n","Collecting unidic-lite\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/2b/8cf7514cb57d028abcef625afa847d60ff1ffbf0049c36b78faa7c35046f/unidic-lite-1.0.8.tar.gz (47.4MB)\n","\u001b[K     |████████████████████████████████| 47.4MB 65kB/s \n","\u001b[?25hBuilding wheels for collected packages: unidic-lite\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-cp37-none-any.whl size=47658838 sha256=e786cf5ae09d82e0750c00fd58a3a7412f20c5319aaab3b9495fa3baf53ded2c\n","  Stored in directory: /root/.cache/pip/wheels/20/48/8d/b66d8361a27f58f41ec86640e4fd2640de0403a6367511eab7\n","Successfully built unidic-lite\n","Installing collected packages: unidic-lite\n","Successfully installed unidic-lite-1.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"crHqX2waUIkd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622775429687,"user_tz":-540,"elapsed":16289,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"adfd6805-0740-44a2-d72c-c7804c3542d0"},"source":["#　Reference https://www.pytry3g.com/entry/gensim-word2veReference \n","import MeCab\n","import codecs\n","import urllib.parse as parser\n","import urllib.request as request\n","from bs4 import BeautifulSoup\n","\n","tagger = MeCab.Tagger(\"-Owakati\")\n","link = \"https://ja.wikipedia.org/wiki/\"\n","\n","#+\n","#キーワードのリストを受け取り、Wikipediaの該当記事を取得して、fileに出力する。\n","#-\n","def kwd2file(kwd,file):\n","  corpus = []\n","  for word in keyword:\n","      with request.urlopen(link + parser.quote_plus(word)) as response:\n","          html = response.read().decode('utf-8')\n","          soup = BeautifulSoup(html, \"lxml\")\n","          p_tags = soup.find_all('p')\n","          for p in p_tags:\n","              corpus.append(tagger.parse(p.text).strip())\n","  print(\"corpus size {}\".format(len(corpus)))\n","  print(\"corpus samples {}\".format(corpus[0:5]))\n","  with codecs.open(file, \"w\", \"utf-8\") as f:\n","      f.write(\"\\n\".join(corpus))\n","\n","keyword = [\n","           \"ロボット\",\"ロケット\",\"コンピュータ\",\"人工知能\",\"自動車\",\"宇宙\",\"機械\",\"道具\",\n","           \"人間\",\"目\",\"口\",\"耳\",\"指\",\"手\",\"足\",\"頭\",\"首\",\"声\",\"腹\",\"背\",\"胃\",\"腸\",\n","           \"動物\",\"植物\",\"猫\",\"犬\",\"鳩\",\"魚\",\"鳥\",\"昆虫\",\"猿\",\"烏\",\"ミミズ\",\"ナメクジ\",\"蟻\",\"熊\",\"クジラ\",\"イルカ\",\n","           \"日本\",\"東京\",\"横浜\",\"アジア\",\"アメリカ\",\"アフリカ\",\"中近東\",\"オセアニア\",\"関東\",\"関西\",\n","           \"電気\",\"信号\",\"電波\",\"重力\",\"原子\",\"分子\",\"量子\",\n","           \"紙\",\"プラスチック\",\"鉄\",\"アルミ\",\"ビニール\",\"土\",\"火\",\"水\",\n","           \"聖徳太子\",\"坂本龍馬\",\"徳川家康\",\"源義経\",\"夏目漱石\",\"宮沢賢治\",\"斎藤茂吉\",\n","           \"サッカー\",\"野球\",\"ラグビー\",\"プロレス\",\"テニス\",\"バレーボール\",\"ホッケー\",\"スキー\",\"スケート\",\n","           ]\n","kwd2file(keyword,'train.txt')\n","keyword = [\"ロボット\",] \n","kwd2file(keyword,'test.txt') #客観的に性能を示すため、本来、トレーニングデータと全く異なるデータを用いる。\n","keyword = [\"ロケット\",]\n","kwd2file(keyword,'valid.txt') #トレーニングの効果を見るため、本来、トレーニングと別データ。トレーニング用のデータを10等分して、順番に一つを使ったりする。\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["corpus size 4619\n","corpus samples ['ロボット （ robot ） は 、 人 の 代わり に 何 等 か の 作業 を 自律 的 に 行う 装置 、 もしくは 機械 の こと 。', '主に 以下 に 大別 する こと が 可能 で ある 。', '近年 で は 無人 機 「 ドローン 」 を 半ば 自律 化 さ せ た もの [ 1 ] も 存在 し 、 自動 運転 車 の 実現 が 視野 に 入っ て き て おり 、 SF の 世界 が 現実 の もの と なり つつ ある 。', '生命 体 に 通常 以上 の 力 を 発揮 さ せる 方策 と し て 何 ら か の 人工 物 を 埋め込ん だり 置き換える など の 方策 を 採っ た 者 は 一般 に 「 サイボーグ 」 など と 呼ば れ 区別 さ れる こと が 多い 。', 'この 言葉 が 初めて 用い られ た の は 、 1920 年 に チェコスロバキア （ 当時 ） の 小説 家 カレル ・ チャペック が 発表 し た 戯曲 『 R . U . R . （ ロッサム 万能 ロボット 商会 ） 』 に おい て で ある が 、 この 作品 で は 現在 認知 さ れ て いる 金属 製 の 機械 で は なく 、 人間 と は 異なる 組成 の 肉体 と 人間 そっくり の 外見 を 持つ もの を 、 化学 的 合成 で 原形 質 を 使っ て 製作 し た もの で 、 現在 の SF で 言う バイオ ノ イド で ある 。']\n","corpus size 49\n","corpus samples ['ロボット （ robot ） は 、 人 の 代わり に 何 等 か の 作業 を 自律 的 に 行う 装置 、 もしくは 機械 の こと 。', '主に 以下 に 大別 する こと が 可能 で ある 。', '近年 で は 無人 機 「 ドローン 」 を 半ば 自律 化 さ せ た もの [ 1 ] も 存在 し 、 自動 運転 車 の 実現 が 視野 に 入っ て き て おり 、 SF の 世界 が 現実 の もの と なり つつ ある 。', '生命 体 に 通常 以上 の 力 を 発揮 さ せる 方策 と し て 何 ら か の 人工 物 を 埋め込ん だり 置き換える など の 方策 を 採っ た 者 は 一般 に 「 サイボーグ 」 など と 呼ば れ 区別 さ れる こと が 多い 。', 'この 言葉 が 初めて 用い られ た の は 、 1920 年 に チェコスロバキア （ 当時 ） の 小説 家 カレル ・ チャペック が 発表 し た 戯曲 『 R . U . R . （ ロッサム 万能 ロボット 商会 ） 』 に おい て で ある が 、 この 作品 で は 現在 認知 さ れ て いる 金属 製 の 機械 で は なく 、 人間 と は 異なる 組成 の 肉体 と 人間 そっくり の 外見 を 持つ もの を 、 化学 的 合成 で 原形 質 を 使っ て 製作 し た もの で 、 現在 の SF で 言う バイオ ノ イド で ある 。']\n","corpus size 87\n","corpus samples ['ロケット （ 英 : rocket ） は 、 自ら の 質量 の 一部 を 後方 に 射出 し 、 その 反 作用 で 進む 力 （ 推力 ） を 得る 装置 （ ロケット エンジン ） 、 もしくは その 推力 を 利用 し て 移動 する 装置 で ある 。 外気 から 酸化 剤 を 取り込む 物 （ ジェット エンジン ） は 除く 。', '狭義 に は ロケット エンジン 自体 を いう が 、 先端 部 に 人工 衛星 や 宇宙 探査 機 など の ペイロード を 搭載 し て 宇宙 空間 の 特定 の 軌道 に 投入 さ せる 手段 と し て 使わ れる 、 ロケット エンジン を 推進 力 と する ローンチ ・ ヴィークル 全体 を ロケット と いう こと も 多い 。 日本 で は 、 地上 から 照射 さ れ た マイクロ 波 や レーザー ビーム を リフレクター で 反射 し 、 空気 の 電離 に よる プラズマ 発生 時 の 爆発 など を 推進 力 と し 、 燃料 を 使わ ない ローンチ ・ ヴィークル も 「 ロケット 」 と 呼ば れる [ 1 ]。', 'なお 、 推力 を 得る ため に 射出さ れる 質量 （ 推進 剤 、 プロペラント ） が 何 か 、 それ ら を 動かす エネルギー は 何 から 得る か に より 、 ロケット は 様々 な 方式 に 分類 さ れる が 、 ここ で は 最も 一般 的 に 使わ れ て いる 化学 ロケット （ 化学 燃料 ロケット ） を 中心 に 述べる 。', 'また 、 ロケット の 先端 部 に 核 弾頭 や 爆薬 など 軍用 の ペイロード を 搭載 し て 標的 や 目的 地 に 着弾 さ せる 兵器 は 、 日本 で は 無 誘導 の 場合 は 「 ロケット 弾 」 、 誘導 装置 を 持つ もの は ミサイル と し て 区別 さ れる （ 「 ロケット 弾 」 を 参照 ） 。 特に 弾道 飛行 を し て 目的 地 に 着弾 さ せる ミサイル は 、 弾道 ミサイル と し て 区別 し て いる 。 なお 、 北朝鮮 に よる 人工 衛星 の 打ち上げ は 、 国際 社会 から 事実 上 の 弾道 ミサイル 発射 実験 と 見なさ れ て おり 、 国際 連合 安全 保障 理事 会 決議 1718 と 1874 と 2087 で も 禁止 さ れ て いる ため 、 特に 日本 国 内 に おい て は 、 人工 衛星 打ち上げ で あっ て も ロケット で は なく ミサイル と 報道 さ れ て いる 。 また 他国 で は ミサイル と さ れる ところ を 、 ロケット や その 類語 で 呼称 する 国 も ある （ 「 ロシア 戦略 ロケット 軍 」 「 中国 人民 解放 軍 ロケット 軍 」 を 参照 ） 。', 'ロケット の 語源 は 、 1379 年 に イタリア の 芸術 家 兼 技術 者 で ある ムラ トーリ [ 注 1 ] が 西欧 で 初めて 火薬 推進 式 の ロケット を 作り 、 それ を 形状 に ちなん で 「 ロッケッタ 」 （ 伊 : Rocchetta 、 「 小さな 糸巻 棒 」 の 意 ） と 名づけ た こと に よる 。']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9s6rqmBrQl5H","executionInfo":{"status":"ok","timestamp":1622775471727,"user_tz":-540,"elapsed":3639,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}}},"source":["import os\n","from io import open\n","import torch\n","\n","class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = []\n","\n","    #+\n","    #単語を受け取り、idx2word, word2idxを作成する\n","    #-\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)\n","\n","class Corpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","\n","    def tokenize(self, path):\n","        #辞書に単語を登録する\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                #print(words)\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","        #コーパスを単語Indexのリストに変換する\n","        with open(path, 'r', encoding=\"utf8\") as f:\n","            idss = []\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                ids = []\n","                for word in words:\n","                    ids.append(self.dictionary.word2idx[word])\n","                idss.append(torch.tensor(ids).type(torch.int64))\n","            ids = torch.cat(idss)\n","        return ids"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0Ar4v2TfUR-"},"source":["#LSTM"]},{"cell_type":"code","metadata":{"id":"fqtkhkQhkFAU","executionInfo":{"status":"ok","timestamp":1622775477960,"user_tz":-540,"elapsed":222,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}}},"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","#+\n","#LSTMあるいはGRUを使ったモデル定義\n","#-\n","class RNNModel(nn.Module):\n","\n","    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n","        super(RNNModel, self).__init__()\n","        self.ntoken = ntoken\n","        self.drop = nn.Dropout(dropout)\n","        self.encoder = nn.Embedding(ntoken, ninp) #語彙数x埋め込みサイズの表\n","        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout) #LSTMのオブジェクト\n","        self.decoder = nn.Linear(nhid, ntoken) #隠れ状態から語彙サイズへの全結合\n","        self.init_weights()\n","        self.nhid = nhid #隠れ状態ベクトル次元数\n","        self.nlayers = nlayers #LSTM/GRUを何枚重ねるか\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n","        nn.init.zeros_(self.decoder.weight)\n","        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n","\n","    def forward(self, input, hidden):\n","        #以下、バッチサイズだけ一挙に計算している\n","        emb = self.drop(self.encoder(input)) #入力単語の埋め込み表現を取り出して、ドロップアウト\n","        output, hidden = self.rnn(emb, hidden) #BPTT_SEQ_LENGTH個分の系列を2レイヤーLSTMにかける\n","        decoded = self.decoder(self.drop(output)) #出力をDropoutかまして語彙サイズへ全結合\n","        decoded = decoded.view(-1, self.ntoken) #shapeを[[...]]に\n","        return F.log_softmax(decoded, dim=1), hidden #softmwaxの対数と、新しい隠れ状態ベクトルを返す\n","\n","    def init_hidden(self, bsz):\n","        weight = next(self.parameters()) #最初のパラメータ（埋め込み表）から使っている型を取得\n","        return (weight.new_zeros(self.nlayers, bsz, self.nhid), #同じ型で0初期化したテンソル(2,20,200)を返す\n","                    weight.new_zeros(self.nlayers, bsz, self.nhid)) #2個目のレイヤ用\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"YrQ-hZ--kqgF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622775612132,"user_tz":-540,"elapsed":130142,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"12ba8f6f-8615-4c72-9d6f-9bac34b37e5f"},"source":["# coding: utf-8\n","import time\n","import math\n","import os\n","import torch\n","import torch.nn as nn\n","import easydict\n","\n","torch.manual_seed(1111) #乱数の初期値設定、結果を安定させる\n","device = torch.device(\"cuda\" if True else \"cpu\")\n","corpus = Corpus('./')\n","BATCH_SIZE = 20\n","EVAL_BATCH_SIZE = 10\n","BPTT_SEQ_LENGTH = 35 #このサイズで逆伝播を停止する。入力が長い場合の対策\n","\n","def batchify(data, bsz):\n","    print('batchfy input size {}'.format(data.size()))\n","    nbatch = data.size(0) // bsz #コーパスの長さをミニバッチサイズで割り、何個のミニバッチかを得る\n","    data = data.narrow(0, 0, nbatch * bsz) #ミニバッチにハマらなかった部分を捨てる\n","    data = data.view(bsz, -1).t().contiguous() #データをミニバッチサイズで分割して、行列を入れ替える\n","    #->shapeが(バッチの個数,バッチサイズ)になる、train_dataなら23154行、20列の行列になる、\n","    #連続した単語は列方向に並んでいる\n","    return data.to(device)\n","\n","#データをミニバッチに分ける\n","train_data = batchify(corpus.train, BATCH_SIZE)\n","val_data = batchify(corpus.valid, EVAL_BATCH_SIZE)\n","test_data = batchify(corpus.test, EVAL_BATCH_SIZE)\n","#print('train_data.size {}'.format(train_data.size()))\n","\n","ntokens = len(corpus.dictionary)\n","model = RNNModel(\n","    len(corpus.dictionary),\n","    200,#埋め込みサイズ \n","    200,#隠れベクトルサイズ \n","    2,#RNNをスタックするレイヤ数, \n","    0.2,#ドロップアウト率。複数レイヤの場合、0.2の割合で次段の入力を0にする\n","    ).to(device)\n","#print(model)\n","#print(model.state_dict().keys())\n","criterion = nn.NLLLoss()\n","\n","#BPTT_SEQ_LENGTHの区切りで、隠れ状態ベクトルから、計算履歴を消去して、逆伝播させないようにする\n","def repackage_hidden(h):\n","    if isinstance(h, torch.Tensor):\n","        return h.detach()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)\n","\n","def get_batch(source, i):\n","    #iには、0から順番にBPTT_SEQ_LENGTHだけ飛んだ値が入ってくる\n","    seq_len = min(BPTT_SEQ_LENGTH, len(source) - 1 - i)\n","    data = source[i:i+seq_len] #列方向からBPTT_SEQ_LENGTHだけ切り出す、連続単語の認識はこの長さの範囲だけで実施\n","    target = source[i+1:i+1+seq_len].view(-1) #正解単語になるように1個後ろにずらしてBPTT_SEQ_LENGTHだけ切り出す\n","    return data, target\n","\n","def train():\n","    model.train() \n","    total_loss = 0.\n","    start_time = time.time()\n","    ntokens = len(corpus.dictionary)\n","    hidden = model.init_hidden(BATCH_SIZE) #隠れ状態初期化\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, BPTT_SEQ_LENGTH)): #BPTT_SEQ_LENGTH分取り出して繰り返す\n","        data, targets = get_batch(train_data, i) #shapeが(BPTT_SEQ_LENGTH,BATCH_SIZE)のデータを得る\n","        #->dataは、35行20列の行列。連続した単語は縦（列方向）に並んでいる。横（行方向）には独立したデータが20個並んでいる\n","        model.zero_grad() #勾配初期化\n","        hidden = repackage_hidden(hidden) #隠れ状態の逆伝播を切る\n","        output, hidden = model(data, hidden) #モデルに通す\n","        loss = criterion(output, targets) #ロスを求める\n","        loss.backward() #逆伝播\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25) #勾配の最大が0.25になるように、全勾配に調整かける\n","        for p in model.parameters(): #学習レート低減\n","            p.data.add_(-lr, p.grad)\n","        total_loss += loss.item()\n","\n","        if batch % 200 == 0 and batch > 0:\n","            cur_loss = total_loss / 200\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch, len(train_data) // BPTT_SEQ_LENGTH , lr, elapsed * 1000 / 200, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","def evaluate(data_source):\n","    model.eval()\n","    total_loss = 0.\n","    ntokens = len(corpus.dictionary)\n","    hidden = model.init_hidden(EVAL_BATCH_SIZE) #隠れ状態初期化\n","    with torch.no_grad(): #評価なので、勾配を計算しない設定にする\n","        for i in range(0, data_source.size(0) - 1, BPTT_SEQ_LENGTH): #BPTT_SEQ_LENGTH分取り出して繰り返す\n","            data, targets = get_batch(data_source, i) #shapeが(BPTT_SEQ_LENGTH,BATCH_SIZE)のデータを得る\n","            output, hidden = model(data, hidden) #モデルに通す\n","            hidden = repackage_hidden(hidden) #隠れ状態の逆伝播を切る\n","            total_loss += len(data) * criterion(output, targets).item()\n","    return total_loss / (len(data_source) - 1)\n","\n","lr = 20\n","best_val_loss = None\n","try:\n","    for epoch in range(1, 10+1): #エポック回数だけ回す \n","        epoch_start_time = time.time()\n","        train() \n","        val_loss = evaluate(val_data)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n","        if not best_val_loss or val_loss < best_val_loss: #一番良いパラメータを保存する\n","            with open('jpn_model.pt', 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        else:\n","            lr /= 4.0 #改善しなくなったら、学習レートを下げる\n","except KeyboardInterrupt:\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open('jpn_model.pt', 'rb') as f:\n","    model = torch.load(f)\n","    model.rnn.flatten_parameters()\n","\n","# Run on test data.\n","test_loss = evaluate(test_data)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["batchfy input size torch.Size([474266])\n","batchfy input size torch.Size([8038])\n","batchfy input size torch.Size([4351])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:73: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n"],"name":"stderr"},{"output_type":"stream","text":["| epoch   1 |   200/  677 batches | lr 20.00 | ms/batch 17.23 | loss  7.15 | ppl  1278.24\n","| epoch   1 |   400/  677 batches | lr 20.00 | ms/batch 16.08 | loss  5.95 | ppl   384.14\n","| epoch   1 |   600/  677 batches | lr 20.00 | ms/batch 16.10 | loss  5.65 | ppl   283.00\n","| end of epoch   1 | time: 11.20s | valid loss  5.66 | valid ppl   287.57\n","| epoch   2 |   200/  677 batches | lr 20.00 | ms/batch 16.48 | loss  5.34 | ppl   208.81\n","| epoch   2 |   400/  677 batches | lr 20.00 | ms/batch 16.19 | loss  5.17 | ppl   175.33\n","| epoch   2 |   600/  677 batches | lr 20.00 | ms/batch 16.25 | loss  5.12 | ppl   167.45\n","| end of epoch   2 | time: 11.15s | valid loss  5.29 | valid ppl   198.37\n","| epoch   3 |   200/  677 batches | lr 20.00 | ms/batch 16.48 | loss  4.94 | ppl   140.31\n","| epoch   3 |   400/  677 batches | lr 20.00 | ms/batch 16.46 | loss  4.85 | ppl   128.08\n","| epoch   3 |   600/  677 batches | lr 20.00 | ms/batch 16.52 | loss  4.84 | ppl   125.84\n","| end of epoch   3 | time: 11.25s | valid loss  4.98 | valid ppl   145.80\n","| epoch   4 |   200/  677 batches | lr 20.00 | ms/batch 16.70 | loss  4.69 | ppl   108.85\n","| epoch   4 |   400/  677 batches | lr 20.00 | ms/batch 16.74 | loss  4.63 | ppl   102.05\n","| epoch   4 |   600/  677 batches | lr 20.00 | ms/batch 16.87 | loss  4.62 | ppl   101.40\n","| end of epoch   4 | time: 11.45s | valid loss  4.78 | valid ppl   119.07\n","| epoch   5 |   200/  677 batches | lr 20.00 | ms/batch 17.12 | loss  4.50 | ppl    89.84\n","| epoch   5 |   400/  677 batches | lr 20.00 | ms/batch 17.11 | loss  4.45 | ppl    85.23\n","| epoch   5 |   600/  677 batches | lr 20.00 | ms/batch 17.31 | loss  4.44 | ppl    85.04\n","| end of epoch   5 | time: 11.73s | valid loss  4.63 | valid ppl   102.21\n","| epoch   6 |   200/  677 batches | lr 20.00 | ms/batch 17.53 | loss  4.34 | ppl    76.38\n","| epoch   6 |   400/  677 batches | lr 20.00 | ms/batch 17.53 | loss  4.29 | ppl    73.00\n","| epoch   6 |   600/  677 batches | lr 20.00 | ms/batch 17.63 | loss  4.30 | ppl    73.54\n","| end of epoch   6 | time: 11.97s | valid loss  4.47 | valid ppl    87.67\n","| epoch   7 |   200/  677 batches | lr 20.00 | ms/batch 17.86 | loss  4.20 | ppl    66.47\n","| epoch   7 |   400/  677 batches | lr 20.00 | ms/batch 17.76 | loss  4.16 | ppl    64.26\n","| epoch   7 |   600/  677 batches | lr 20.00 | ms/batch 17.88 | loss  4.17 | ppl    64.42\n","| end of epoch   7 | time: 12.15s | valid loss  4.33 | valid ppl    75.90\n","| epoch   8 |   200/  677 batches | lr 20.00 | ms/batch 17.97 | loss  4.07 | ppl    58.84\n","| epoch   8 |   400/  677 batches | lr 20.00 | ms/batch 17.80 | loss  4.05 | ppl    57.15\n","| epoch   8 |   600/  677 batches | lr 20.00 | ms/batch 17.82 | loss  4.05 | ppl    57.57\n","| end of epoch   8 | time: 12.16s | valid loss  4.24 | valid ppl    69.52\n","| epoch   9 |   200/  677 batches | lr 20.00 | ms/batch 17.74 | loss  3.97 | ppl    53.09\n","| epoch   9 |   400/  677 batches | lr 20.00 | ms/batch 17.66 | loss  3.94 | ppl    51.45\n","| epoch   9 |   600/  677 batches | lr 20.00 | ms/batch 17.60 | loss  3.96 | ppl    52.35\n","| end of epoch   9 | time: 12.03s | valid loss  4.16 | valid ppl    64.33\n","| epoch  10 |   200/  677 batches | lr 20.00 | ms/batch 17.65 | loss  3.88 | ppl    48.40\n","| epoch  10 |   400/  677 batches | lr 20.00 | ms/batch 17.53 | loss  3.85 | ppl    47.08\n","| epoch  10 |   600/  677 batches | lr 20.00 | ms/batch 17.54 | loss  3.87 | ppl    48.08\n","| end of epoch  10 | time: 11.97s | valid loss  4.03 | valid ppl    56.39\n","| End of training | test loss  4.02 | test ppl    55.94\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m4B_Al2T2rKl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622775630279,"user_tz":-540,"elapsed":1814,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"5db0e72a-fd21-43cd-b1b9-7406dbf1e68f"},"source":["import torch\n","torch.manual_seed(1111)\n","device = torch.device(\"cuda\" if True else \"cpu\")\n","with open('./jpn_model.pt', 'rb') as f:\n","    model = torch.load(f).to(device)\n","model.eval()\n","corpus = Corpus('./')\n","ntokens = len(corpus.dictionary)\n","hidden = model.init_hidden(1) #バッチサイズを１にして初期化\n","input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device) #先頭単語をランダムに選ぶ\n","with torch.no_grad():  \n","    sentence = '' #生成文\n","    for i in range(500):\n","        output, hidden = model(input, hidden) #outputは語彙サイズの確率ベクトル[[...]]\n","        word_weights = output.squeeze().div(1.0).exp().cpu() #語彙サイズのEXP(x)\n","        word_idx = torch.multinomial(word_weights, 1)[0] #確率が一番大きな単語のIndexをとる\n","        input.fill_(word_idx) #この予測単語を、次のループ時のinputに設定\n","        word = corpus.dictionary.idx2word[word_idx]\n","        sentence += (word  + ('\\n' if i % 20 == 19 else ' '))\n","    print(sentence)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["年 に は 477 、 援 染色 思い 放棄 の ゲーム ・ 茂吉 を 使用 し た 大会 で は\n","ネコ 圏 で は なく 、 航路 は 、 下腹 と 呼ば れ 、 喪 あるいは 眼球 を まとめ た\n","1945 年 の メス が 出馬 縫い合わさ が 、 内部 の 超 一 例 で ある 。 第 二 次\n","世界 大戦 後 も 5 億 ドル の ピアニスト 話者 を 発する 。 また 、 近世 は G 2 万\n","5 千 万 台 と さ れ 、 大 世界 の 安定 な かたち で は 40 と さ れる\n","世界 の 約 100 % （ また 9 . dmbox m ) で あっ た [ 105 ]。 <eos> また\n","、 制圧 や ともない の なか で は （ 通常 と は 目撃 さ れる ） が 、 タマムシ の\n","汚職 オキアミ など 第 1 在学 に わたっ て は 関係 の ない こと で 、 生活 を する 三\n","人 は 必ず しも 大きく の 横向き で ある 。 両者 の 中 で 食べ られ 、 それ まで の\n","とおり 、 隔絶 し た 構造 を よく 使用 する など は 口 や 動物 の 他 の 哺乳 類\n","が 著しく 連続 的 な 特徴 で ある [ 107 ]。 <eos> ジュラ 紀 に は 軟体 種 に は\n","鰓 ごみ が 上下 型 硫黄 を 集会 し て 最大 の 歯 を 制限 し て いる が 、\n","幾 つ か の 生態 系 を 持つ 例 で は 判断 個体 が 市内 行わ れる が 、 アルファ\n","者 の 中 に は 森林 原料 の 瞳孔 を D と 構成 する 例 も あっ た が 、\n","なか に も ツバメ を 溜め て うち 度合い を 知る べき と し て 行わ れる 。 変異 じぎ\n","を 攻撃 し て いる 鳥類 も 、 実在 し て いる 。 出産 中 より 1 マルク を 上行\n","し て 消費 する こと で 腓 用 型 の 結膜 に ある ため の オットー 小型 231 . 5\n","割 の 窒素 を 掌握 し て いる が 、 眼杯 の 検査 の ため の 摂取 が 認め られ\n","たり 、 人間 を 孵化 する 者 も いる 。 <eos> 個眼 は 1 境界 6 色 ）[ 11 ]。\n","1995 年 に は 網膜 が 変更 さ れ て い た が 、 一般 に 静止 し て 472\n","が 特に 夏 が 駆逐 部分 を 夢 と し て 行わ れる 研究 結果 で 長く なっ た 。\n","脳 の 捕らえる グラム など の 大 ほか わかり カモ 類 は 引き続き 構成 さ れる ともさ れ て いる\n","[ 95 ]。 <eos> 凍結 など は 、 白亜 紀 に は 鳥類 の 関係 を 示唆 し 、 これ\n","ら の 構造 が 行う ため に 解析 し 、 鳥類 と の 関連 点 が あり 、 まれ に\n","親鳥 で 例 が ある 。 しかし 、 猫 が 区別 し て い た センサ の 肉 を 、\n","\n"],"name":"stdout"}]}]}