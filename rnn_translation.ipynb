{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"rnn_translation.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/a60617788061539b5449701ae76aee56/seq2seq_translation_tutorial.ipynb","timestamp":1592791125626}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7T02y8SYYvtx"},"source":["#データ準備"]},{"cell_type":"code","metadata":{"id":"UwGQDFaEPnrO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622771432318,"user_tz":-540,"elapsed":25704,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"93f961d8-59bf-4e82-cc3d-eee6105304e2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pVUPtzFqMoR_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622771459507,"user_tz":-540,"elapsed":401,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"e79536bf-a952-43ea-f554-36609cc0a067"},"source":["!pwd\n","%cd /content/drive/My Drive/Colab Notebooks/NLP20211H"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n","/content/drive/My Drive/Colab Notebooks/NLP20211H\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jy3rDiICv5yD","executionInfo":{"status":"ok","timestamp":1622771505363,"user_tz":-540,"elapsed":346,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}}},"source":["%matplotlib inline"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5K_PA3RF5W4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622771537461,"user_tz":-540,"elapsed":29124,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"55b8e48f-b877-4b43-9f4e-835e6817e98a"},"source":["!pip install mecab-python3\n","!pip install unidic-lite\n","import MeCab"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting mecab-python3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/72/20f8f60b858556fdff6c0376b480c230e594621fff8be780603ac9c47f6a/mecab_python3-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (487kB)\n","\u001b[K     |████████████████████████████████| 491kB 2.9MB/s \n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-1.0.3\n","Collecting unidic-lite\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/2b/8cf7514cb57d028abcef625afa847d60ff1ffbf0049c36b78faa7c35046f/unidic-lite-1.0.8.tar.gz (47.4MB)\n","\u001b[K     |████████████████████████████████| 47.4MB 63kB/s \n","\u001b[?25hBuilding wheels for collected packages: unidic-lite\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-cp37-none-any.whl size=47658838 sha256=342fe7ae3deb98c74a05036c66e3542558fa7b729278dfb6250a182a158d8351\n","  Stored in directory: /root/.cache/pip/wheels/20/48/8d/b66d8361a27f58f41ec86640e4fd2640de0403a6367511eab7\n","Successfully built unidic-lite\n","Installing collected packages: unidic-lite\n","Successfully installed unidic-lite-1.0.8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1Pclg0IAv5yj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622771582352,"user_tz":-540,"elapsed":6434,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"e1089d87-1ff4-4767-9f93-725015d3db19"},"source":["from __future__ import unicode_literals, print_function, division\n","from io import open\n","import unicodedata\n","import string\n","import re\n","import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","SOS_token = 0 #文頭マークを予約\n","EOS_token = 1 #文末マークを予約\n","\n","class Lang:\n","    #+\n","    #言語ごとに、単語からIndex,Indexから単語へ変換する辞書を準備\n","    #-\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        self.n_words = 2  # Count SOS and EOS\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","#+\n","#英語に関しては、全小文字にして、その他特殊文字を無視するように正規化\n","#-\n","def normalizeString(s):\n","    s = s.lower().strip()\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s) #rは文字列を生で扱うraw演算子(例：\\を\\として扱う]) 、re.subrは正規表現置き換えメソッド\n","    s = re.sub(r\"[^a-zA-Z0-9.!?]+\", r\" \", s) #アルファベット、数字以外の特殊文字を空白に置き換え\n","    return s\n","\n","def readLangs(lang1,lang2,reverse=False):  #第3引数は省略されたらFalse\n","    print(\"Reading lines...\")\n","    tagger = MeCab.Tagger(\"-Owakati\")\n","    lines = open('jpn.txt', encoding='utf-8').read().strip().split('\\n') #jpn.txtに、英語、日本語のペアが入っている\n","    # 英語文は文字を正規化、日本語文は分かち書きして、ペアを作る\n","    pairs = [\n","             [ normalizeString(l.split('\\t')[0]), (tagger.parse(l.split('\\t')[1])).replace('\\n','') ] \n","             for l in lines\n","             ]\n","    print(pairs[:5])\n","\n","    # ソースとターゲットを逆にしたいときのため\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        input_lang = Lang(lang2)\n","        output_lang = Lang(lang1)\n","    else:\n","        input_lang = Lang(lang1)\n","        output_lang = Lang(lang2)\n","    return input_lang, output_lang, pairs\n","\n","#+\n","#データが多いと語彙数が増えて負荷が大きいので、10単語以下で、かつ以下のパターンを持つ英文を含む行だけにフィルターする\n","#-\n","MAX_LENGTH = 10\n","eng_prefixes = (\"i am \", \"i m \", \"he is\", \"he s \", \"she is\", \"she s \", \"you are\", \"you re \", \"we are\", \"we re \",  \"they are\", \"they re \")\n","def filterPairs(pairs, reverse): \n","    def filterPair(p, reverse):\n","      if reverse:\n","        return (len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH) and p[1].startswith(eng_prefixes)\n","      else:\n","        return (len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH) and p[0].startswith(eng_prefixes)\n","    return [pair for pair in pairs if filterPair(pair, reverse)]\n","\n","#+\n","#ファイルからデータを読み込み、フィルターし、input_lang, output_langにそれぞれ格納する\n","#-\n","def prepareData(lang1, lang2, reverse=False): #第3引数は省略されたらFalse\n","    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    pairs = filterPairs(pairs, reverse)\n","    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","input_lang, output_lang, pairs = prepareData('eng', 'jpn', False)\n","print(random.choice(pairs))\n","print(pairs[:5])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Reading lines...\n","[['go .', '行け 。 '], ['go .', '行き なさい 。 '], ['hi .', 'こんにちは 。 '], ['hi .', 'もしもし 。 '], ['hi .', 'やっほー 。 ']]\n","Read 48955 sentence pairs\n","Trimmed to 2080 sentence pairs\n","Counting words...\n","Counted words:\n","eng 1230\n","jpn 1560\n","['i m very happy to see you .', 'お 会い でき て とても 嬉しい です 。 ']\n","[['i m 19 .', '１９ 歳 です 。 '], ['i m ok .', '大丈夫 です よ 。 '], ['i m ok .', '私 は 大丈夫 です 。 '], ['i m up .', '起き てる よ 。 '], ['i m tom .', 'トム と 申し ます 。 ']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CNaZEdtlY2cZ"},"source":["#Seq2Seq"]},{"cell_type":"code","metadata":{"id":"biZC3iqgv5zK","executionInfo":{"status":"ok","timestamp":1622771626900,"user_tz":-540,"elapsed":369,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}}},"source":["#+\n","#入力文をGRUで符号化する\n","#-\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(input_size, hidden_size)  #単語の埋め込み表を作る\n","        self.gru = nn.GRU(hidden_size, hidden_size) #GRUオブジェクトを生成する\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1) #入力単語の埋め込みベクトルを取り出す\n","        output, hidden = self.gru(embedded, hidden) #GRUに通す\n","        return output, hidden #新しい隠れ状態を返す\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)\n","\n","#+\n","#入力文をGRUで符号化した結果（最終隠れ状態）を使って、出力単語列に復元する\n","#-\n","class DecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(output_size, hidden_size) #埋め込み表を準備する\n","        self.gru = nn.GRU(hidden_size, hidden_size) #GRUオブジェクトを作る\n","        self.out = nn.Linear(hidden_size, output_size) #GRUの出力を出力言語語彙数サイズのベクトルへ全結合\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden):\n","        output = F.relu(self.embedding(input).view(1, 1, -1)) #単語の埋め込みベクトルを得る\n","        output, hidden = self.gru(output, hidden) #GRUに通して、出力（次単語予測）を得る\n","        output = self.softmax(self.out(output[0])) #全結合で出力言語語彙数サイズのベクトルへ変換し、softmax\n","        return output, hidden #次単語予測（次回は入力となる）と、隠れ状態を返す\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"aw180tw3EAKm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622772376166,"user_tz":-540,"elapsed":741391,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"355521cc-c16e-445c-d1d8-8c62e4831112"},"source":["#+\n","#文を単語IDリストにする\n","#-\n","def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","#-\n","#文を、単語の並びのテンソルにする\n","#-\n","def tensorFromSentence(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token) #文末マークを付加する\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1) #(単語数,1)のshapeにしてテンソル化\n","\n","#+\n","#入力文とターゲット文をそれぞれ、単語を並べたテンソルにする\n","#-\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence(input_lang, pair[0]) #入力文をshape(単語数,1)の行列にする  \n","    target_tensor = tensorFromSentence(output_lang, pair[1]) #ターゲット文をshape(単語数,1)の行列にする \n","    return (input_tensor, target_tensor)\n","\n","def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n","    encoder_hidden = encoder.initHidden() #エンコーダーの隠れ状態を初期化\n","    encoder_optimizer.zero_grad() #エンコーダーの勾配を初期化\n","    decoder_optimizer.zero_grad() #デコーダーの勾配を初期化\n","    input_length = input_tensor.size(0) #入力単語数をとる\n","    target_length = target_tensor.size(0) #ターゲット単語数をとる\n","    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device) #エンコーダーの出力を初期化\n","    loss = 0\n","\n","    for ei in range(input_length): #入力単語数だけエンコーダーを回す\n","        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden) #エンコーダーに入力単語を1個づつ渡す\n","        #エンコーダーのouputは利用していないことに注意\n","\n","    decoder_input = torch.tensor([[SOS_token]], device=device) #デコーダーの最初の入力を文頭マークにする\n","    decoder_hidden = encoder_hidden #エンコーダーの最終隠れ状態を、デコーダーの隠れ状態の初期値にする\n","    for di in range(target_length): #デコーダーを出力単語数だけ回す\n","        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) #デコーダーに分党から始まる単語を1個づつ予測させる\n","        loss += criterion(decoder_output, target_tensor[di]) #正解単語と比べてロスを計算する\n","        topv, topi = decoder_output.topk(1) #予測単語を得る\n","        decoder_input = topi.squeeze().detach()  #予測単語を取り出して、次のループの入力に設定する\n","        if decoder_input.item() == EOS_token: break #予測単語が文末ならループを出る\n","   \n","    loss.backward() #逆伝播\n","    encoder_optimizer.step() #学習レートを調整\n","    decoder_optimizer.step() #学習レートを調整\n","    return loss.item() / target_length #平均ロスを返す\n","\n","def trainIters(encoder, decoder, n_iters, every=1000):\n","    loss_total = 0  # Reset every print_every\n","    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)] #トレーニングデータを取り出す\n","    criterion = nn.NLLLoss() #ロスをクロスエントロピーに設定\n","\n","    for i in range(1, n_iters + 1): #n_itersだけトレーニングを回す\n","        training_pair = training_pairs[i - 1]\n","        input_tensor = training_pair[0] #入力文の単語列、shape(単語数,1)\n","        target_tensor = training_pair[1] #ターゲット文の単語列、shape(単語数,1)\n","        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n","        loss_total += loss\n","        if i % every == 0:\n","            loss_avg = loss_total / every\n","            loss_total = 0\n","            print('(%d %d%%) %.4f' % (i, i / n_iters * 100, loss_avg))\n","\n","hidden_size = 256\n","encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device) #エンコーダーオブジェクト作成\n","decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device) #デコーダーオブジェクト作成\n","learning_rate=0.01\n","encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate) #SGDオプティマイザーオブジェクト作成\n","decoder_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate) #SGDオプティマイザーオブジェクト作成\n","trainIters(encoder1, decoder1, 50000) #trainItersを起動\n","#トレーニングに時間がかかるので終了したらいったん保存。\n","torch.save({ \n","            'enc': encoder1.state_dict(),\n","            'dec': decoder1.state_dict(),\n","            'eopt': encoder_optimizer.state_dict(),\n","            'dopt': decoder_optimizer.state_dict(),\n","            }, 'seq2seq.pt')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["(1000 2%) 3.6985\n","(2000 4%) 3.3566\n","(3000 6%) 3.1957\n","(4000 8%) 3.0646\n","(5000 10%) 2.9030\n","(6000 12%) 2.7358\n","(7000 14%) 2.5682\n","(8000 16%) 2.3587\n","(9000 18%) 2.3117\n","(10000 20%) 2.0308\n","(11000 22%) 2.0047\n","(12000 24%) 1.7912\n","(13000 26%) 1.6514\n","(14000 28%) 1.5485\n","(15000 30%) 1.4971\n","(16000 32%) 1.3049\n","(17000 34%) 1.2549\n","(18000 36%) 1.1474\n","(19000 38%) 1.0425\n","(20000 40%) 0.9922\n","(21000 42%) 0.9120\n","(22000 44%) 0.8568\n","(23000 46%) 0.7808\n","(24000 48%) 0.7273\n","(25000 50%) 0.7648\n","(26000 52%) 0.6387\n","(27000 54%) 0.6254\n","(28000 56%) 0.5863\n","(29000 57%) 0.5789\n","(30000 60%) 0.5140\n","(31000 62%) 0.5165\n","(32000 64%) 0.4918\n","(33000 66%) 0.4629\n","(34000 68%) 0.4440\n","(35000 70%) 0.4510\n","(36000 72%) 0.4311\n","(37000 74%) 0.3950\n","(38000 76%) 0.4156\n","(39000 78%) 0.3766\n","(40000 80%) 0.3524\n","(41000 82%) 0.3452\n","(42000 84%) 0.3386\n","(43000 86%) 0.3591\n","(44000 88%) 0.3706\n","(45000 90%) 0.3607\n","(46000 92%) 0.3633\n","(47000 94%) 0.3089\n","(48000 96%) 0.3350\n","(49000 98%) 0.3458\n","(50000 100%) 0.2995\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jz53oi_Iv50D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622774756862,"user_tz":-540,"elapsed":819,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"9d088b5c-2f01-4c83-a22a-3ae03f8a84d8"},"source":["def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n","    with torch.no_grad(): #勾配計算しないモード\n","        input_tensor = tensorFromSentence(input_lang, sentence) #入力文をID列のテンソルにする\n","        input_length = input_tensor.size()[0]\n","        encoder_hidden = encoder.initHidden() #エンコーダーの隠れ状態初期化\n","        for ei in range(input_length): #入力単語数だけ、ループを回す\n","            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden) #エンコーダーで隠れ状態を更新\n","        decoder_input = torch.tensor([[SOS_token]], device=device)  # デコーダーの最初の入力に文頭マークを設定\n","        decoder_hidden = encoder_hidden #エンコーダーの出力をデコーダーの隠れ状態の初期値とする\n","        decoded_words = []\n","        for di in range(max_length): #文末を予測するまで、ループ\n","            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) #デコーダーで次の単語を予測し、隠れ状態更新\n","            topv, topi = decoder_output.data.topk(1) #次に来る予測単語を取り出す\n","            if topi.item() == EOS_token:\n","                decoded_words.append('<EOS>') #文末を予測したら終了\n","                break\n","            else:\n","                decoded_words.append(output_lang.index2word[topi.item()]) #予測単語を出力単語リストに追加\n","            decoder_input = topi.squeeze().detach() #デコーダへの入力として、今得た予測単語を設定\n","        return decoded_words\n","  \n","def evaluateRandomly(encoder, decoder, n=10): #ランダムに10個翻訳させる\n","    for i in range(n):\n","        pair = random.choice(pairs) \n","        print('>', pair[0])\n","        print('=', pair[1])\n","        output_words = evaluate(encoder, decoder, pair[0])\n","        output_sentence = ' '.join(output_words)\n","        print('<', output_sentence)\n","        print('')\n","\n","hidden_size = 256\n","encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n","decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n","learning_rate=0.01\n","encoder_optimizer = optim.SGD(encoder1.parameters(), lr=learning_rate)\n","decoder_optimizer = optim.SGD(decoder1.parameters(), lr=learning_rate)\n","#エンコーダー、デコーダーのパラメータを保存ファイルからロード\n","m = torch.load('seq2seq.pt')\n","encoder1.load_state_dict(m['enc'])\n","decoder1.load_state_dict(m['dec'])\n","encoder_optimizer.load_state_dict(m['eopt'])\n","decoder_optimizer.load_state_dict(m['dopt'])\n","evaluateRandomly(encoder1, decoder1,30)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["> he is a fast runner .\n","= 彼 は 走る の が 速い 。 \n","< 彼 は 走る の が 速い 。  <EOS>\n","\n","> he is busy with job hunting .\n","= 彼 は 職探し に 忙しい 。 \n","< 彼 は 職探し に 忙しい 。  <EOS>\n","\n","> i m breast feeding my baby .\n","= 母乳 で 育て て い ます 。 \n","< 母乳 で 育て て い ます 。  <EOS>\n","\n","> he is an honest man .\n","= 彼 は 正直 な 男 だ 。 \n","< 彼 は 正直 な 男 だ 。  <EOS>\n","\n","> i m sorry for being late .\n","= 遅く なっ て 申し訳 あり ませ ん 。 \n","< 遅れ なっ ごめん 。  。  <EOS>\n","\n","> he is looking for a job .\n","= 彼 は 職 を 探し て いる 。 \n","< 彼 は 職 を 探し て いる 。  <EOS>\n","\n","> i m leaving town for a few days .\n","= 数 日 町 を 離れ ます 。 \n","< 数 日 町 を 離れ ます 。  <EOS>\n","\n","> we are going to have a storm .\n","= 嵐 に なる だろう 。 \n","< 嵐 に なる だろう 。  <EOS>\n","\n","> she is very clever .\n","= 彼女 は とても 利口 だ 。 \n","< 彼女 は とても 賢い 人 だ 。  <EOS>\n","\n","> she is a little shy .\n","= あの 子 ちょっと シャイ な の 。 \n","< あの 子 ちょっと シャイ な の 。  <EOS>\n","\n","> she is attractive .\n","= 彼女 は 魅力 的 だ 。 \n","< 彼女 は 魅力 的 だ 。  <EOS>\n","\n","> you re just guessing .\n","= 単なる 推測 だろ 。 \n","< 単なる 推測 だろ 。  <EOS>\n","\n","> i m cooperative .\n","= 私 は 協調 性 が ある 。 \n","< 私 は 協調 性 が ある 。  <EOS>\n","\n","> she is now on vacation .\n","= 彼女 は 休暇 中 です 。 \n","< 彼女 は 休暇 中 です 。  <EOS>\n","\n","> i m rolling in cash .\n","= 金 なら 腐る ほど ある ぜ 。 \n","< 金 なら 腐る ほど ある ぜ 。  <EOS>\n","\n","> i m not busy now .\n","= 私 は 今 暇 です 。 \n","< 私 は 今 暇 です 。  <EOS>\n","\n","> i m sorry about the other day .\n","= 先日 は ごめん なさい 。 \n","< 先日 間 は ごめん 。  <EOS>\n","\n","> i m being followed .\n","= 追わ れ てる ん だ 。 \n","< 追わ れ てる ん だ 。  <EOS>\n","\n","> he is delicate .\n","= 彼 は 繊細 だ 。 \n","< 彼 は 繊細 だ 。  <EOS>\n","\n","> i m just looking .\n","= 見 てる だけ です 。 \n","< 見 てる て だけ です 。 。 <EOS>\n","\n","> he is watching tv .\n","= 彼 は テレビ で 見 て いる 。 \n","< 彼 は テレビ で 見 て いる 。  <EOS>\n","\n","> you re looking very well .\n","= あなた は とても 元気 そう です ね 。 \n","< あなた は とても 元気 です です ね 。  <EOS>\n","\n","> he is nice .\n","= いい 人 です 。 \n","< いい は いい 。  。  <EOS>\n","\n","> he s a high school student .\n","= 彼 は 高校 生 です 。 \n","< 彼 は 高校 の です 。  <EOS>\n","\n","> he is handsome and clever .\n","= 彼 は ハンサム で 頭 も よい 。 \n","< 彼 は ハンサム で 頭 も よい 。  <EOS>\n","\n","> he is lying .\n","= 彼 は うそ を つい て いる 。 \n","< 彼 は 嘘 を 言っ て いる 。  <EOS>\n","\n","> he s doing a handstand .\n","= 彼 は 逆立ち し て いる 。 \n","< 彼 は 逆立ち し て いる 。  <EOS>\n","\n","> i m looking forward to the party .\n","= パーティー が 楽しみ だ 。 \n","< パーティー が 楽しみ だ 。  <EOS>\n","\n","> she is french .\n","= 彼女 は フランス 人 だ 。 \n","< 彼女 は フランス 人 だ 。  <EOS>\n","\n","> i m not frightened .\n","= 怖く なん て ない さ 。 \n","< 怖く なん て ない さ 。  <EOS>\n","\n"],"name":"stdout"}]}]}