{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP、RNNCNNTransformer.ipynb","private_outputs":true,"provenance":[{"file_id":"1fQjD4-gf4b4yI-bU_rPDS0eUGbgCHjZh","timestamp":1610693786643}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1FNjgh0hX7sRyAFw0cWJ7yaPfuOow3HwC","authorship_tag":"ABX9TyNUsQ8jE5gGonxWO8mFzZkW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0dxiHwhmdcXp"},"source":["!pwd\n","%cd /content/drive/My Drive/Colab Notebooks/NLP"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f94C95jj4nmN"},"source":["#第9章: RNN, CNN, Transformer"]},{"cell_type":"markdown","metadata":{"id":"l11Bbcix4q6M"},"source":["##課題80. ID番号への変換\n","問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"]},{"cell_type":"markdown","metadata":{"id":"xHxjUuLGw9DF"},"source":["###解説\n","学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与する。\n","\n","データを準備する手順。\n","\n","１．記事データをpandasで読み込み、必要な部分だけ抜き取る。\n","\n","２．記事Titleに含まれる単語をカウントする。\n","\n","３．単語からIDを引く辞書を作る。\n","\n","４．タイトルに含まれる単語から、ID列に変換する補助関数を作っておく。\n","\n","５．TITLEをID列にして、CATEGORY文字を数字に変換したデータを準備しておく。\n","\n","６．それをtrain,valid,testに分割する。"]},{"cell_type":"markdown","metadata":{"id":"Jv5bDF1K_wIe"},"source":["###解答例\n","\n"]},{"cell_type":"code","metadata":{"id":"QiADlCWZkAYx"},"source":["#From From 50,51\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n","!unzip NewsAggregatorDataset.zip\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pickle\n","#\n","# 記事データをpandasで読み込み、必要な部分だけ抜き取る。\n","#\n","df = pd.read_csv('./newsCorpora.csv', header=None, sep='\\t', \n","    names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), \n","            ['TITLE', 'CATEGORY']]\n","df['TITLE'] = df['TITLE'].str.lower()\n","df['TITLE'] = df['TITLE'].str.replace('[^a-z]',' ', regex=True)\n","df['TITLE'] = df['TITLE'].str.replace(' +',' ', regex=True)\n","df.to_csv('./news.csv',sep='\\t',index=False,header=False)\n","#\n","# 記事Titleに含まれる単語をカウントする。\n","#\n","lex={}\n","for title in df['TITLE']:\n","  words = title.split()\n","  for w in words:\n","      if w in lex:\n","        lex[w] += 1\n","      else:\n","        lex[w] = 1 \n","freq = sorted(lex.items(), key=lambda x: x[1], reverse=True)\n","for w in freq[:10]: print(w)\n","#\n","# 単語からIDを引く辞書を作る\n","#\n","word2id = { word: i + 1  for i, (word, cnt) in enumerate(freq) if cnt>1 }\n","print(len(word2id))\n","for e in list(word2id)[:10]: print(f'({e}, {word2id[e]})')\n","with open('./word2id.dict', mode='wb') as f:  pickle.dump(word2id,f)\n","#\n","# タイトルに含まれる単語から、ID列に変換する補助関数\n","#\n","def words2ids(words):\n","  ids = []\n","  for w in words.split():\n","    if w in word2id:\n","      ids.append(word2id[w])\n","    else:\n","      ids.append(0)\n","  return ' '.join([str(i) for i in ids])\n","\n","text = df.iloc[0, 0]\n","print(text)\n","print(words2ids(text))\n","#\n","# TITLEをID列にして、CATEGORY文字を数字に変換したデータを準備しておく。\n","#\n","df['TITLE'] = df['TITLE'].map(words2ids)\n","df['CATEGORY'] = df['CATEGORY'].map({'b': 0, 'e': 1, 't': 2, 'm': 3})\n","data = pd.DataFrame(df.to_numpy())\n","print(data)\n","#\n","# train、valid, testに分割する\n","#\n","train, valid_test = train_test_split(data,  train_size=0.8, shuffle=True, stratify=data[1])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, stratify=valid_test[1])\n","\n","train.to_csv('./train.txt', sep='\\t', index=False, header=False)\n","valid.to_csv('./valid.txt', sep='\\t', index=False, header=False)\n","test.to_csv('./test.txt', sep='\\t', index=False, header=False)\n","!wc -l train.txt valid.txt test.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zt4f7TyZnN1S"},"source":["from torch.utils.data import Dataset\n","import torch\n","from torch import nn\n","\n","class NewsDataset(Dataset):\n","  def __init__(self, x, y):  self.x, self.y = x, y\n","  def __len__(self):  return len(self.y)\n","  def __getitem__(self, idx):  \n","    text = self.x[idx]\n","    #単語IDが並んでいるので、数値に型変換してリストにする\n","    inputs = [int(x) for x in text.split()]\n","    #torch.tensor方へ変換\n","    return { 'inputs': torch.tensor(inputs, dtype=torch.int64), 'labels': torch.tensor(self.y[idx], dtype=torch.int64) }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yncoxVdV9V3s"},"source":["##課題81. RNNによる予測\n","ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈RVは単語のID番号のone-hot表記である（Vは単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリy\n","\n","を予測するモデルとして，次式を実装せよ．\n","h→0=0,h→t=RNN−→−−(emb(xt),h→t−1),y=softmax(W(yh)h→T+b(y))\n","\n","ただし，emb(x)∈Rdw\n","は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），h→t∈Rdhは時刻tの隠れ状態ベクトル，RNN−→−−(x,h)は入力xと前時刻の隠れ状態hから次状態を計算するRNNユニット，W(yh)∈RL×dhは隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈RLはバイアス項である（dw,dh,Lはそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニットRNN−→−−(x,h)\n","\n","には様々な構成が考えられるが，典型例として次式が挙げられる．\n","RNN−→−−(x,h)=g(W(hx)x+W(hh)h+b(h))\n","\n","ただし，W(hx)∈Rdh×dw，W(hh)∈Rdh×dh,b(h)∈Rdh\n","はRNNユニットのパラメータ，gは活性化関数（例えばtanh\n","\n","やReLUなど）である．\n","\n","なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでy\n","を計算するだけでよい．次元数などのハイパーパラメータは，dw=300,dh=50など，適当な値に設定せよ（以降の問題でも同様である）．"]},{"cell_type":"markdown","metadata":{"id":"v1k0nERtyGp3"},"source":["###解説\n","数式が分かりにくいが、PyTorchのクラスRNNなどを使うだけでよい。この課題では、入力データを埋め込み表現にして、RNNに通し、最後に得た隠れ状態ベクトルを、全結合層介して、出力する。\n"]},{"cell_type":"markdown","metadata":{"id":"PJ4AvvhJ_2Uv"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"YOgPxwk3mopT"},"source":["import torch\n","from torch import nn\n","\n","DEBUG=True\n","\n","class RNN(nn.Module):\n","\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size):\n","    super().__init__()\n","    self.hidden_size = hidden_size\n","    #縦が全単語数、横が埋め込みベクトルサイズの埋め込み表を定義。\n","    self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","    # RNNを定義。\n","    self.rnn = nn.RNN(emb_size, hidden_size, num_layers=1, bias=True, nonlinearity='tanh', batch_first=True)\n","    # RNNの最後の隠れ状態ベクトルを入力とし、4個のどのクラスに属するかのベクトルを出力する\n","    self.fc = nn.Linear(hidden_size, output_size)\n","\n","  def forward(self, x):\n","    #ここでは、ミニバッチサイズは１．後で複数にする。\n","    self.batch_size = x.size()[0]\n","    #隠れ状態ベクトルを初期化\n","    hidden = torch.zeros(1, self.batch_size, self.hidden_size)\n","    #入力（単語IDが単語数分並んでいる）に対し、その埋め込みベクトルを返す。\n","    emb = self.emb(x)\n","    if DEBUG: print(f'emb size {emb.size()}')\n","    #埋め込み表現された単語列をRNNに入力し、hiddenに隠れ状態ベクトルをためる\n","    out, hidden = self.rnn(emb, hidden)\n","    if DEBUG: print(f'rnn out size: {out.size()}')\n","    if DEBUG: print(f'fc input size: {out[:,-1,:].size()}')\n","    #最後の隠れ状態ベクトルを取り出し、全結合に入力する。\n","    out = self.fc(out[:, -1, :]) # fc input is the final hidden state vector\n","    if DEBUG: print(f'fc out size: {out.size()}')\n","    return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cKml70QZimFJ"},"source":["import torch\n","from torch import nn\n","import pandas as pd\n","\n","torch.manual_seed(1)\n","with open('./word2id.dict', mode='rb') as f: word2id = pickle.load(f)\n","train = pd.read_csv('./train.txt', sep='\\t', header=None, names=['TITLE','CATEGORY'])\n","dataset_train = NewsDataset(train['TITLE'], train['CATEGORY'])\n","\n","szVOCAB = len(word2id.values()) + 1 \n","szEMB = 300\n","PADDING_IDX = len(word2id.values())\n","szOUTPUT = 4\n","szHIDDEN = 64\n","model = RNN(szVOCAB, szEMB, PADDING_IDX, szOUTPUT, szHIDDEN)\n","\n","for i in range(10):\n","  #ii番目の単語列を取り出す。\n","  x = dataset_train[i]['inputs']\n","  if DEBUG: print(f'model input: {x}')\n","  #ミニバッチの次元を追加\n","  x = x.unsqueeze(0) # insert batch_size=1 by unsqueeze(0)\n","  if DEBUG: print(f'model input unsqueezed: {x}')\n","  #モデルに単語ID列を入力し、どのクラスに属するかの4個の評価値ベクトルを得る。\n","  out = model(x)\n","  predict = torch.softmax(out, dim=-1)\n","  if DEBUG: print(f'model output: {predict}')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIB3Ptk6-7l4"},"source":["##課題82. 確率的勾配降下法による学習　[省略]\n","確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ"]},{"cell_type":"markdown","metadata":{"id":"t2Zute7DyglD"},"source":["###解説\n","73相当で、83までの途中段階の課題で、トレーニングと評価のエポックを回す。\n","時間がかかるので省略する。\n"]},{"cell_type":"markdown","metadata":{"id":"IIRmMPZBGVmz"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"xGm6zxluRagt"},"source":["import matplotlib.pyplot as plt\n","\n","class Measure():\n","  def __init__(self):\n","    self.loss_train_list = []\n","    self.loss_valid_list = []\n","    self.accuracy_train_list = []\n","    self.accuracy_valid_list = []\n","\n","  def init_epoch(self):\n","    self.correct_train = 0\n","    self.total_train = 0\n","    self.correct_valid = 0\n","    self.total_valid = 0\n","    self.loss_train = 0.0\n","    self.loss_valid = 0.0\n","\n","  def accuracy(self, inputs,outputs,labels):\n","    total = len(inputs)\n","    prediction = torch.argmax(outputs, dim=1)\n","    correct = torch.sum(prediction==labels)\n","    return total, correct\n","\n","  def record_train(self,inputs,outputs,labels,lossitem):\n","    self.loss_train += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_train += total\n","    self.correct_train += correct\n","\n","  def train(self):\n","    self.loss_train /= len(dataset_train)\n","    self.accuracy_train = self.correct_train / self.total_train\n","\n","  def record_valid(self,inputs,outputs,labels, lossitem):\n","    self.loss_valid += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_valid += total\n","    self.correct_valid += correct\n","\n","  def valid(self):\n","    self.loss_valid /= len(dataset_valid)\n","    self.accuracy_valid = self.correct_valid / self.total_valid\n","\n","  def record_epoch(self, epoch):\n","    print(f'epoch: {epoch + 1}, loss_train: {self.loss_train:.4f}, loss_valid: {self.loss_valid:.4f}, \\\n","    accuracy_train: {self.accuracy_train:.2f}, accuracy_valid: {self.accuracy_valid:.2f}')\n","    self.loss_train_list.append(self.loss_train)\n","    self.loss_valid_list.append(self.loss_valid)\n","    self.accuracy_train_list.append(self.accuracy_train)\n","    self.accuracy_valid_list.append(self.accuracy_valid)\n","\n","  def degrading(self, epoch):\n","   return (epoch > 1 and self.loss_valid_list[epoch - 2] <= self.loss_valid_list[epoch - 1] <= self.loss_valid_list[epoch])\n","\n","def draw(numEpochs, loss_train_list, loss_valid_list, accuracy_train_list, accuracy_valid_list ):\n","  plt.figure()\n","  plt.plot(range(numEpochs), loss_train_list, label='loss_train')\n","  plt.plot(range(numEpochs), loss_valid_list, label='loss_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()\n","  plt.figure()\n","  plt.plot(range(numEpochs), accuracy_train_list, label='accuracy_train')\n","  plt.plot(range(numEpochs), accuracy_valid_list, label='accuracy_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izk7V2UfoNDj"},"source":["from torch import optim\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","DEBUG = False\n","\n","torch.manual_seed(0)\n","train = pd.read_csv('./train.txt', sep='\\t', header=None, names=['TITLE','CAT'])\n","dataset_train = NewsDataset(train['TITLE'], train['CAT'])\n","valid = pd.read_csv('./valid.txt', sep='\\t', header=None, names=['TITLE','CAT'])\n","dataset_valid = NewsDataset(valid['TITLE'], valid['CAT'])\n","\n","szVOCAB = len(set(word2id.values())) + 1 \n","szEMB = 300\n","PADDING_IDX = len(set(word2id.values()))\n","szOUTPUT = 4\n","szHIDDEN = 64\n","LEARNING_RATE = 0.005\n","szBATCH = 1\n","numEPOCHS = 10\n","\n","model = RNN(szVOCAB, szEMB, PADDING_IDX, szOUTPUT, szHIDDEN)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n","dataloader_valid = DataLoader(dataset_valid, batch_size=1, shuffle=False)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, numEPOCHS, gamma=0.8)\n","\n","measure = Measure()\n","for epoch in range(numEPOCHS):\n","  measure.init_epoch()\n","  model.train()\n","  for data in dataloader_train:\n","    optimizer.zero_grad()\n","    inputs, labels = data['inputs'], data['labels']\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    measure.record_train(inputs,outputs,labels,loss.item())  \n","  measure.train()  \n","  model.eval()\n","  with torch.no_grad():\n","    for data in dataloader_valid:\n","      optimizer.zero_grad()\n","      inputs, outputs = data['inputs'], data['labels']\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels)\n","      measure.record_valid(inputs,outputs,labels,loss.item())\n","  measure.valid()\n","  measure.record_epoch(epoch)\n","  if measure.degrading(epoch): break\n","  scheduler.step()\n","draw(epoch+1, measure.loss_train_list, measure.loss_valid_list, measure.accuracy_train_list, measure.accuracy_valid_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"msJLI2QbGHHs"},"source":["##課題83. ミニバッチ化・GPU上での学習\n","問題82のコードを改変し，B事例ごとに損失・勾配を計算して学習を行えるようにせよ（Bの値は適当に選べ）．また，GPU上で学習を実行せよ．"]},{"cell_type":"markdown","metadata":{"id":"m_DY4UFiy6Un"},"source":["###解説\n","ミニバッチ化して、GPUを利用する、77,78相当。\n","ミニバッチ化する際、並列実行しやすいように、ミニバッチごとに、単語ID列を最大個数で固定長にする。\n","素のRNNは、やや非力なので、GRUという変種を使う。"]},{"cell_type":"markdown","metadata":{"id":"cDHEs_FWHqPj"},"source":["###解答例"]},{"cell_type":"markdown","metadata":{"id":"SfxClvFUuMFF"},"source":["ここで、CPUタイプをGPUに変更する.\n","また、80番を実行する"]},{"cell_type":"code","metadata":{"id":"N3nLrH6UzZOr"},"source":["import matplotlib.pyplot as plt\n","\n","class Measure():\n","  def __init__(self):\n","    self.loss_train_list = []\n","    self.loss_valid_list = []\n","    self.accuracy_train_list = []\n","    self.accuracy_valid_list = []\n","\n","  def init_epoch(self):\n","    self.correct_train = 0\n","    self.total_train = 0\n","    self.correct_valid = 0\n","    self.total_valid = 0\n","    self.loss_train = 0.0\n","    self.loss_valid = 0.0\n","\n","  def accuracy(self, inputs,outputs,labels):\n","    total = len(inputs)\n","    prediction = torch.argmax(outputs, dim=1)\n","    correct = torch.sum(prediction==labels)\n","    return total, correct\n","\n","  def record_train(self,inputs,outputs,labels,lossitem):\n","    self.loss_train += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_train += total\n","    self.correct_train += correct\n","\n","  def train(self):\n","    self.loss_train /= len(dataset_train)\n","    self.accuracy_train = self.correct_train / self.total_train\n","\n","  def record_valid(self,inputs,outputs,labels, lossitem):\n","    self.loss_valid += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_valid += total\n","    self.correct_valid += correct\n","\n","  def valid(self):\n","    self.loss_valid /= len(dataset_valid)\n","    self.accuracy_valid = self.correct_valid / self.total_valid\n","\n","  def record_epoch(self, epoch):\n","    print(f'epoch: {epoch + 1}, loss_train: {self.loss_train:.4f}, loss_valid: {self.loss_valid:.4f}, \\\n","    accuracy_train: {self.accuracy_train:.2f}, accuracy_valid: {self.accuracy_valid:.2f}')\n","    self.loss_train_list.append(self.loss_train)\n","    self.loss_valid_list.append(self.loss_valid)\n","    self.accuracy_train_list.append(self.accuracy_train)\n","    self.accuracy_valid_list.append(self.accuracy_valid)\n","\n","  def degrading(self, epoch):\n","   return (epoch > 1 and self.loss_valid_list[epoch - 2] <= self.loss_valid_list[epoch - 1] <= self.loss_valid_list[epoch])\n","\n","def draw(numEpochs, loss_train_list, loss_valid_list, accuracy_train_list, accuracy_valid_list ):\n","  plt.figure()\n","  plt.plot(range(numEpochs), loss_train_list, label='loss_train')\n","  plt.plot(range(numEpochs), loss_valid_list, label='loss_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()\n","  plt.figure()\n","  plt.plot(range(numEpochs), accuracy_train_list, label='accuracy_train')\n","  plt.plot(range(numEpochs), accuracy_valid_list, label='accuracy_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ViPALWw_uW7y"},"source":["#\n","#複数データをミニバッチとして、並行して処理するようにする。RNNは入力が可変長であるため、並列実行しにくい。\n","#そこで、ミニバッチ単位で、最も長い単語列のサイズに合わせる。\n","#\n","class Padsequence():\n","  def __init__(self, padding_idx):\n","    self.padding_idx = padding_idx\n","\n","  def __call__(self, batch):\n","    sequences = [x['inputs'] for x in batch]\n","    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)\n","    labels = torch.LongTensor([x['labels'] for x in batch])\n","    return {'inputs': sequences_padded, 'labels': labels}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbdrribaNzuz"},"source":["from torch import optim\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pickle\n","import numpy as np\n","\n","class RNN(nn.Module):\n","\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size):\n","    super().__init__()\n","    self.hidden_size = hidden_size\n","    self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","    #self.rnn = nn.RNN(emb_size, hidden_size, num_layers=1, nonlinearity='tanh', bias=True, batch_first=True, dropout=0.3)\n","    self.rnn = nn.GRU(emb_size, hidden_size, num_layers=1, bias=True, batch_first=True, dropout=0.3)\n","    self.fc = nn.Linear(hidden_size, output_size)\n","\n","  def forward(self, x):\n","    self.batch_size = x.size()[0]\n","    hidden = torch.zeros(1, self.batch_size, self.hidden_size).to(device) #<------\n","    emb = self.emb(x)\n","    out, hidden = self.rnn(emb, hidden)\n","    out = self.fc(out[:, -1, :]) # fc input is the final hidden state vector\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRML4nw4lCEz"},"source":["torch.manual_seed(0)\n","with open('./word2id.dict', mode='rb') as f: word2id = pickle.load(f)\n","train = pd.read_csv('./train.txt', sep='\\t', header=None, names=['TITLE','CATEGORY'])\n","dataset_train = NewsDataset(train['TITLE'], train['CATEGORY'])\n","valid = pd.read_csv('./valid.txt', sep='\\t', header=None, names=['TITLE','CATEGORY'])\n","dataset_valid = NewsDataset(valid['TITLE'], valid['CATEGORY'])\n","\n","szVOCAB = len(set(word2id.values())) + 1 \n","szEMB = 300\n","PADDING_IDX = len(set(word2id.values()))\n","szOUTPUT = 4\n","szHIDDEN = 300\n","LEARNING_RATE = 0.005\n","numEPOCHS = 30\n","szBATCH = 32\n","\n","device = torch.device('cuda') #<-----\n","model = RNN(szVOCAB, szEMB, PADDING_IDX, szOUTPUT, szHIDDEN)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","model.to(device) #<------\n","dataloader_train = DataLoader(dataset_train, batch_size=szBATCH, shuffle=True, \n","                              collate_fn=Padsequence(PADDING_IDX)) #<-----------\n","dataloader_valid = DataLoader(dataset_valid, batch_size=szBATCH, shuffle=False,\n","                              collate_fn=Padsequence(PADDING_IDX)) #<-----------\n","scheduler = optim.lr_scheduler.StepLR(optimizer, numEPOCHS, gamma=0.5)\n","\n","def train_loop():\n","  measure = Measure()\n","  for epoch in range(numEPOCHS):\n","    measure.init_epoch()\n","    model.train()\n","    for data in dataloader_train:\n","      optimizer.zero_grad()\n","      inputs = data['inputs'].to(device) #<-----------\n","      labels = data['labels'].to(device) #<-----------\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","      measure.record_train(inputs,outputs,labels,loss.item())\n","    measure.train()  \n","    model.eval()\n","    with torch.no_grad():\n","      for data in dataloader_valid:\n","        optimizer.zero_grad()\n","        inputs = data['inputs'].to(device) #<------\n","        labels = data['labels'].to(device) #<------\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        measure.record_valid(inputs,outputs,labels,loss.item())\n","    measure.valid()\n","    measure.record_epoch(epoch)\n","    if measure.degrading(epoch): break\n","    scheduler.step()\n","  draw(epoch+1, measure.loss_train_list, measure.loss_valid_list, measure.accuracy_train_list, measure.accuracy_valid_list)\n","\n","train_loop()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nMkg02bHwSK"},"source":["##課題84. 単語ベクトルの導入 [省略]\n","事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での学習済み単語ベクトル）で単語埋め込みemb(x)を初期化し，学習せよ．"]},{"cell_type":"markdown","metadata":{"id":"I_MPepRVzp43"},"source":["###解説\n","Googleの埋め込みベクトルを、埋め込み表の初期値として使う。\n","相性が良くないようで、いい結果が出ない。省略。"]},{"cell_type":"markdown","metadata":{"id":"KfKEbuvDPl1D"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"nK7-E48PczAm"},"source":["import pickle\n","import numpy as np\n","import torch\n","\n","with open('./word2id.dict', mode='rb') as f: word2id = pickle.load(f)\n","szVOCAB = len(set(word2id.values())) + 1 \n","szEMB = 300\n","\n","from gensim.models import KeyedVectors\n","wordModel = KeyedVectors.load_word2vec_format('./GoogleNews.bin.gz', binary=True)\n","\n","weights = np.zeros((szVOCAB, szEMB))\n","hit = 0\n","err = 0\n","for i, word in enumerate(word2id.keys()):\n","  if word in wordModel:\n","    weights[i] = wordModel[word]\n","    hit += 1\n","  else:\n","    weights[i] = np.random.normal(scale=0.4, size=(szEMB,))\n","    err += 1\n","embeds = torch.from_numpy(weights.astype((np.float32)))\n","print(f'hit {hit}, err {err}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMEZ-d0wrJOV"},"source":["from torch import optim\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pickle\n","import numpy as np\n","\n","torch.manual_seed(0)\n","with open('./word2id.dict', mode='rb') as f: word2id = pickle.load(f)\n","train = pd.read_csv('./train.txt', sep='\\t', header=None, names=['TITLE','CAT'])\n","dataset_train = NewsDataset(train['TITLE'], train['CAT'])\n","valid = pd.read_csv('./valid.txt', sep='\\t', header=None, names=['TITLE','CAT'])\n","dataset_valid = NewsDataset(valid['TITLE'], valid['CAT'])\n","\n","szVOCAB = len(set(word2id.values())) + 1 \n","szEMB = 300\n","PADDING_IDX = len(set(word2id.values()))\n","szOUTPUT = 4\n","szHIDDEN = 300\n","LEARNING_RATE = 0.001\n","numEPOCHS = 30\n","szBATCH = 32\n","device = torch.device('cuda')\n","\n","class RNN(nn.Module):\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size):\n","    super().__init__()\n","    self.hidden_size = hidden_size\n","    self.emb = nn.Embedding.from_pretrained(embeds, padding_idx=padding_idx) #<-----------------\n","    #self.rnn = nn.RNN(emb_size, hidden_size, 1, nonlinearity='tanh', batch_first=True, dropout=0.3)\n","    self.rnn = nn.GRU(emb_size, hidden_size, 1, batch_first=True, dropout=0.3)\n","    self.fc = nn.Linear(hidden_size, output_size)\n","\n","  def forward(self, x):\n","    self.batch_size = x.size()[0]\n","    hidden = torch.zeros(1, self.batch_size, self.hidden_size).to(device)\n","    emb = self.emb(x)\n","    out, hidden = self.rnn(emb, hidden)\n","    out = self.fc(out[:, -1, :])\n","    return out\n","\n","model = RNN(szVOCAB, szEMB, PADDING_IDX, szOUTPUT, szHIDDEN)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","model.to(device)\n","dataloader_train = DataLoader(dataset_train, batch_size=szBATCH, shuffle=True, collate_fn=Padsequence(PADDING_IDX)) \n","dataloader_valid = DataLoader(dataset_valid, batch_size=szBATCH, shuffle=False, collate_fn=Padsequence(PADDING_IDX))\n","scheduler = optim.lr_scheduler.StepLR(optimizer, numEPOCHS, gamma=0.5)\n","\n","train_loop()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l7T3HnJjQJnw"},"source":["##課題85. 双方向RNN・多層化 [省略]\n","順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n","h←T+1=0,h←t=RNN←−−−(emb(xt),h←t+1),y=softmax(W(yh)[h→T;h←1]+b(y))\n","\n","ただし，h→t∈Rdh,h←t∈Rdh\n","はそれぞれ，順方向および逆方向のRNNで求めた時刻tの隠れ状態ベクトル，RNN←−−−(x,h)は入力xと次時刻の隠れ状態hから前状態を計算するRNNユニット，W(yh)∈RL×2dhは隠れ状態ベクトルからカテゴリを予測するための行列，b(y)∈RLはバイアス項である．また，[a;b]はベクトルaとb\n","\n","の連結を表す。\n","\n","さらに，双方向RNNを多層化して実験せよ．"]},{"cell_type":"markdown","metadata":{"id":"lk_ITmVTz5Sr"},"source":["###解説\n","双方向RNNというのは、入力を頭からRNNに通すのと、おしりから通すのと、両方やるやりかた。\n","多層化は、RNNを何枚か重ねて、下層の最後の隠れ状態を次の層の隠れベクトルの値とするように、つなげるもの。\n","この規模のデータだと、単一方向、単層と比べて、大きな改善はないので、省略。"]},{"cell_type":"markdown","metadata":{"id":"EXHwuCQhQS8g"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"nN-Qta_rXRN9"},"source":["!pwd\n","%cd /content/drive/My Drive/Colab Notebooks/NLP100Exercises2020後期\n","\n","from torch import optim\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pickle\n","import numpy as np\n","\n","torch.manual_seed(0)\n","with open('./word2id.dict', mode='rb') as f: word2id = pickle.load(f)\n","train = pd.read_csv('./train.txt', sep='\\t', header=None, names=['TITLE','CATEGORY'])\n","dataset_train = NewsDataset(train['TITLE'], train['CATEGORY'])\n","valid = pd.read_csv('./valid.txt', sep='\\t', header=None, names=['TITLE','CATEGORY'])\n","dataset_valid = NewsDataset(valid['TITLE'], valid['CATEGORY'])\n","\n","szVOCAB = len(set(word2id.values())) + 1 \n","szEMB = 300\n","PADDING_IDX = len(set(word2id.values()))\n","szOUTPUT = 4\n","szHIDDEN = 300\n","LEARNING_RATE = 0.01\n","numEPOCHS = 100\n","szBATCH = 32\n","BIDIRECTIONAL = True #<----\n","numLAYERS = 3 #<---\n","device = torch.device('cuda')\n","\n","class RNN(nn.Module):\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, hidden_size, \n","               num_layers, #<-----\n","               bidirectional #<-----\n","               ):\n","    super().__init__()\n","    self.hidden_size = hidden_size\n","    self.num_layers = num_layers\n","    self.num_directions = 2 if bidirectional else 1\n","    #self.emb = nn.Embedding.from_pretrained(embeds, padding_idx=padding_idx)\n","    self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","    #self.rnn = nn.RNN(emb_size, hidden_size, num_layers, #<---\n","    self.rnn = nn.GRU(emb_size, hidden_size, num_layers, #<---\n","                      batch_first=True, \n","                      bidirectional=bidirectional, #<---\n","                      dropout=0.3)\n","    self.fc = nn.Linear(hidden_size*self.num_directions, output_size)\n","\n","  def forward(self, x):\n","    self.batch_size = x.size()[0]\n","    hidden = torch.zeros(self.num_layers*self.num_directions, #<-----\n","                         self.batch_size, self.hidden_size).to(device)\n","    emb = self.emb(x)\n","    out, hidden = self.rnn(emb, hidden)\n","    out = self.fc(out[:, -1, :])\n","    return out\n","\n","model = RNN(szVOCAB, szEMB, PADDING_IDX, szOUTPUT, szHIDDEN, \n","            numLAYERS, BIDIRECTIONAL) #<-----\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","model.to(device)\n","dataloader_train = DataLoader(dataset_train, batch_size=szBATCH, shuffle=True, collate_fn=Padsequence(PADDING_IDX)) \n","dataloader_valid = DataLoader(dataset_valid, batch_size=szBATCH, shuffle=False, collate_fn=Padsequence(PADDING_IDX))\n","scheduler = optim.lr_scheduler.StepLR(optimizer, numEPOCHS, gamma=0.5)\n","\n","train_loop()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUYfKNPClpjD"},"source":["##課題86. 畳み込みニューラルネットワーク (CNN)\n","ID番号で表現された単語列x=(x1,x2,…,xT)がある．ただし，Tは単語列の長さ，xt∈RVは単語のID番号のone-hot表記である（Vは単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列xからカテゴリy\n","\n","を予測するモデルを実装せよ．\n","\n","ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n","\n","    単語埋め込みの次元数: dw\n","\n","畳み込みのフィルターのサイズ: 3 トークン\n","畳み込みのストライド: 1 トークン\n","畳み込みのパディング: あり\n","畳み込み演算後の各時刻のベクトルの次元数: dh\n","畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文をdh\n","\n","    次元の隠れベクトルで表現\n","\n","すなわち，時刻t\n","の特徴ベクトルpt∈Rdh\n","\n","は次式で表される．\n","pt=g(W(px)[emb(xt−1);emb(xt);emb(xt+1)]+b(p))\n","\n","ただし，W(px)∈Rdh×3dw,b(p)∈Rdh\n","はCNNのパラメータ，gは活性化関数（例えばtanhやReLUなど），[a;b;c]はベクトルa,b,cの連結である．なお，行列W(px)の列数が3dw\n","\n","になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n","\n","最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトルc∈Rdh\n","を求める．c[i]でベクトルcのi\n","\n","番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n","c[i]=max1≤t≤Tpt[i]\n","\n","最後に，入力文書の特徴ベクトルc\n","に行列W(yc)∈RL×dhとバイアス項b(y)∈RLによる線形変換とソフトマックス関数を適用し，カテゴリy\n","\n","を予測する．\n","y=softmax(W(yc)c+b(y))\n","\n","なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列でy\n","を計算するだけでよい．"]},{"cell_type":"markdown","metadata":{"id":"DQQk4QDM0OlH"},"source":["###解説\n","系列データに対し、画像処理で定番のCNNをかけてみる。\n","埋め込みベクトル表現した単語列データを画像のように見なし、ある単語の前後1単語ずつ含めて3単語分の埋め込みベクトルのデータに対し、畳み込みをかける。"]},{"cell_type":"markdown","metadata":{"id":"yToMgjx-l1wy"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"o1e9a1FUSGdN"},"source":["!pwd\n","%cd /content/drive/My Drive/Colab Notebooks/NLP"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qY-y6u9YSXlz"},"source":["80の再実行"]},{"cell_type":"code","metadata":{"id":"rqYtbjkMOtO_"},"source":["from torch import optim\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pickle\n","import numpy as np\n","from torch.nn import functional as F\n","\n","DEBUG = True\n","\n","class CNN(nn.Module):\n","  def __init__(self, vocab_size, emb_size, padding_idx, output_size, out_channels, kernel_heights, stride, padding):\n","    super().__init__()\n","    #self.emb = nn.Embedding.from_pretrained(embeds, padding_idx=padding_idx)\n","    self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx)\n","    #入力1個を、out_channles個のFearure Mapに変換する。\n","    #変換は、seq長x埋め込みベクトルサイズの入力に対し、(3x埋め込みベクトルサイズ)のカーネルで畳み込む。結果は、seq長x1\n","    self.conv = nn.Conv2d(1, out_channels, (kernel_heights, emb_size), stride, (padding, 0))\n","    self.drop = nn.Dropout(0.3)\n","    self.fc = nn.Linear(out_channels, output_size)\n"," \n","  def forward(self, x):\n","    if DEBUG: print(f'input: mini-batch size {x.size()[0]}, seq len {x.size()[1]}')\n","    #SEQ長(ミニバッチごと固定長)を並べてConvolution（カーネルサイズは、縦が３、横が埋め込みベクトルサイズ）かけるため、次元を挿入する。\n","    emb = self.emb(x).unsqueeze(1)\n","    if DEBUG: print(f'embed: min-batch size {emb.size()[0]}, input channles {emb.size()[1]}, seq len {emb.size()[2]}, embed size {emb.size()[3]}')\n","    #3x埋め込みサイズのカーネルで畳み込みし、out_channels個のSEQ長x1の中間データを出力する。\n","    conv = self.conv(emb)\n","    conv = conv.squeeze(3)\n","    if DEBUG: print(f'conv output: mini-batch size {conv.size()[0]}, out_channels {conv.size()[1]}, Conv result {conv.size()[2]}')\n","    act = F.relu(conv)\n","    #seq長の方向に最大値を取得\n","    max_pool = F.max_pool1d(act, act.size()[2])\n","    max_pool = max_pool.squeeze(2)\n","    if DEBUG: print(f'max pooled: mini-batch size {act.size()[0]}, out_channels {act.size()[1]}')\n","    out = self.fc(self.drop(max_pool))\n","    if DEBUG: print(f'output {out.size()}')\n","    return out\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4f-3RStxrphM"},"source":["# パラメータの設定\n","szVOCAB = len(set(word2id.values())) + 1\n","szEMB = 300\n","PADDING_IDX = len(set(word2id.values()))\n","szOUTPUT = 4\n","OUT_CHANNELS = 256\n","KERNEL_HEIGHTS = 3\n","STRIDE = 1\n","PADDING = 1\n","\n","torch.manual_seed(0)\n","with open('./word2id.dict', mode='rb') as f: word2id = pickle.load(f)\n","train = pd.read_csv('./train.txt', sep='\\t', header=None, names=['TITLE','CAT'])\n","dataset_train = NewsDataset(train['TITLE'], train['CAT'])\n","valid = pd.read_csv('./valid.txt', sep='\\t', header=None, names=['TITLE','CAT'])\n","dataset_valid = NewsDataset(valid['TITLE'], valid['CAT'])\n","\n","# モデルの定義\n","model = CNN(szVOCAB, szEMB, PADDING_IDX, szOUTPUT, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING)\n","\n","# 先頭10件の予測値取得\n","for i in range(10):\n","  X = dataset_train[i]['inputs']\n","  out = model(X.unsqueeze(0))\n","  print(torch.softmax(out, dim=-1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Npq4Wn43nyZL"},"source":["##課題87. 確率的勾配降下法によるCNNの学習　[省略]\n","確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"]},{"cell_type":"markdown","metadata":{"id":"9IF7aEow0cAm"},"source":["###解説\n","86をミニバッチにして、GPU利用。77,78相当。とても時間がかかるので省略していい。GoogleがフリーのVMに割り当てるGPUのグレードを落としたかな？"]},{"cell_type":"markdown","metadata":{"id":"h9kf_sh-n4ze"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"wy_-PbeEUh8d"},"source":["#\n","#複数データをミニバッチとして、並行して処理するようにする。RNNは入力が可変長であるため、並列実行しにくい。\n","#そこで、ミニバッチ単位で、最も長い単語列のサイズに合わせる。\n","#\n","class Padsequence():\n","  def __init__(self, padding_idx):\n","    self.padding_idx = padding_idx\n","\n","  def __call__(self, batch):\n","    sequences = [x['inputs'] for x in batch]\n","    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=self.padding_idx)\n","    labels = torch.LongTensor([x['labels'] for x in batch])\n","    return {'inputs': sequences_padded, 'labels': labels}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdQQPljWU0Ei"},"source":["import matplotlib.pyplot as plt\n","\n","class Measure():\n","  def __init__(self):\n","    self.loss_train_list = []\n","    self.loss_valid_list = []\n","    self.accuracy_train_list = []\n","    self.accuracy_valid_list = []\n","\n","  def init_epoch(self):\n","    self.correct_train = 0\n","    self.total_train = 0\n","    self.correct_valid = 0\n","    self.total_valid = 0\n","    self.loss_train = 0.0\n","    self.loss_valid = 0.0\n","\n","  def accuracy(self, inputs,outputs,labels):\n","    total = len(inputs)\n","    prediction = torch.argmax(outputs, dim=1)\n","    correct = torch.sum(prediction==labels)\n","    return total, correct\n","\n","  def record_train(self,inputs,outputs,labels,lossitem):\n","    self.loss_train += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_train += total\n","    self.correct_train += correct\n","\n","  def train(self):\n","    self.loss_train /= len(dataset_train)\n","    self.accuracy_train = self.correct_train / self.total_train\n","\n","  def record_valid(self,inputs,outputs,labels, lossitem):\n","    self.loss_valid += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_valid += total\n","    self.correct_valid += correct\n","\n","  def valid(self):\n","    self.loss_valid /= len(dataset_valid)\n","    self.accuracy_valid = self.correct_valid / self.total_valid\n","\n","  def record_epoch(self, epoch):\n","    print(f'epoch: {epoch + 1}, loss_train: {self.loss_train:.4f}, loss_valid: {self.loss_valid:.4f}, \\\n","    accuracy_train: {self.accuracy_train:.2f}, accuracy_valid: {self.accuracy_valid:.2f}')\n","    self.loss_train_list.append(self.loss_train)\n","    self.loss_valid_list.append(self.loss_valid)\n","    self.accuracy_train_list.append(self.accuracy_train)\n","    self.accuracy_valid_list.append(self.accuracy_valid)\n","\n","  def degrading(self, epoch):\n","   return (epoch > 1 and self.loss_valid_list[epoch - 2] <= self.loss_valid_list[epoch - 1] <= self.loss_valid_list[epoch])\n","\n","def draw(numEpochs, loss_train_list, loss_valid_list, accuracy_train_list, accuracy_valid_list ):\n","  plt.figure()\n","  plt.plot(range(numEpochs), loss_train_list, label='loss_train')\n","  plt.plot(range(numEpochs), loss_valid_list, label='loss_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()\n","  plt.figure()\n","  plt.plot(range(numEpochs), accuracy_train_list, label='accuracy_train')\n","  plt.plot(range(numEpochs), accuracy_valid_list, label='accuracy_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FkLtiSCz3E7p"},"source":["from torch import optim\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pickle\n","import numpy as np\n","\n","DEBUG=False\n","\n","torch.manual_seed(0)\n","with open('./word2id.dict', mode='rb') as f: word2id = pickle.load(f)\n","train = pd.read_csv('./train.txt', sep='\\t', header=None, names=['TITLE','CAT'])\n","dataset_train = NewsDataset(train['TITLE'], train['CAT'])\n","valid = pd.read_csv('./valid.txt', sep='\\t', header=None, names=['TITLE','CAT'])\n","dataset_valid = NewsDataset(valid['TITLE'], valid['CAT'])\n","\n","szVOCAB = len(set(word2id.values())) + 1 \n","szEMB = 300\n","PADDING_IDX = len(set(word2id.values()))\n","szOUTPUT = 4\n","LEARNING_RATE = 0.003\n","numEPOCHS = 30\n","szBATCH = 32\n","\n","OUT_CHANNELS = 256\n","KERNEL_HEIGHTS = 3\n","STRIDE = 1\n","PADDING = 1\n","\n","device = torch.device('cuda')\n","model = CNN(szVOCAB, szEMB, PADDING_IDX, szOUTPUT, OUT_CHANNELS, KERNEL_HEIGHTS, STRIDE, PADDING)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","model.to(device)\n","dataloader_train = DataLoader(dataset_train, batch_size=szBATCH, shuffle=True, collate_fn=Padsequence(PADDING_IDX)) \n","dataloader_valid = DataLoader(dataset_valid, batch_size=szBATCH, shuffle=False, collate_fn=Padsequence(PADDING_IDX))\n","scheduler = optim.lr_scheduler.StepLR(optimizer, numEPOCHS, gamma=0.5)\n","\n","def train_loop():\n","  measure = Measure()\n","  for epoch in range(numEPOCHS):\n","    measure.init_epoch()\n","    model.train()\n","    for data in dataloader_train:\n","      optimizer.zero_grad()\n","      inputs = data['inputs'].to(device) #<-----------\n","      labels = data['labels'].to(device) #<-----------\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels)\n","      loss.backward()\n","      optimizer.step()\n","      measure.record_train(inputs,outputs,labels,loss.item())\n","    measure.train()  \n","    model.eval()\n","    with torch.no_grad():\n","      for data in dataloader_valid:\n","        optimizer.zero_grad()\n","        inputs = data['inputs'].to(device) #<------\n","        labels = data['labels'].to(device) #<------\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        measure.record_valid(inputs,outputs,labels,loss.item())\n","    measure.valid()\n","    measure.record_epoch(epoch)\n","    if measure.degrading(epoch): break\n","    scheduler.step()\n","  draw(epoch+1, measure.loss_train_list, measure.loss_valid_list, measure.accuracy_train_list, measure.accuracy_valid_list)\n","\n","train_loop()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDQtX0Szo2zs"},"source":["##課題88. パラメータチューニング [省略]\n","問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ\n"]},{"cell_type":"markdown","metadata":{"id":"ZUVk1ii7C_eC"},"source":["##課題89. 事前学習済み言語モデルからの転移学習\n","事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"]},{"cell_type":"markdown","metadata":{"id":"fF5hMero-720"},"source":["###解説\n","\n","https://qiita.com/yamaru/items/63a342c844cff056a549　を参考にさせてもらいました。\n","\n","読みやすいように書き直しています。\n","\n","また精度が出るように、ネット構成、Optimizer、ロス関数を変えています。\n","\n","PyTorchのtransformerクラスでなく、transformers というパッケージを使っています。"]},{"cell_type":"code","metadata":{"id":"hcT5Snc051Ye"},"source":["!pwd\n","%cd /content/drive/My Drive/Colab Notebooks/NLP"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"alDPQJ0wGd0o"},"source":["!pip install transformers==3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oV53DJ6hGjWy"},"source":["import numpy as np\n","import transformers\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","from torch import optim\n","from torch import cuda\n","import time\n","from matplotlib import pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_N9aOpRVntW"},"source":["df = pd.read_csv('./newsCorpora.csv', header=None, sep='\\t', \n","    names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), \n","            ['TITLE', 'CATEGORY']]\n","df['TITLE'] = df['TITLE'].str.replace('[^a-zA-Z]',' ', regex=True)\n","df['TITLE'] = df['TITLE'].str.replace(' +',' ', regex=True)\n","df.to_csv('./news.csv',sep='\\t',index=False,header=False)\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","df = pd.read_csv('./newsCorpora.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n","train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n","train.reset_index(drop=True, inplace=True)\n","valid.reset_index(drop=True, inplace=True)\n","test.reset_index(drop=True, inplace=True)\n","print(train.head())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XA1NYMq8GoW7"},"source":["class NewsDataset(Dataset):\n","  def __init__(self, x, y, tokenizer, max_len):\n","    self.x = x\n","    self.y = y\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):  # len(Dataset)で返す値を指定\n","    return len(self.y)\n","\n","  def __getitem__(self, index):  # Dataset[index]で返す値を指定\n","    text = self.x[index]\n","    inputs = self.tokenizer.encode_plus(\n","      text,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      pad_to_max_length=True,\n","      truncation=True\n","    )\n","    ids = inputs['input_ids']\n","    mask = inputs['attention_mask']　# 固定長で処理するため、短い入力は0 padding。その位置を示す。\n","\n","    return {\n","      'ids': torch.LongTensor(ids),\n","      'mask': torch.LongTensor(mask),\n","      'labels': torch.Tensor(self.y[index])\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kfq5NSK_lMk0"},"source":["# 正解ラベルのone-hot化\n","y_train = pd.get_dummies(train, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n","y_valid = pd.get_dummies(valid, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n","y_test = pd.get_dummies(test, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n","\n","# Datasetの作成\n","max_len = 20\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","dataset_train = NewsDataset(train['TITLE'], y_train, tokenizer, max_len)\n","dataset_valid = NewsDataset(valid['TITLE'], y_valid, tokenizer, max_len)\n","dataset_test = NewsDataset(test['TITLE'], y_test, tokenizer, max_len)\n","\n","print(dataset_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amAOQ7d0lmKi"},"source":["class BERTClass(torch.nn.Module):\n","  def __init__(self, drop_rate, output_size):\n","    super().__init__()\n","    self.bert = BertModel.from_pretrained('bert-base-uncased')\n","    #self.drop = torch.nn.Dropout(drop_rate)\n","    #self.fc = torch.nn.Linear(768, output_size)  # BERTの出力に合わせて768次元を指定\n","\n","    self.fc1 = nn.Linear(768, 64)\n","    nn.init.kaiming_normal_(self.fc1.weight)\n","    self.fc2 = nn.Linear(64, output_size)\n","    nn.init.kaiming_normal_(self.fc2.weight)\n","    self.bn = nn.BatchNorm1d(64)\n","\n","  def forward(self, ids, mask):\n","    _, out = self.bert(ids, attention_mask=mask)\n","    #out = self.fc(self.drop(out))\n","    out = self.fc2(F.relu(self.bn(self.fc1(out))))\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHkB9oFW6XLc"},"source":["import matplotlib.pyplot as plt\n","\n","class Measure():\n","  def __init__(self):\n","    self.loss_train_list = []\n","    self.loss_valid_list = []\n","    self.accuracy_train_list = []\n","    self.accuracy_valid_list = []\n","\n","  def init_epoch(self):\n","    self.correct_train = 0\n","    self.total_train = 0\n","    self.correct_valid = 0\n","    self.total_valid = 0\n","    self.loss_train = 0.0\n","    self.loss_valid = 0.0\n","\n","  def accuracy(self, inputs,outputs,labels):\n","    total = len(inputs)\n","    prediction = torch.argmax(outputs, dim=1)\n","    correct = torch.sum(prediction==labels)\n","    return total, correct\n","\n","  def record_train(self,inputs,outputs,labels,lossitem):\n","    self.loss_train += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_train += total\n","    self.correct_train += correct\n","\n","  def train(self):\n","    self.loss_train /= len(dataset_train)\n","    self.accuracy_train = self.correct_train / self.total_train\n","\n","  def record_valid(self,inputs,outputs,labels, lossitem):\n","    self.loss_valid += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_valid += total\n","    self.correct_valid += correct\n","\n","  def valid(self):\n","    self.loss_valid /= len(dataset_valid)\n","    self.accuracy_valid = self.correct_valid / self.total_valid\n","\n","  def record_epoch(self, epoch):\n","    print(f'epoch: {epoch + 1}, loss_train: {self.loss_train:.4f}, loss_valid: {self.loss_valid:.4f}, \\\n","    accuracy_train: {self.accuracy_train:.2f}, accuracy_valid: {self.accuracy_valid:.2f}')\n","    self.loss_train_list.append(self.loss_train)\n","    self.loss_valid_list.append(self.loss_valid)\n","    self.accuracy_train_list.append(self.accuracy_train)\n","    self.accuracy_valid_list.append(self.accuracy_valid)\n","\n","  def degrading(self, epoch):\n","   return (epoch > 1 and self.loss_valid_list[epoch - 2] <= self.loss_valid_list[epoch - 1] <= self.loss_valid_list[epoch])\n","\n","def draw(numEpochs, loss_train_list, loss_valid_list, accuracy_train_list, accuracy_valid_list ):\n","  plt.figure()\n","  plt.plot(range(numEpochs), loss_train_list, label='loss_train')\n","  plt.plot(range(numEpochs), loss_valid_list, label='loss_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()\n","  plt.figure()\n","  plt.plot(range(numEpochs), accuracy_train_list, label='accuracy_train')\n","  plt.plot(range(numEpochs), accuracy_valid_list, label='accuracy_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PbK9ba3MHOkO"},"source":["class Measure2(Measure):\n","  def __init__(self):\n","    super().__init__()\n","  def accuracy(self, inputs,outputs,labels):\n","    prediction = torch.argmax(outputs, dim=-1) # バッチサイズの長さの予測ラベル配列\n","    labels = torch.argmax(labels, dim=-1)  # バッチサイズの長さの正解ラベル配列\n","    total = len(labels)\n","    correct = torch.sum(prediction == labels)\n","    return total, correct"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lK8mxJqzr_UA"},"source":["\n","DROP_RATE = 0.2\n","szOUTPUT = 4\n","szBATCH = 32\n","numEPOCHS = 10\n","LEARNING_RATE = 0.005\n","\n","model = BERTClass(DROP_RATE, szOUTPUT)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","model.to(device)\n","dataloader_train = DataLoader(dataset_train, batch_size=szBATCH, shuffle=True)\n","dataloader_valid = DataLoader(dataset_valid, batch_size=szBATCH, shuffle=False)\n","measure = Measure2()\n","for epoch in range(numEPOCHS):\n","  measure.init_epoch()\n","  model.train()\n","  for data in dataloader_train:\n","    optimizer.zero_grad()\n","    ids, mask, labels = data['ids'].to(device), data['mask'].to(device), data['labels'].to(device)\n","    outputs = model(ids, mask)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    measure.record_train(ids,outputs,labels,loss.item())  \n","  measure.train()  \n","  model.eval()\n","  with torch.no_grad():\n","    for data in dataloader_valid:\n","      optimizer.zero_grad()\n","      ids, mask, labels = data['ids'].to(device), data['mask'].to(device), data['labels'].to(device)\n","      outputs = model(ids, mask)\n","      loss = criterion(outputs, labels)\n","      measure.record_valid(ids,outputs,labels,loss.item())\n","  measure.valid()\n","  measure.record_epoch(epoch)\n","  if measure.degrading(epoch): break\n","  optimizer.step()\n","draw(epoch+1, measure.loss_train_list, measure.loss_valid_list, measure.accuracy_train_list, measure.accuracy_valid_list)\n"],"execution_count":null,"outputs":[]}]}