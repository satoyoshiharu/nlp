{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP、ニューラルネット.ipynb","provenance":[],"collapsed_sections":["cy_LEmEf8JPr","WKBWtbS8Jn1c","AbZHm2nMQOS5","bHluS0qfUg07","UunA8dLKZDKT","wKfEXYOaantp","ysUMvFQ3a7Ak","vr7lRmiMbR45","YFf3-g25pJSb","zPA4yV8odMcA"],"toc_visible":true,"mount_file_id":"15ugvZD-eqHpoLmg1IFlwC4LdESoj_iKr","authorship_tag":"ABX9TyOqhDwuAq0AgQae2WUmDVee"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"f94C95jj4nmN"},"source":["#第8章: ニューラルネット\n","\n","【注】 https://nlp100.github.io/ja/ch08.html　(ノートーブックでは数式などのフォーマットが崩れているので、このURL先の課題を見てください。)\n","\n","第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eu_fUNtw1B22","executionInfo":{"status":"ok","timestamp":1635646249053,"user_tz":-540,"elapsed":271,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"e8777597-3c58-4cdb-cfa9-1340ab4c3aff"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"cy_LEmEf8JPr"},"source":["##準備\n","ファイルIOするので、GDriveをマウントして、適当なフォルダーにカレントディレクトリーを設定する\n","\n","最初はGPUなしで実行してください。あとで、GPUアリの効果を見るため。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0dxiHwhmdcXp","executionInfo":{"status":"ok","timestamp":1635646249819,"user_tz":-540,"elapsed":438,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"8e8eb30b-ffb4-49f0-90cf-0d3361390716"},"source":["!pwd\n","%cd /content/drive/MyDrive/Colab Notebooks/NLP"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Colab Notebooks/NLP\n","/content/drive/MyDrive/Colab Notebooks/NLP\n"]}]},{"cell_type":"markdown","metadata":{"id":"l11Bbcix4q6M"},"source":["##課題70. 単語ベクトルの和による特徴量\n","問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例xiの特徴ベクトルxiを並べた行列Xと，正解ラベルを並べた行列（ベクトル）Y\n","\n","を作成したい．\n","X=⎛⎝⎜⎜⎜x1x2…xn⎞⎠⎟⎟⎟∈Rn×d,Y=⎛⎝⎜⎜⎜y1y2…yn⎞⎠⎟⎟⎟∈Nn\n","\n","ここで，n\n","は学習データの事例数であり，xi∈Rdとyi∈Nはそれぞれ，i∈{1,…,n}番目の事例の特徴量ベクトルと正解ラベルを表す． なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．N<4で4未満の自然数（0を含む）を表すことにすれば，任意の事例の正解ラベルyiはyi∈N<4で表現できる． 以降では，ラベルの種類数をLで表す（今回の分類タスクではL=4\n","\n","である）．\n","\n","i\n","番目の事例の特徴ベクトルxi\n","\n","は，次式で求める．\n","xi=1Ti∑t=1Tiemb(wi,t)\n","\n","ここで，i\n","番目の事例はTi個の（記事見出しの）単語列(wi,1,wi,2,…,wi,Ti)から構成され，emb(w)∈Rdは単語wに対応する単語ベクトル（次元数はd）である．すなわち，i番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものがxiである．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．300次元の単語ベクトルを用いたので，d=300\n","\n","である．\n","\n","i\n","番目の事例のラベルyi\n","\n","は，次のように定義する．\n","yi=⎧⎩⎨⎪⎪⎪⎪⎪⎪0123(記事xiが「ビジネス」カテゴリの場合)(記事xiが「科学技術」カテゴリの場合)(記事xiが「エンターテイメント」カテゴリの場合)(記事xiが「健康」カテゴリの場合)\n","\n","なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n","\n","以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n","\n","    学習データの特徴量行列: Xtrain∈RNt×d\n","\n","学習データのラベルベクトル: Ytrain∈NNt\n","検証データの特徴量行列: Xvalid∈RNv×d\n","検証データのラベルベクトル: Yvalid∈NNv\n","評価データの特徴量行列: Xtest∈RNe×d\n","評価データのラベルベクトル: Ytest∈NNe\n","\n","なお，Nt,Nv,Ne\n","はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"]},{"cell_type":"markdown","metadata":{"id":"rWaXQgURxZYy"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"BbT1UgKiHPJ6"},"source":["###解説\n","課題は数式でかえってわかりにくくなっているが、要するに、記事のタイトルと記事分類（4種類）のデータから、Googleの単語埋め込みベクトルを利用して、記事タイトルに含まれる単語の埋め込みベクトルの平均ベクトルデータと、記事分類正解ラベルデータを作れということである。\n","\n","ここで、単語ベクトルというのは、ある固定長ベクトルでそれぞれの単語を表現したもの。\n","\n","データを準備する手順：\n","\n","\n","\n","1.   この課題では、https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM　にある、GoogleNews-vectors-negative300.bin.gz（1.5G）というGoogleがあらかじめ英単語に関してニューラルネットでトレーニングして作った単語ベクトルモデルを使う。従来、動的にダウンロードできたが、最近、Google　クラウド側からでかすぎると拒否られ、Windows側でもでかすぎてウィルスチェックできないと拒否られるようになってしまった。そこで、すでに入手してあるファイルで GoogleNews.bin.gz を現作業フォルダー（Google Drive）にあらかじめ置いておく。もしもローカルなディスク領域とクラウドのGoogle Driveとを同期とるように設定しているならば、ローカルとクラウドとで同期を済ませておく（時間かかります）。\n","2.   記事データをpandasで読み込み、必要な部分だけ抜き取る。（「機械学習」の課題のデータを利用する）\n","1.   Googleの単語埋め込みベクトルデータをgensimでロードする。（「単語ベクトル」の課題で利用するデータを利用する）\n","2.   Titleに含まれる単語から、埋め込みベクトルを取り出し、ベクトルの平均をとる関数embAvgを定義しておく。\n","1.   Category文字を数字に変換した列と、Titleから計算した埋め込み平均ベクトル列を連結する。\n","2.   それを、sklearn.model_selection の train_test_splitメソッドで、train,valid,testに分割する。 \n","\n","実行の2回目以降の場合は、ファイルを上書きするかどうかプロンプトが来て待ち状態になるので、気を付けてください。"]},{"cell_type":"markdown","metadata":{"id":"Jv5bDF1K_wIe"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"QiADlCWZkAYx"},"source":["#\n","# 記事データをpandasで読み込み、必要な部分だけ抜き取る。\n","#\n","\n","#From From 50,51\n","!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n","!unzip NewsAggregatorDataset.zip\n","#\n","\n","!wc -l ./newsCorpora.csv\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('./newsCorpora.csv', header=None, sep='\\t', \n","    names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n","df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n","df['TITLE'] = df['TITLE'].str.lower()\n","df['TITLE'] = df['TITLE'].str.replace('[^a-z]',' ', regex=True)\n","df['TITLE'] = df['TITLE'].str.replace(' +',' ', regex=True)\n","df.to_csv('./news.csv',sep='\\t',index=False,header=False)\n","\n","!wc -l ./news.csv\n","\n","#\n","# Googleの単語埋め込みベクトルデータをgensimでロードする。\n","#\n","\n","# From 60\n","#import gdown\n","#url = \"https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n","#output = './GoogleNews.bin.gz'\n","#gdown.download(url, output, quiet=True)\n","#\n","\n","from gensim.models import KeyedVectors\n","model = KeyedVectors.load_word2vec_format('./GoogleNews.bin.gz', binary=True)\n","\n","# word vectors\n","import numpy as np\n","#\n","# Titleに含まれる単語から、埋め込みベクトルを取り出し、ベクトルの平均をとるために、embAvgを定義しておく。\n","#\n","def embAvg(text):\n","  global model\n","  #print(f'text: {text}')\n","  em = np.array([model[w] for w in text.split() if w in model])\n","  #print(f'em: {em}')\n","  em = ??????????????????\n","  #print(f'mean: {em}')\n","  return em\n","\n","def embSum(text):\n","  global model\n","  #print(f'text: {text}')\n","  em = np.array([model[w] for w in text.split() if w in model])\n","  #print(f'em: {em}')\n","  em = np.sum(em, axis=0)\n","  #print(f'mean: {em}')\n","  return em\n","#\n","# Category文字を数字に変換した列（正解ラベル）と、Titleから計算した埋め込み平均ベクトル列（入力データ）\n","# を連結する。\n","#\n","labels = df['CATEGORY'].reset_index()\n","labels = labels['CATEGORY'].map({'b': 0, 'e': 1, 't': 2, 'm': 3})\n","vects = pd.DataFrame([embSum(text) for text in df['TITLE']])\n","data = pd.concat(?????????????????????????)\n","#\n","# train,valid,testに分割する。\n","#\n","train, valid_test = train_test_split(data, train_size=0.8, shuffle=True, stratify=data['CATEGORY'])\n","valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, stratify=valid_test['CATEGORY'])\n","print(train.shape, valid.shape, test.shape)\n","train.iloc[:, :1].to_csv('train.label.txt', sep='\\t', index=False, header=False)\n","train.iloc[:,1:].to_csv('train.vector.txt', sep='\\t', index=False, header=False)\n","valid.iloc[:, :1].to_csv('valid.label.txt', sep='\\t', index=False, header=False)\n","valid.iloc[:,1:].to_csv('valid.vector.txt', sep='\\t', index=False, header=False)\n","test.iloc[:, :1].to_csv('test.label.txt', sep='\\t', index=False, header=False)\n","test.iloc[:,1:].to_csv('test.vector.txt', sep='\\t', index=False, header=False)\n","!wc -l train.label.txt train.vector.txt valid.label.txt valid.vector.txt test.label.txt test.vector.txt\n","!head train.label.txt\n","!head train.vector.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yncoxVdV9V3s"},"source":["##課題71. 単層ニューラルネットワークによる予測\n","問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n","y^1=softmax(x1W),Y^=softmax(X[1:4]W)\n","\n","ただし，softmax\n","はソフトマックス関数，X[1:4]∈R4×dは特徴ベクトルx1,x2,x3,x4\n","\n","を縦に並べた行列である．\n","X[1:4]=⎛⎝⎜⎜⎜x1x2x3x4⎞⎠⎟⎟⎟\n","\n","行列W∈Rd×L\n","は単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．なお，y^1∈RLは未学習の行列Wで事例x1を分類したときに，各カテゴリに属する確率を表すベクトルである． 同様に，Y^∈Rn×Lは，学習データの事例x1,x2,x3,x4\n","\n","について，各カテゴリに属する確率を行列として表現している．\n"]},{"cell_type":"markdown","metadata":{"id":"WKBWtbS8Jn1c"},"source":["###解説\n","ここでは、全結合層1層だけのモデルを定義し、その出力にSoftmaxかけることを、試しに先頭4個のデータで実行する。\n","\n","PyTorchでは、ネットモデルを書くには、nn.Moduleを継承し、コンストラクター__init__メソッドとforwardメソッドを定義する。 \n","\n","参考：\n","https://note.com/dngri/n/n0a86e12b1393  \n","https://qiita.com/perrying/items/857df46bb6cdc3047bd8。\n","\n","学習、検証・テストは、モデルクラスのインスタンスを作り、そのforward関数を繰り返し呼び出すことで行う。\n","\n","全結合Linear層定義の引数の意味、モデルオブジェクトを呼び出している書き方、softmaxメソッドの引数の意味、は、絵をここに書きにくいので、スライドの参考ページを参照してください。"]},{"cell_type":"markdown","metadata":{"id":"PJ4AvvhJ_2Uv"},"source":["###解答例"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKml70QZimFJ","executionInfo":{"status":"ok","timestamp":1635646793244,"user_tz":-540,"elapsed":2088,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"418ffe4a-ef72-460c-c920-26d975ffd3cd"},"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","#\n","# ネットモデルを書くには、mm.Moduleを継承し、コンストラクター__init__メソッドとforwardメソッドを定義する。\n","#\n","# class Net(nn.Module):\n","#   def __init__(self, input_size, output_size):\n","#     super().__init__()\n","#     ネットワーク要素定義、要素の初期設定\n","#   def forward(self, x):\n","#     ネットワーク要素を連結して前向き推論\n","#     return 計算結果出力\n","#\n","class Net(nn.Module):\n","  def __init__(self, input_size, output_size):\n","    super().__init__()\n","    self.fc = nn.Linear(input_size, output_size) #スライド参照\n","    nn.init.kaiming_normal_(self.fc.weight) #全結合層の重み初期化\n","\n","  def forward(self, x):\n","    x = ???????????????????????????????????\n","    return x\n","\n","#PyTorchの基本的なデータは、テンソル（多次元配列）で、配列データに加え、学習のための逆伝播に必要な計算履歴情報を含む。\n","#PyTorchで演算をするためにtorch.tensor(配列)で型変換をする\n","x_train = torch.tensor(pd.read_csv('train.vector.txt', sep='\\t', header=None).to_numpy(), dtype=torch.float)\n","print(f'入力 データ件数, ベクトル要素数 {x_train.shape}')\n","y_train = torch.tensor(pd.read_csv('train.label.txt', sep='\\t', header=None).to_numpy(), dtype=torch.long)\n","print(f'出力 データ件数, ラベル数 {y_train.shape}')\n","#Netクラスのコンストラクター呼び出し。入力と出力のサイズを設定(データのバッチサイズは別途)。\n","model = Net(300, 4)\n","##モデルのforwardを先頭4個に対し実行し、softmaxをかける\n","result = torch.softmax(model(x_train[:4]), dim=1) #modelオブジェクト呼び出しと、softmaxはスライドを参考\n","print(f'result データ件数, 出力ベクトル要素数 {result.shape}') #4件のそれぞれが4クラスのそれぞれに属する確率\n","print(result)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["入力 データ件数, ベクトル要素数 torch.Size([10672, 300])\n","出力 データ件数, ラベル数 torch.Size([10672, 1])\n","result データ件数, 出力ベクトル要素数 torch.Size([4, 4])\n","tensor([[0.0373, 0.2381, 0.4521, 0.2725],\n","        [0.3621, 0.2924, 0.1027, 0.2428],\n","        [0.4501, 0.1071, 0.3061, 0.1368],\n","        [0.1516, 0.2678, 0.3126, 0.2680]], grad_fn=<SoftmaxBackward>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"xIB3Ptk6-7l4"},"source":["##課題72. 損失と勾配の計算\n","学習データの事例x1と事例集合x1,x2,x3,x4に対して，クロスエントロピー損失と，行列Wに対する勾配を計算せよ．なお，ある事例xiに対して損失は次式で計算される．\n","li=−log[事例xiがyiに分類される確率]\n","\n","ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"]},{"cell_type":"markdown","metadata":{"id":"AbZHm2nMQOS5"},"source":["###解説\n","PyTorchにはCrossEntropyLossというクラスがあり、それは、softmaxを取り、自然対数を取ってクロスエントロピーを計算する、ことをすべてやってくれます。\n","\n","実行時の引数は、モデルの出力と、正解ラベルの1次元テンソル。https://pytorch.org/docs/stable/nn.html#loss-functions\n","スライドの参考を参照してください。\n","\n","ロス関数のbackward()というメソッドは、逆伝播を起動し、各パラメータの勾配を計算します。\n"]},{"cell_type":"markdown","metadata":{"id":"IIRmMPZBGVmz"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"duDXeKx-rUFo"},"source":["lossFunction = nn.CrossEntropyLoss()\n","print(model(x_train[:4]), '\\n', y_train.squeeze(1)[:4])\n","#CrossEntropLossのインスタンスに、モデルの出力と、正解ラベルの1次元テンソルを与える。\n","loss = lossFunction(model(x_train[:4]), y_train.squeeze(1)[:4])\n","print(loss)\n","#勾配を0で初期化\n","model.zero_grad() \n","#計算したロスをもとに、勾配を逆伝播する。\n","????????????????????????\n","lossAvg = loss.item()/4\n","print(f'損失: {lossAvg:.2f}')\n","print(f'勾配:\\n{model.fc.weight.grad}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"msJLI2QbGHHs"},"source":["##課題73. 確率的勾配降下法による学習\n","確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"]},{"cell_type":"markdown","metadata":{"id":"bHluS0qfUg07"},"source":["###解説\n","ここで、初めて、学習するトレーニングと評価を回します。大枠は以下の通りです。\n","\n","---\n","\n","前処理\n","\n","エポックの回数だけループ\n","\n","..trainのミニバッチの個数だけループ\n","\n","....トレーニング（正解と予測を比較してロスを計算し、逆伝播学習）\n","\n","..validのミニバッチの個数だけループ\n","\n","....評価（学習なしで予測）\n","\n","後処理\n","\n","---\n","なお、PyTorchは、生データを定義するデータセットクラスと、 ある（ミニバッチ）単位でまとめてforwardに入力するデータローダーとを準備して、前向き推論させる。\n","\n","データセットクラスは、__init__()、__len__()、__getitems__()を準備する。\n","\n","https://pytorch.org/docs/stable/data.html　\n","\n","ここでは、trainのミニバッチ数は１、valid/testのは全部として、データローダーの定義を省略している。\n"]},{"cell_type":"markdown","metadata":{"id":"cDHEs_FWHqPj"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"Jud0-LNlyJPy"},"source":["from torch.utils.data import Dataset, DataLoader\n","\n","TRAIN_BATCH_SIZE = 1\n","#\n","# データセットの定義 \n","#\n","class NewsDataset(Dataset):\n","  def __init__(self, x, y):  self.x, self.y = x, y\n","  def __len__(self):  return len(self.y)\n","  def __getitem__(self, idx):  return [self.x[idx], self.y[idx]]\n","\n","x_train = torch.tensor(pd.read_csv('train.vector.txt', sep='\\t', header=None).to_numpy(), dtype=torch.float)\n","y_train = torch.tensor(pd.read_csv('train.label.txt', sep='\\t', header=None).to_numpy(), dtype=torch.long)\n","x_valid = torch.tensor(pd.read_csv('valid.vector.txt', sep='\\t', header=None).to_numpy(), dtype=torch.float)\n","y_valid = torch.tensor(pd.read_csv('valid.label.txt', sep='\\t', header=None).to_numpy(), dtype=torch.long)\n","x_test = torch.tensor(pd.read_csv('test.vector.txt', sep='\\t', header=None).to_numpy(), dtype=torch.float)\n","y_test = torch.tensor(pd.read_csv('test.label.txt', sep='\\t', header=None).to_numpy(), dtype=torch.long)\n","dataset_train = NewsDataset(x_train, y_train)\n","dataset_valid = NewsDataset(x_valid, y_valid)\n","dataset_test = NewsDataset(x_test, y_test)\n","dataloader_train = DataLoader(dataset_train, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n","dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n","dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zA19-KBryQeZ"},"source":["import torch\n","import torch.nn as nn\n","#\n","# 前処理\n","#\n","#配列の初期化などで乱数が使われる。乱数の種を固定し、何度実行しても同じ結果を出すようにする。\n","torch.manual_seed(0)\n","model = Net(300, 4)\n","criterion = nn.CrossEntropyLoss()\n","#オプティマイザーは、逆伝播してパラメータ更新をするオブジェクト。\n","#アルゴリズムは、SGD（確率的勾配降下法）が代表的。stepメソッドで、  勾配を使ってパラメータを更新する。\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","numEpochs = 10\n","#\n","# エポック数だけループ\n","#\n","for epoch in range(numEpochs):\n","  # トレーニング前処理\n","  model.train() #学習モードに設定\n","  loss_train = 0.0\n","  #\n","  # トレーニングのミニバッチ数だけのループ\n","  #\n","  for batch_index, (inputs, labels) in enumerate(dataloader_train): # batch size is 1\n","    #\n","    #　トレーニング\n","    #\n","    optimizer.zero_grad() #勾配初期化\n","    outputs = ???????????????????????????? #モデルで前向き推論\n","    loss = criterion(outputs, labels.squeeze(1)) #labels=tensor([[a]]) shape=(1,1) -> labels.squeeze(dim=1)==tensor([a]), shape=(1)\n","    loss.backward() #逆伝播学習\n","    optimizer.step() #次の準備\n","    loss_train += loss.item()\n","  loss_train /= len(dataset_train)\n","  #評価前処理\n","  model.eval() #評価モードに設定\n","  #\n","  #評価のミニバッチ数だけのループ \n","  #\n","  with torch.no_grad(): # 勾配計算なしコンテキスト。batch size is len(dataset_valid)\n","    #\n","    # 評価\n","    #\n","    inputs, labels = next(iter(dataloader_valid))\n","    outputs = model(inputs)\n","    loss_valid = criterion(outputs, labels.squeeze(1))  #labels=tensor([[a],[b],...]) shape=(1334,1) -> labels.squeeze(dim=1)==tensor([a,b...]), shape=(1334)\n","  print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}')  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nMkg02bHwSK"},"source":["##課題74. 正解率の計測\n","問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"]},{"cell_type":"markdown","metadata":{"id":"UunA8dLKZDKT"},"source":["###解説\n","73のコードにAccuracyを計測する機能を追加する\n"]},{"cell_type":"markdown","metadata":{"id":"KfKEbuvDPl1D"},"source":["###解答例"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVT0CdiTpcoO","executionInfo":{"status":"ok","timestamp":1635646825972,"user_tz":-540,"elapsed":301,"user":{"displayName":"Yoshiharu Sato","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOVFRjsCzHOaL-FTnjtH-Ig_MMnJoOzLSKvGvF=s64","userId":"10555330050966070983"}},"outputId":"8c24b378-0a33-4476-ff04-a6dacd785c20"},"source":["#73のモデル定義前提\n","\n","#\n","#　AccuracyAccuracyを計算する補助関数\n","#\n","def accuracy(inputs,outputs,labels):\n","  total = len(inputs)\n","  #torch.argmax(テンソル、次元): テンソルの指定次元に沿って最大値を持つindexを返す。\n","  #nクラスに属する確率を示す要素のテンソルを渡すと、もっとも大きい確率のindexが得られる。\n","  #正解ラベルは、4つのクラスのどれかを示す0..3の数字\n","  prediction = torch.argmax(outputs, dim=1)\n","  #torch.sum(テンソル): 合計を返す。条件式の評価結果が、[True, False, …]などの場合、Trueの数を返す。\n","  correct = torch.sum(prediction==labels.squeeze(1))\n","  return total, correct\n","\n","# Accuracy計算のためのグローバル変数の初期化\n","correct_train = 0 #<--\n","total_train = 0 #<--\n","correct_valid = 0 #<--\n","total_valid = 0 #<--\n","\n","model.eval() \n","with torch.no_grad():\n","  for batch_index, (inputs, labels) in enumerate(dataloader_train):\n","    outputs = model(inputs)\n","    total, correct = accuracy(inputs,outputs,labels) #<--\n","    total_train += total #<--\n","    correct_train += correct #<--\n","  for batch_index, (inputs, labels) in enumerate(dataloader_valid):\n","    outputs = model(inputs)\n","    total, correct = accuracy(inputs,outputs,labels) #<--\n","    total_valid += total #<--\n","    correct_valid += correct #<--\n","#全体のAccuracyの計算\n","accuracy_train = correct_train / total_train #<--\n","accuracy_valid = ????????????????? #<--\n","print(f'accuracy_train: {accuracy_train:.2f}, accuracy_valid: {accuracy_valid:.2f}')  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy_train: 0.32, accuracy_valid: 0.30\n"]}]},{"cell_type":"markdown","metadata":{"id":"l7T3HnJjQJnw"},"source":["##課題75. 損失と正解率のプロット\n","問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"]},{"cell_type":"markdown","metadata":{"id":"wKfEXYOaantp"},"source":["###解説\n","73のコードを変更し、エポックごとに、accuracyを求める処理を追加する。\n","\n","エポックごとのLoss, Accuracyをリストにappendしておいて、全エポック終了後に、プロット処理を実行する。"]},{"cell_type":"markdown","metadata":{"id":"EXHwuCQhQS8g"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"RcTotNCnUTdA"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","#\n","# Accuracy, Lossを求める計算をメインループに入れると、煩雑なので、\n","# クラスにまとめておく\n","#\n","class Measure():\n","  def __init__(self):\n","    self.loss_train_list = []\n","    self.loss_valid_list = []\n","    self.accuracy_train_list = []\n","    self.accuracy_valid_list = []\n","\n","  def init_epoch(self):\n","    self.correct_train = 0\n","    self.total_train = 0\n","    self.correct_valid = 0\n","    self.total_valid = 0\n","    self.loss_train = 0.0\n","    self.loss_valid = 0.0\n","\n","  def accuracy(self, inputs,outputs,labels):\n","    total = len(inputs)\n","    prediction = torch.argmax(outputs, dim=1)\n","    correct = torch.sum(prediction==labels.squeeze(1))\n","    return total, correct\n","\n","  def record_train(self,inputs,outputs,labels,lossitem):\n","    self.loss_train += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_train += total\n","    self.correct_train += correct\n","\n","  def train(self):\n","    self.loss_train /= len(dataset_train)\n","    self.accuracy_train = self.correct_train / self.total_train\n","\n","  def record_valid(self,inputs,outputs,labels, lossitem):\n","    self.loss_valid += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_valid += total\n","    self.correct_valid += correct\n","\n","  def valid(self):\n","    self.accuracy_valid = self.correct_valid / self.total_valid\n","\n","  def record_epoch(self,epoch):\n","    print(f'epoch: {epoch + 1}, loss_train: {self.loss_train:.4f}, loss_valid: {self.loss_valid:.4f}, \\\n","    accuracy_train: {self.accuracy_train:.2f}, accuracy_valid: {self.accuracy_valid:.2f}')\n","    self.loss_train_list.append(self.loss_train)\n","    self.loss_valid_list.append(self.loss_valid)\n","    self.accuracy_train_list.append(self.accuracy_train)\n","    self.accuracy_valid_list.append(self.accuracy_valid)\n","\n","#\n","# 結果のプロット用の補助関数\n","#\n","def draw(numEpochs, loss_train_list, loss_valid_list, accuracy_train_list, accuracy_valid_list):\n","  plt.figure()\n","  plt.plot(range(numEpochs), loss_train_list, label='loss_train')\n","  plt.plot(range(numEpochs), loss_valid_list, label='loss_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()\n","  plt.figure()\n","  plt.plot(range(numEpochs), accuracy_train_list, label='accuracy_train')\n","  plt.plot(range(numEpochs), ????????????????????????, label='accuracy_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()\n","\n","torch.manual_seed(0)\n","model = Net(300, 4)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","numEpochs = 10\n","measure = Measure() #<---\n","\n","for epoch in range(numEpochs):\n","  measure.init_epoch() #<---\n","  model.train()\n","  for batch_index, (inputs, labels) in enumerate(dataloader_train): # batch size is 1\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels.squeeze(1)) #labels=tensor([[a]]) shape=(1,1) -> labels.squeeze(dim=1)==tensor([a]), shape=(1)\n","    loss.backward()\n","    optimizer.step()\n","    measure.record_train(inputs,outputs,labels,loss.item()) #<---\n","  measure.train() #<---\n","\n","  model.eval() \n","  with torch.no_grad(): # batch size is len(dataset_valid)\n","    inputs, labels = next(iter(dataloader_valid))\n","    outputs = model(inputs)\n","    loss_valid = criterion(outputs,labels.squeeze(1)) #labels=tensor([[a],[b],...]) shape=(1334,1) -> labels.squeeze(dim=1)==tensor([a,b...]), shape=(1334)\n","    measure.record_valid(inputs,outputs,labels,loss_valid.item()) #<---\n","  measure.valid() #<---\n","\n","  measure.record_epoch(epoch) #<---\n","\n","draw(numEpochs, measure.loss_train_list, measure.loss_valid_list, measure.accuracy_train_list, measure.accuracy_valid_list) #<---\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUYfKNPClpjD"},"source":["##課題76. チェックポイント\n","問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．"]},{"cell_type":"markdown","metadata":{"id":"ysUMvFQ3a7Ak"},"source":["###解説\n","75のコードの最後に、以下を追加する。\n"," torch.save({'epoch': epoch, \n","              'model_state_dict': model.state_dict(), \n","              'optimizer_state_dict': optimizer.state_dict()}, \n","              f'checkpoint{epoch + 1}.pt')\n"]},{"cell_type":"markdown","metadata":{"id":"yToMgjx-l1wy"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"NIZ2VUj61rI5"},"source":["from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","torch.manual_seed(1)\n","model = Net(300, 4)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","numEpochs = 10\n","\n","for epoch in range(numEpochs):\n","  model.train()\n","  for batch_index, (inputs, labels) in enumerate(dataloader_train): # batch size is 1\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels.squeeze(dim=0)) #labels=tensor([[a]]) shape=(1,1) -> labels.squeeze(dim=0)==tensor([a]), shape=(1)\n","    loss.backward()\n","    optimizer.step()\n","  model.eval() \n","  with torch.no_grad(): # batch size is len(dataset_valid)\n","    inputs, labels = next(iter(dataloader_valid))\n","    outputs = model(inputs)\n","  #\n","  #エポックごとにモデルパラメータを保存\n","  #\n","  ???????????????????????\n","#\n","# 最後に保存したものを、試しにダンプ\n","# 長いJobの途中で中断して、再開するときなどに便利。\n","#\n","checkpoint = torch.load(f'checkpoint{epoch + 1}.pt')\n","print(checkpoint['epoch'])\n","model.load_state_dict(checkpoint['model_state_dict'])\n","print(\"Model's state_dict:\")\n","for param_tensor in model.state_dict():\n","    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","print(\"Optimizer's state_dict:\")\n","for var_name in optimizer.state_dict():\n","    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Npq4Wn43nyZL"},"source":["##課題77. ミニバッチ化\n","問題76のコードを改変し，B事例ごとに損失・勾配を計算し，行列Wの値を更新せよ（ミニバッチ化）．Bの値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"]},{"cell_type":"markdown","metadata":{"id":"vr7lRmiMbR45"},"source":["###解説\n","ミニバッチのサイズによって、かかる時間がどういう影響を受けるかを見たい。\n","75などのコードは、DataLoaderの定義時にtrainは１、その他はデータセット全体のサイズにしていた。\n","\n","73ないし75のコードから、trainingしているところを切り出して関数trainにする。\n","\n","引数をバッチサイズとし、dataloader_trainを、そのサイズで定義しなおす。\n","\n","そのあと、通常のトレーニングを行う。\n","\n","速さを見るため、LossやAccuracyの関連コードは、いったん抜く。そのうえで、以下のコードを実行する。\n","\n","for batch_size in [2 ** i for i in range(11)]:\n","\n","..start = time.time()\n","\n","..train(batch_size)\n","\n","..end = time.time()\n","\n","..print(batch_size, (end - start)/numEpochs)"]},{"cell_type":"markdown","metadata":{"id":"h9kf_sh-n4ze"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"G_Fx5w7x546L"},"source":["#71,73前提\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","\n","numEpochs = 10\n","\n","#\n","# 73ないし75のコードから、trainingしているところを切り出して関数trainにする。\n","#\n","def train(train_batch_size):\n","  dataset_train = NewsDataset(x_train, y_train)\n","  #dataloader_trainを定義し、ミニバッチサイズを設定する。\n","  dataloader_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True)\n","  torch.manual_seed(1)\n","\n","  model = Net(300, 4)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","\n","  for epoch in range(numEpochs):\n","    for batch_index, (inputs, labels) in enumerate(dataloader_train): \n","      optimizer.zero_grad()\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels.squeeze(1))\n","      loss.backward()\n","      optimizer.step()\n","\n","for batch_size in [2 ** i for i in range(11)]:\n","  start = ?????????????????????\n","  train(batch_size)\n","  end = ???????????????????\n","  print(batch_size, (end - start)/numEpochs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDQtX0Szo2zs"},"source":["##課題78. GPU上での学習\n","問題77のコードを改変し，GPU上で学習を実行せよ．"]},{"cell_type":"markdown","metadata":{"id":"jOk3s54q7fcC"},"source":["###解説\n","77のコードを変更するが、CPUタイプをここでGPUに切り替える（VM・メモリのリセットが起きる）。そのため、カレントフォルダー設定、クラスNetの定義、trainに関するデータセット、データローダーの定義、をやり直す必要がある。\n","\n","以下の三か所で、GPUで計算するための処理を追加。\n","\n","冒頭で、\n","device = torch.device('cuda')\n","\n","モデルのインスタンス化のところで\n","model = Net(300, 4).to(device)\n","\n","データを取り出してモデルに送るところで、以下のように変更する。\n","\n","for batch_index, (inputs, labels) in enumerate(dataloader_train): \n","\n","..inputs = inputs.to(device)\n","\n","..labels = labels.to(device)\n"]},{"cell_type":"code","metadata":{"id":"BhuN_WxU7dVl"},"source":["!pwd\n","%cd /content/drive/My Drive/Colab Notebooks/NLP"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKTurISZ6O3a"},"source":["from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","\n","class Net(nn.Module):\n","  def __init__(self, input_size, output_size):\n","    super().__init__()\n","    self.fc = nn.Linear(input_size, output_size)\n","    nn.init.kaiming_normal_(self.fc.weight)\n","\n","  def forward(self, x):\n","    x = self.fc(x)\n","    return x\n","\n","class NewsDataset(Dataset):\n","  def __init__(self, x, y):  self.x, self.y = x, y\n","  def __len__(self):  return len(self.y)\n","  def __getitem__(self, idx):  return [self.x[idx], self.y[idx]]\n","\n","x_train = torch.tensor(pd.read_csv('train.vector.txt', sep='\\t', header=None).to_numpy(), dtype=torch.float)\n","y_train = torch.tensor(pd.read_csv('train.label.txt', sep='\\t', header=None).to_numpy(), dtype=torch.long)\n","dataset_train = NewsDataset(x_train, y_train)\n","\n","device = ??????????????????????????? #<----------\n","numEpochs = 10\n","\n","def train(train_batch_size):\n","  dataloader_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True)\n","  torch.manual_seed(0)\n","\n","  model = Net(300, 4).to(device) #<----------\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","  numEpochs = 10\n","\n","  for epoch in range(numEpochs):\n","    for batch_index, (inputs, labels) in enumerate(dataloader_train): \n","      inputs = inputs.to(device) #<----------\n","      labels = labels.to(device) #<----------\n","      optimizer.zero_grad()\n","      outputs = model(inputs)\n","      loss = criterion(outputs, labels.squeeze(1))\n","      loss.backward()\n","      optimizer.step()\n","\n","for batch_size in [2 ** i for i in range(11)]:\n","  start = time.time()\n","  train(batch_size)\n","  end = time.time()\n","  print(batch_size, (end - start)/numEpochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YFf3-g25pJSb"},"source":["###解答例"]},{"cell_type":"markdown","metadata":{"id":"ZUVk1ii7C_eC"},"source":["##課題79. 多層ニューラルネットワーク\n","問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"]},{"cell_type":"markdown","metadata":{"id":"zPA4yV8odMcA"},"source":["###解説\n","以上やったコードをまとめ上げて、以下などを試してみてください。\n","\n","クラスNetを変更し、単一全結合でなく、複数の全結合をつないでみる。\n","\n","全結合の入出力ベクトル要素数は、いきなり300->４でなく、少しづつ数を減らすようにするといいです。\n","\n","全結合の中間にBatchNormalizationを挟むといいです。\n","\n","モデルパラメータの初期値を変えてみる。\n","\n","Learning_rateを変更してみる。\n","\n","Epoch数を変更してみる。"]},{"cell_type":"markdown","metadata":{"id":"pAQySAubDHJC"},"source":["###解答例"]},{"cell_type":"code","metadata":{"id":"co7K_3UT-VOQ"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","\n","class NewsDataset(Dataset):\n","  def __init__(self, x, y):  self.x, self.y = x, y\n","  def __len__(self):  return len(self.y)\n","  def __getitem__(self, idx):  return [self.x[idx], self.y[idx]]\n","\n","class Net(nn.Module):\n","\n","  def __init__(self, input_size, output_size):\n","    super().__init__()\n","    self.fc1 = nn.Linear(input_size, 64)\n","    nn.init.kaiming_normal_(self.fc1.weight)\n","    self.fc2 = nn.Linear(64, 16)\n","    nn.init.kaiming_normal_(self.fc2.weight)\n","    self.bn = nn.BatchNorm1d(64)\n","    self.fc3 = nn.Linear(16, output_size)\n","    nn.init.kaiming_normal_(self.fc3.weight)\n","    self.bn2 = nn.BatchNorm1d(16)\n","\n","  def forward(self, x):\n","    x = self.fc3(F.relu(self.bn2(self.fc2(F.relu(self.bn(self.fc1(x)))))))\n","    return x\n","\n","class Measure():\n","  def __init__(self):\n","    self.loss_train_list = []\n","    self.loss_valid_list = []\n","    self.accuracy_train_list = []\n","    self.accuracy_valid_list = []\n","\n","  def init_epoch(self):\n","    self.correct_train = 0\n","    self.total_train = 0\n","    self.correct_valid = 0\n","    self.total_valid = 0\n","    self.loss_train = 0.0\n","    self.loss_valid = 0.0\n","\n","  def accuracy(self, inputs,outputs,labels):\n","    total = len(inputs)\n","    prediction = torch.argmax(outputs, dim=1)\n","    correct = torch.sum(prediction==labels.squeeze(1))\n","    return total, correct\n","\n","  def record_train(self,inputs,outputs,labels,lossitem):\n","    self.loss_train += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_train += total\n","    self.correct_train += correct\n","\n","  def train(self):\n","    self.loss_train /= len(dataset_train)\n","    self.accuracy_train = self.correct_train / self.total_train\n","\n","  def record_valid(self,inputs,outputs,labels, lossitem):\n","    self.loss_valid += lossitem\n","    total, correct = self.accuracy(inputs,outputs,labels)\n","    self.total_valid += total\n","    self.correct_valid += correct\n","\n","  def valid(self):\n","    self.loss_valid /= len(dataset_valid)\n","    self.accuracy_valid = self.correct_valid / self.total_valid\n","\n","  def record_epoch(self, epoch):\n","    print(f'epoch: {epoch + 1}, loss_train: {self.loss_train:.4f}, loss_valid: {self.loss_valid:.4f}, \\\n","    accuracy_train: {self.accuracy_train:.2f}, accuracy_valid: {self.accuracy_valid:.2f}')\n","    self.loss_train_list.append(self.loss_train)\n","    self.loss_valid_list.append(self.loss_valid)\n","    self.accuracy_train_list.append(self.accuracy_train)\n","    self.accuracy_valid_list.append(self.accuracy_valid)\n","\n","def draw(numEpochs, loss_train_list, loss_valid_list, accuracy_train_list, accuracy_valid_list ):\n","  plt.figure()\n","  plt.plot(range(numEpochs), loss_train_list, label='loss_train')\n","  plt.plot(range(numEpochs), loss_valid_list, label='loss_valid')\n","  plt.legend()\n","  plt.xlabel('epoch')\n","  plt.show()\n","  #plt.figure()\n","  #plt.plot(range(numEpochs), accuracy_train_list, label='accuracy_train')\n","  #plt.plot(range(numEpochs), accuracy_valid_list, label='accuracy_valid')\n","  #plt.legend()\n","  #plt.xlabel('epoch')\n","  #plt.show()\n","\n","torch.manual_seed(1)\n","device = torch.device('cuda')\n","train_batch_size = 1024\n","\n","numEpochs = 200\n","learning_rate = 0.01\n","\n","x_train = torch.tensor(pd.read_csv('train.vector.txt', sep='\\t', header=None).to_numpy(), dtype=torch.float)\n","y_train = torch.tensor(pd.read_csv('train.label.txt', sep='\\t', header=None).to_numpy(), dtype=torch.long)\n","dataset_train = NewsDataset(x_train, y_train)\n","dataloader_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True)\n","x_valid = torch.tensor(pd.read_csv('valid.vector.txt', sep='\\t', header=None).to_numpy(), dtype=torch.float)\n","y_valid = torch.tensor(pd.read_csv('valid.label.txt', sep='\\t', header=None).to_numpy(), dtype=torch.long)\n","dataset_valid = NewsDataset(x_valid, y_valid)\n","dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n","\n","model = Net(300, 4).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","measure = Measure()\n","\n","for epoch in range(numEpochs):\n","  measure.init_epoch()\n","\n","  model.train()\n","  for batch_index, (inputs, labels) in enumerate(dataloader_train): \n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels.squeeze(1)) #labels=tensor([[a]]) shape=(1,1) -> labels.squeeze(dim=1)==tensor([a]), shape=(1)\n","    loss.backward()\n","    optimizer.step()\n","    measure.record_train(inputs,outputs,labels, loss.item())\n","  measure.train()\n","  \n","  model.eval() \n","  with torch.no_grad(): # batch size is len(dataset_valid)\n","    inputs, labels = next(iter(dataloader_valid))\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","    outputs = model(inputs)\n","    loss_valid = criterion(outputs, labels.squeeze(1))  #labels=tensor([[a],[b],...]) shape=(1334,1) -> labels.squeeze(dim=1)==tensor([a,b...]), shape=(1334)\n","    measure.record_valid(inputs,outputs,labels, loss_valid.item())\n","  measure.valid()\n","\n","  measure.record_epoch(epoch)\n","  \n","draw(numEpochs, measure.loss_train_list, measure.loss_valid_list, measure.accuracy_train_list, measure.accuracy_valid_list)\n"],"execution_count":null,"outputs":[]}]}